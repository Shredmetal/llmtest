{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"llmtest","text":"<p>A semantic testing framework for LLM applications that uses LLMs to validate semantic equivalence in test outputs. </p> <p>\u2728 Test your LLM apps in minutes, not hours</p> <p>\ud83d\ude80 CI/CD ready out of the box</p> <p>\ud83d\udcb0 Cost-effective testing solution</p> <p>\ud83d\udd27 No infrastructure needed</p>"},{"location":"#what-llmtest-does","title":"What llmtest Does","text":"<ul> <li>Tests LLM applications (not the LLMs themselves)</li> <li>Validates system message + prompt template outputs</li> <li>Ensures semantic equivalence of responses</li> <li>Tests the parts YOU control in your LLM application</li> </ul>"},{"location":"#what-llmtest-doesnt-do","title":"What llmtest Doesn't Do","text":"<ul> <li>Test LLM model performance (that's the provider's responsibility)</li> <li>Validate base model capabilities</li> <li>Test model reliability</li> <li>Handle model safety features</li> </ul>"},{"location":"#when-to-use-llmtest","title":"When to Use llmtest","text":"<ul> <li>Testing application-level LLM integration</li> <li>Validating prompt engineering</li> <li>Testing system message effectiveness</li> <li>Ensuring consistent response patterns</li> </ul>"},{"location":"#when-not-to-use-llmtest","title":"When Not to Use llmtest","text":"<ul> <li>Testing base LLM performance</li> <li>Evaluating model capabilities</li> <li>Testing model safety features</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>API Reference</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>CI/CD Integration</li> <li>Best Practices</li> <li>API Reference</li> </ul>"},{"location":"#license","title":"License","text":""},{"location":"#license_1","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"api/configuration/","title":"Configuration","text":"<p>\u2190 Back to Home</p>"},{"location":"api/configuration/#configuration","title":"Configuration","text":"<p>llmtest provides flexible configuration through environment variables and direct parameters.</p>"},{"location":"api/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"api/configuration/#provider-selection","title":"Provider Selection","text":"<pre><code>LLM_PROVIDER=openai # or 'anthropic'\n</code></pre>"},{"location":"api/configuration/#api-keys","title":"API Keys","text":"<pre><code># For OpenAI (default provider)\n\nOPENAI_API_KEY=your-openai-key\n\n# For Anthropic\n\nANTHROPIC_API_KEY=your-anthropic-key\n</code></pre>"},{"location":"api/configuration/#optional-settings","title":"Optional Settings","text":"<pre><code>LLM_MODEL=gpt-4o # Model name - default for OpenAI: gpt-4o, default for anthropic: claude-3-5-sonnet-latest\nLLM_TEMPERATURE=0.0 # Response randomness (0.0-1.0) \nLLM_MAX_TOKENS=4096 # Maximum response length \nLLM_MAX_RETRIES=2 # API retry attempts\n</code></pre>"},{"location":"api/configuration/#direct-configuration","title":"Direct Configuration","text":"<p>You can also configure llmtest programmatically:</p> <pre><code>from llmtest.semanticassert.semantic_assert import SemanticAssertion\n\nasserter = SemanticAssertion(api_key=\"your-api-key\", # Strongly advised against, use env vars \n                             provider=\"openai\", # or 'anthropic' \n                             model=\"gpt-4o\", # See Supported Models \n                             temperature=0.0, # Default: 0.0 \n                             max_tokens=4096, # Default: 4096 \n                             max_retries=2 # Default: 2 \n                             )\n</code></pre>"},{"location":"api/configuration/#supported-models","title":"Supported Models","text":"<p>Model supported is restricted due to poorer semantic matching performance of less advanced models.</p>"},{"location":"api/configuration/#openai","title":"OpenAI","text":"<ul> <li>gpt-4o</li> <li>gpt-4-turbo</li> </ul>"},{"location":"api/configuration/#anthropic","title":"Anthropic","text":"<ul> <li>claude-3-5-sonnet-latest</li> <li>claude-3-opus-latest (EXPENSIVE!!!)</li> </ul>"},{"location":"api/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Configuration values are resolved in this order:</p> <ol> <li>Directly passed parameters</li> <li>Environment variables</li> <li>Default values</li> </ol>"},{"location":"api/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for API keys</li> <li>Keep temperature at 0.0 for consistent testing</li> <li>Use default max_tokens unless you have specific needs</li> <li>Start with default max_retries (2)</li> </ol> <p>Note: llmtest validates all configuration values at startup to prevent runtime errors.</p>"},{"location":"api/configuration/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"api/error-handling/","title":"Error Handling","text":"<p>\u2190 Back to Home</p>"},{"location":"api/error-handling/#error-handling","title":"Error Handling","text":"<p>llmtest provides specific exceptions for different error scenarios to help you handle errors gracefully in your tests.</p>"},{"location":"api/error-handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>LLMTestError # Base exception for all llmtest errors \n\u251c\u2500\u2500 SemanticAssertionError # When semantic matching fails \n\u251c\u2500\u2500 LLMConfigurationError # When configuration is invalid \n\u251c\u2500\u2500 LLMConnectionError # When LLM service fails \n\u2514\u2500\u2500 InvalidPromptError # When prompt construction fails - not currently in use, code left in situ for future development\n</code></pre>"},{"location":"api/error-handling/#exception-details","title":"Exception Details","text":""},{"location":"api/error-handling/#semanticassertionerror","title":"SemanticAssertionError","text":"<p>Raised when the semantic assertion fails:</p> <pre><code>try: \n    semantic_assert.assert_semantic_match(actual=\"Hello Bob\", \n                                          expected_behavior=\"A greeting for Alice\") \nexcept SemanticAssertionError as e: \n    print(f\"Test failed: {e}\") # Includes detailed reason why outputs don't match\n</code></pre>"},{"location":"api/error-handling/#llmconfigurationerror","title":"LLMConfigurationError","text":"<p>Raised when configuration is invalid:</p> <pre><code>try: \n    semantic_assert = SemanticAssertion(provider=\"invalid_provider\") \nexcept LLMConfigurationError as e: \n    print(f\"Configuration error: {e}\") # Details about invalid configuration\n</code></pre>"},{"location":"api/error-handling/#llmconnectionerror","title":"LLMConnectionError","text":"<p>Raised when LLM service fails:</p> <pre><code>try: \n    semantic_assert.assert_semantic_match(...) \nexcept LLMConnectionError as e: \n    print(f\"Service error: {e}\") # Contains original provider error details\n</code></pre>"},{"location":"api/error-handling/#invalidprompterror-not-currently-in-use","title":"InvalidPromptError (NOT CURRENTLY IN USE)","text":"<p>Raised when prompt construction fails:</p> <pre><code>try: semantic_assert.assert_semantic_match(actual=None, # Invalid input \n                                            expected_behavior=\"Some behavior\") \nexcept InvalidPromptError as e: \n    print(f\"Prompt error: {e}\")\n</code></pre>"},{"location":"api/error-handling/#error-information","title":"Error Information","text":"<p>All exceptions should provide: - Clear error message - Detailed reason (when available) - Additional context in details dictionary</p> <p>Example:</p> <pre><code>try: \n    semantic_assert.assert_semantic_match(...) \nexcept LLMTestError as e: \n    print(f\"Message: {e.message}\") print(f\"Reason: {e.reason}\") print(f\"Details: {e.details}\")\n</code></pre>"},{"location":"api/error-handling/#best-practices","title":"Best Practices","text":"<ol> <li>Always catch specific exceptions rather than the base LLMTestError</li> <li>Log the full error information for debugging</li> <li>Handle LLMConnectionError for retry logic</li> <li>Use error details for test reporting</li> </ol> <p>Note: All exceptions properly chain to their root cause, preserving the full error context.</p>"},{"location":"api/error-handling/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"api/semantic-assertion/","title":"SemanticAssertion","text":"<p>\u2190 Back to Home</p>"},{"location":"api/semantic-assertion/#semanticassertion","title":"SemanticAssertion","text":"<p>Core class for semantic testing of LLM applications.</p>"},{"location":"api/semantic-assertion/#constructor","title":"Constructor","text":"<pre><code>SemanticAssertion(api_key: Optional[str] = None, \n                  llm: Optional[BaseLanguageModel] = None, \n                  provider: Optional[Union[str, LLMProvider]] = None, \n                  model: Optional[str] = None, \n                  temperature: Optional[float] = None, \n                  max_tokens: Optional[int] = None, \n                  max_retries: Optional[int] = None )\n</code></pre>"},{"location":"api/semantic-assertion/#parameters","title":"Parameters","text":"<p>All parameters are optional (except for API key) and will use environment variables or defaults if not specified:</p> <ul> <li> <p>api_key: API key for the LLM provider</p> <ul> <li>Environment: <code>OPENAI_API_KEY</code> or <code>ANTHROPIC_API_KEY</code></li> <li>Default: None (must be provided via environment or parameter)</li> <li>If using default provider: use <code>OPENAI_API_KEY</code> since default provider is <code>openai</code></li> </ul> </li> <li> <p>llm: Pre-configured LLM instance (must be of type <code>langchain_core.language_models import BaseLanguageModel</code>)</p> <ul> <li>Default: None (if provided, bypasses all other configuration)</li> </ul> </li> <li> <p>provider: LLM provider ('openai' or 'anthropic')</p> <ul> <li>Environment: <code>LLM_PROVIDER</code></li> <li>Default: 'openai'</li> </ul> </li> <li> <p>model: Model name to use</p> </li> <li> <p>Environment: <code>LLM_MODEL</code></p> <ul> <li>Default: 'gpt-4o' for OpenAI, 'claude-3-5-sonnet-latest' for Anthropic</li> </ul> </li> <li> <p>temperature: Temperature setting for LLM responses</p> </li> <li> <p>Environment: <code>LLM_TEMPERATURE</code></p> <ul> <li>Default: 0.0</li> <li>Range: 0.0 to 1.0</li> </ul> </li> <li> <p>max_tokens: Maximum tokens for response</p> </li> <li> <p>Environment: <code>LLM_MAX_TOKENS</code></p> <ul> <li>Default: 4096</li> </ul> </li> <li> <p>max_retries: Maximum number of API call retries</p> </li> <li>Environment: <code>LLM_MAX_RETRIES</code><ul> <li>Default: 2</li> </ul> </li> </ul>"},{"location":"api/semantic-assertion/#methods","title":"Methods","text":""},{"location":"api/semantic-assertion/#assert_semantic_match","title":"assert_semantic_match","text":"<pre><code>def assert_semantic_match(actual: str, expected_behavior: str ) -&gt; None\n</code></pre> <p>Asserts that actual output semantically matches expected behavior.</p>"},{"location":"api/semantic-assertion/#parameters_1","title":"Parameters","text":"<ul> <li>actual: The actual output to test</li> <li>expected_behavior: Natural language description of expected behavior</li> </ul>"},{"location":"api/semantic-assertion/#raises","title":"Raises","text":"<ul> <li>SemanticAssertionError: If outputs don't match semantically</li> <li>LLMConnectionError: If LLM service fails</li> <li>LLMConfigurationError: If configuration is invalid</li> <li>TypeError: If inputs are None</li> </ul>"},{"location":"api/semantic-assertion/#examples","title":"Examples","text":""},{"location":"api/semantic-assertion/#basic-usage","title":"Basic Usage","text":"<pre><code>asserter = SemanticAssertion() # Uses environment variables \nasserter.assert_semantic_match(actual=\"Hello Alice, how are you?\", # In practice, use the output from your LLM application\n                               expected_behavior=\"A greeting addressing Alice\" \n                               )\n</code></pre>"},{"location":"api/semantic-assertion/#custom-configuration","title":"Custom Configuration","text":"<pre><code>asserter = SemanticAssertion(provider=\"anthropic\", \n                             model=\"claude-3-5-sonnet-latest\", # Look I can't stop you from burning a hole in your wallet but please don't use Claude 3 Opus. \n                             temperature=0.1 )\n</code></pre>"},{"location":"api/semantic-assertion/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>Configuration Reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>\u2190 Back to Home</p>"},{"location":"getting-started/installation/#installation","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip (package installer for Python)</li> </ul>"},{"location":"getting-started/installation/#installing-llmtest","title":"Installing llmtest","text":"<p>Install directly from GitHub:</p> <pre><code>pip install git+https://github.com/Shredmetal/llmtest.git\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to llmtest, install with development dependencies:</p> <pre><code># Clone the repository\n\ngit clone https://github.com/Shredmetal/llmtest.git\n\n# Change to project directory\n\ncd llmtest\n\n# Install with development dependencies\n\npip install -e \".[dev]\"]\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Learn how to use llmtest</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>\u2190 Back to Home</p>"},{"location":"getting-started/quickstart/#quick-start","title":"Quick Start","text":"<p>Get up and running with llmtest in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#1-set-up-environment","title":"1. Set Up Environment","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># For OpenAI\n\nOPENAI_API_KEY=your-openai-api-key-here\n\n# OR for Anthropic\n\nANTHROPIC_API_KEY=your-anthropic-key-here\n</code></pre>"},{"location":"getting-started/quickstart/#2-write-your-first-test","title":"2. Write Your First Test","text":"<pre><code>from llmtest.semanticassert.semantic_assert import SemanticAssertion\n\ndef my_first_semantic_test(): \n\n    # Initialize asserter \n    semantic_assert = SemanticAssertion()\n\n    # Your LLM output\n    actual_output = \"The sky is blue\"\n\n    # Expected behavior in natural language\n    expected_behavior = \"A statement about the color of the sky\"\n\n    # Test semantic equivalence\n    semantic_assert.assert_semantic_match(\n        actual=actual_output,\n        expected_behavior=expected_behavior\n)\n</code></pre>"},{"location":"getting-started/quickstart/#3-run-your-test","title":"3. Run Your Test","text":"<pre><code>pytest my_first_semantic_test.py\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>CI/CD Integration - Set up automated testing</li> <li>Configuration - Configure llmtest for your needs</li> </ul>"},{"location":"getting-started/quickstart/#additional-notes","title":"Additional notes:","text":""},{"location":"getting-started/quickstart/#real-world-example","title":"Real World Example","text":"<p>Want to see llmtest in action? Here's a real test from our test suite:</p> <pre><code>from llmtest.semanticassert.semantic_assert import SemanticAssertion \nfrom llmtest.tests.test_content_generators.test_greeting_bot import SimpleGreetingBot\n\ndef test_greeting_semantic(): \n\n    semantic_assert = SemanticAssertion()\n\n    bot = SimpleGreetingBot()\n    actual_output = bot.generate_greeting(\"Alice\")\n\n    expected_behavior = \"\"\"\n    A polite greeting that:\n    1. Addresses the person by name (Alice)\n    2. Asks about their wellbeing\n    \"\"\"\n\n    semantic_assert.assert_semantic_match(\n        actual=actual_output,\n        expected_behavior=expected_behavior\n    )\n</code></pre> <p>You can find this example in our repository: test_greeting.py</p> <p>It can be run from this project root with the following command:</p> <pre><code>pytest tests/actual_usage_tests/test_greeting.py\n</code></pre> <p>You can find and run this example:</p> <pre><code># Clone the repository\n\ngit clone https://github.com/Shredmetal/llmtest.git\n\n# Run the test\n\npytest tests/actual_usage_tests/test_greeting.py\n</code></pre>"},{"location":"guides/best-practices/","title":"Best Practices","text":"<p>\u2190 Back to Home</p>"},{"location":"guides/best-practices/#best-practices","title":"Best Practices","text":"<p>Guidelines for effective semantic testing with llmtest.</p>"},{"location":"guides/best-practices/#understanding-semantic-testing","title":"Understanding Semantic Testing","text":"<p>Semantic testing focuses on meaning rather than exact matches. For example:</p> <pre><code># THESE ARE SEMANTICALLY EQUIVALENT\n\nactual_1 = \"Hello Alice, how are you today?\" \nactual_2 = \"Hi Alice! Hope you're doing well!\" \nexpected = \"A polite greeting addressing Alice\"\n</code></pre>"},{"location":"guides/best-practices/#writing-good-expected-behaviors","title":"Writing Good Expected Behaviors","text":"<ol> <li>Be Specific:</li> </ol> <pre><code># Good\n\nexpected_behavior = \"\"\" A polite greeting that:\n                    Addresses the person by name (Alice)\n                    Asks about their wellbeing \"\"\"\n\n# Bad - too vague\n\nexpected_behavior = \"A nice greeting\"\n</code></pre> <ol> <li>Focus on Requirements:</li> </ol> <pre><code># Good\n\nexpected_behavior = \"\"\" An error message that:\n                    Explains the API key is invalid\n                    Provides steps to fix the issue \"\"\"\n\n# Bad - testing exact wording - DO NOT USE LLMTEST FOR THIS, JUST USE REGULAR PYTEST\n\nexpected_behavior = \"Should say 'Invalid API key'\"\n</code></pre>"},{"location":"guides/best-practices/#when-to-use-semantic-testing","title":"When to Use Semantic Testing","text":"<p>Good Use Cases: - Testing LLM outputs - Validating natural language responses - Testing content generation</p> <p>Not Suitable For: - Exact string matching - Numerical comparisons - Binary conditions</p>"},{"location":"guides/best-practices/#test-structure","title":"Test Structure","text":"<ol> <li>Keep Tests Focused:</li> </ol> <pre><code># Good\n\ndef test_greeting_format(): \"\"\"Test greeting format only\"\"\"\n\ndef test_greeting_personalization(): \"\"\"Test name usage separately\"\"\"\n\n# Bad - testing too much\n\ndef test_everything_about_greeting(): \"\"\"Testing multiple aspects at once\"\"\"\n</code></pre> <ol> <li>Clear Test Names:</li> </ol> <pre><code># Good\n\ndef test_error_message_includes_solution_steps():\n\n# Bad\n\ndef test_error():\n</code></pre>"},{"location":"guides/best-practices/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Over-Specific Expected Behaviors:</li> </ol> <pre><code># Too specific\n\nexpected = \"Must say hello and use exactly these words\"\n\n# Better\n\nexpected = \"Should be a greeting in conversational English\"\n</code></pre> <ol> <li>Under-Specific Expected Behaviors:</li> </ol> <pre><code># Too vague\n\nexpected = \"Should be good\"\n\n# Better\n\nexpected = \"\"\" Response should:\n           Answer the user's question\n           Use professional language\n           Stay on topic \"\"\"\n</code></pre>"},{"location":"guides/best-practices/#cost-optimization","title":"Cost Optimization","text":"<ol> <li>Use specific, focused tests</li> <li>Group related semantic tests</li> <li>Consider test importance vs cost</li> <li>Use appropriate model tiers</li> </ol>"},{"location":"guides/best-practices/#closing-words","title":"Closing words","text":"<p>In general, this is a complete different type of testing designed to mimic a human testing your LLM application.  You might need to use your brain a little bit to figure out to instruct your assistant on what to check.</p>"},{"location":"guides/best-practices/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"guides/ci-cd/","title":"CI/CD Integration","text":"<p>\u2190 Back to Home</p>"},{"location":"guides/ci-cd/#cicd-integration","title":"CI/CD Integration","text":"<p>llmtest is designed for seamless integration with CI/CD pipelines.</p>"},{"location":"guides/ci-cd/#setting-up-cicd","title":"Setting Up CI/CD","text":""},{"location":"guides/ci-cd/#1-environment-setup","title":"1. Environment Setup","text":"<p>Add your API keys as secrets in your CI/CD environment:</p> <pre><code># GitHub Actions example\n\nenv: OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n\n# OR\n\nenv: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n</code></pre>"},{"location":"guides/ci-cd/#2-test-configuration","title":"2. Test Configuration","text":"<pre><code># tests/test_llm_app.py\n\nfrom llmtest.semanticassert.semantic_assert import SemanticAssertion\n\ndef test_llm_output(): \n    semantic_assert = SemanticAssertion() # Your tests here\n</code></pre>"},{"location":"guides/ci-cd/#3-cicd-pipeline-configuration","title":"3. CI/CD Pipeline Configuration","text":""},{"location":"guides/ci-cd/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: LLM Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.8'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install git+https://github.com/Shredmetal/llmtest.git\n\n      - name: Run tests\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: pytest tests/\n</code></pre>"},{"location":"guides/ci-cd/#best-practices","title":"Best Practices","text":"<ol> <li> <p>API Key Management:</p> <ul> <li>Never commit API keys</li> <li>Use CI/CD environment secrets</li> <li>Rotate keys regularly</li> </ul> </li> <li> <p>Cost Control:</p> <ul> <li>Use cheaper models in CI/CD</li> <li>Run semantic tests only on critical paths</li> <li>Consider test result caching</li> </ul> </li> <li> <p>Pipeline Optimization:</p> <ul> <li>Run traditional tests first</li> <li>Run semantic tests in parallel</li> <li>Set appropriate timeouts</li> </ul> </li> <li> <p>Error Handling:</p> <ul> <li>Implement retry logic for API failures</li> <li>Log detailed error information</li> <li>Set up alerts for repeated failures</li> </ul> </li> </ol>"},{"location":"guides/ci-cd/#common-issues","title":"Common Issues","text":"<ol> <li> <p>API Rate Limits:</p> <ul> <li>Implement exponential backoff</li> <li>Use test result caching</li> <li>Consider parallel test execution</li> </ul> </li> <li> <p>Cost Management:</p> <ul> <li>Monitor API usage</li> <li>Set budget alerts</li> <li>Use test filtering</li> </ul> </li> </ol>"},{"location":"guides/ci-cd/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"}]}