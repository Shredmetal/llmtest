{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"llm_app_test","text":"<p>A unit testing framework for LLM applications that uses LLMs to validate semantic equivalence in test outputs with a syntax familiar to software engineers.</p> <p>\u26a0\ufe0f Note on Reliability: While we cannot guarantee 100% reliability (due to the fundamental nature of LLMs), we ran tens of thousands of tests to establish reliability. However, past success doesn't guarantee future determinism - this is an unsolvable problem in LLM testing, but we've implemented extensive mitigations to make it as reliable as possible. </p> <p>\u2728 Test your LLM apps in minutes, not hours</p> <p>\ud83d\ude80 CI/CD ready out of the box (Still in beta so, while we're pretty sure it just works\u2122, we're still trying our damnedest to break it in GitHub Actions, GitLab CI, Jenkins, and other CI/CD environments. We'll document any failures we find and their fixes)</p> <p>\ud83d\udcb0 Cost-effective testing solution</p> <p>\ud83d\udd27 No infrastructure needed (Unless if you want to inject a custom LLM, please refer to the configuration page for details)</p>"},{"location":"#library-reliability-testing","title":"Library Reliability Testing","text":"<p>Format Compliance Reliability Testing</p> <p>Semantic Reliability Testing</p> <p>Semantic Boundary Discovery and Analysis</p>"},{"location":"#testing-philosophy","title":"Testing Philosophy","text":"<p>When integrating LLMs into your application, treat them as you would any closed-source third-party library:</p> <ol> <li>Write tests for expected behaviour</li> <li>Focus on interface boundaries</li> <li>Test application-level functionality</li> <li>Maintain clear separation of concerns</li> </ol>"},{"location":"#important-information-on-understanding-responsibilities","title":"\u26a0\ufe0f Important Information on Understanding Responsibilities","text":"<p>This library is built by software engineers to give software engineers a tool to validate the behaviour of applications that have had an LLM stuffed in them. It is NOT a Data Science tool nor a replacement for model metrics used by Data Science teams to validate model suitability.</p>"},{"location":"#software-engineers-role","title":"Software Engineer's Role","text":"<ul> <li>Write tests for expected application behaviour</li> <li>Validate inputs and outputs</li> <li>Ensure proper integration</li> <li>Monitor system performance</li> <li>Escalate consistent failures to DS team (as this might indicate a fundamental problem with the model, or perhaps to seek assistance with the <code>expected_behavior</code> prompt in the <code>assert_semantic_match</code> function)</li> </ul>"},{"location":"#data-science-teams-role","title":"Data Science Team's Role","text":"<ul> <li>Handle model-level issues</li> <li>Address consistent test failures</li> <li>Evaluate model suitability</li> <li>Optimise model performance</li> <li>Adjust prompts when needed</li> </ul>"},{"location":"#when-to-escalate","title":"When to Escalate","text":"<p>Escalate to your Data Science team when:</p> <ol> <li>Tests consistently fail despite correct implementation</li> <li>Model responses are consistently inappropriate</li> <li>Performance degradation is observed</li> <li>Pattern of failures indicates model-level issues</li> </ol>"},{"location":"#what-makes-this-different","title":"\ud83d\udd0d What Makes This Different?","text":"<p>This is an ENGINEERING tool, not a data science tool. The difference is crucial:</p> <p>Data Science Tools: - Test model performance - Evaluate model accuracy - Measure model metrics</p> <p>llm_app_test (Engineering Tool): - Tests your APPLICATION code - Validates integration points - Ensures system behaviour - Maintains production reliability</p> <p>Think of it this way: You don't test Redis itself, you test your application's use of Redis.  Similarly, llm_app_test helps you test your application's use of LLMs.</p>"},{"location":"#in-summary","title":"In summary:","text":""},{"location":"#what-llm_app_test-does","title":"What llm_app_test Does","text":"<ul> <li>Tests LLM applications (not the LLMs themselves)</li> <li>Validates system message + prompt template outputs</li> <li>Ensures semantic equivalence of responses</li> <li>Tests the parts YOU control in your LLM application</li> </ul>"},{"location":"#what-llm_app_test-doesnt-do","title":"What llm_app_test Doesn't Do","text":"<ul> <li>Test LLM model performance (that's the provider's responsibility)</li> <li>Validate base model capabilities</li> <li>Test model reliability</li> <li>Handle model safety features</li> </ul>"},{"location":"#when-to-use-llm_app_test","title":"When to Use llm_app_test","text":"<ul> <li>Testing application-level LLM integration</li> <li>Validating prompt engineering</li> <li>Testing system message effectiveness</li> <li>Ensuring consistent response patterns</li> </ul>"},{"location":"#when-not-to-use-llm_app_test","title":"When Not to Use llm_app_test","text":"<ul> <li>Testing base LLM performance</li> <li>Evaluating model capabilities</li> <li>Testing model safety features</li> </ul>"},{"location":"#need-testing-ideas-check-out-the-tests-we-used-to-test-llm-app-test-here","title":"Need testing ideas? Check out the tests we used to test llm-app-test here","text":""},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>CI/CD Integration</li> <li>Best Practices</li> <li>API Reference</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter issues:</p> <ol> <li>Create an issue on our GitHub repository</li> <li>Include your Python version and environment details</li> <li>Describe the problem you encountered with version 0.1.0b4</li> </ol>"},{"location":"#support","title":"\ud83c\udd98 Support","text":"<ul> <li>Discord: Join our community</li> <li>Issues: GitHub Issues</li> <li>Documentation: Full Docs</li> <li>Email: morganj.lee01@gmail.com</li> </ul>"},{"location":"#due-to-the-number-of-downloads-i-am-seeing-on-pypistatsorg-i-am-including-these-instructions-in-case-a-beta-update-breaks-something-on-your-end","title":"Due to the number of downloads I am seeing on pypistats.org, I am including these instructions in case a beta update breaks something on your end:","text":""},{"location":"#emergency-rollback-instructions","title":"Emergency Rollback Instructions","text":"<p>If you experience issues with version 0.1.0b4, you can roll back to the previous stable version (0.1.0b3.post3) using one of these methods:</p>"},{"location":"#method-1-direct-installation-of-previous-version","title":"Method 1: Direct Installation of Previous Version","text":"<pre><code>pip uninstall llm-app-test \npip install llm-app-test==0.1.0b3.post3\n</code></pre>"},{"location":"#method-2-force-reinstall-if-method-1-fails","title":"Method 2: Force Reinstall (if Method 1 fails)","text":"<pre><code>pip install --force-reinstall llm-app-test==0.1.0b3.post3\n</code></pre>"},{"location":"#verification","title":"Verification","text":"<p>After rolling back, verify the installation: <pre><code>import llm_app_test \nprint(llm_app_test.version) # Should show 0.1.0b3.post3\n</code></pre></p>"},{"location":"#important-note-about-rate-limits-if-running-large-numbers-of-tests","title":"\u26a0\ufe0f Important Note About Rate Limits - If Running Large Numbers of Tests:","text":""},{"location":"#anthropic-rate-limits","title":"Anthropic Rate limits:","text":"<p>Tier 1:</p> Model Maximum Requests per minute (RPM) Maximum Tokens per minute (TPM) Maximum Tokens per day (TPD) Claude 3.5 Sonnet 2024-10-22 50 40,000 1,000,000 Claude 3.5 Sonnet 2024-06-20 50 40,000 1,000,000 Claude 3 Opus 50 20,000 1,000,000 <p>Tier 2:</p> Model Maximum Requests per minute (RPM) Maximum Tokens per minute (TPM) Maximum Tokens per day (TPD) Claude 3.5 Sonnet 2024-10-22 1,000 80,000 2,500,000 Claude 3.5 Sonnet 2024-06-20 1,000 80,000 2,500,000 Claude 3 Opus 1,000 40,000 2,500,000"},{"location":"#openai-rate-limits","title":"OpenAI Rate Limits","text":"<p>Tier 1</p> Model RPM RPD TPM Batch Queue Limit gpt-4o 500 - 30,000 90,000 gpt-4o-mini 500 10,000 200,000 2,000,000 gpt-4o-realtime-preview 100 100 20,000 - gpt-4-turbo 500 - 30,000 90,000 <p>Tier 2:</p> Model RPM TPM Batch Queue Limit gpt-4o 5,000 450,000 1,350,000 gpt-4o-mini 5,000 2,000,000 20,000,000 gpt-4o-realtime-preview 200 40,000 - gpt-4-turbo 5,000 450,000 1,350,000"},{"location":"api/configuration/","title":"Configuration","text":"<p>\u2190 Back to Home</p>"},{"location":"api/configuration/#configuration","title":"Configuration","text":"<p>llm_app_test provides flexible configuration through environment variables and direct parameters.</p>"},{"location":"api/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"api/configuration/#provider-selection","title":"Provider Selection","text":"<pre><code>LLM_PROVIDER=openai # or 'anthropic'\n</code></pre>"},{"location":"api/configuration/#api-keys","title":"API Keys","text":"<pre><code># For OpenAI (default provider)\n\nOPENAI_API_KEY=your-openai-key\n\n# For Anthropic\n\nANTHROPIC_API_KEY=your-anthropic-key\n</code></pre>"},{"location":"api/configuration/#optional-settings","title":"Optional Settings","text":"<pre><code>LLM_MODEL=gpt-4o # Model name - default for OpenAI: gpt-4o, default for anthropic: claude-3-5-sonnet-latest\nLLM_TEMPERATURE=0.0 # Response randomness (0.0-1.0) \nLLM_MAX_TOKENS=4096 # Maximum response length \nLLM_MAX_RETRIES=2 # API retry attempts\n</code></pre>"},{"location":"api/configuration/#direct-configuration","title":"Direct Configuration","text":"<p>You can also configure llm_app_test programmatically:</p> <pre><code>from llm_app_test.semanticassert.semantic_assert import SemanticAssertion\n\nasserter = SemanticAssertion(api_key=\"your-api-key\", # Strongly advised against, use env vars \n                             provider=\"openai\", # or 'anthropic' \n                             model=\"gpt-4o\", # See Supported Models \n                             temperature=0.0, # Default: 0.0 \n                             max_tokens=4096, # Default: 4096 \n                             max_retries=2 # Default: 2 \n                             )\n</code></pre>"},{"location":"api/configuration/#supported-and-recommended-models","title":"Supported (and recommended) Models","text":"<p>Model support is restricted due to the complex semantic reasoning required for accurate testing. We've found that only the most advanced models can reliably handle semantic comparison tasks.</p>"},{"location":"api/configuration/#openai","title":"OpenAI","text":"<ul> <li>gpt-4o</li> <li>gpt-4-turbo</li> </ul>"},{"location":"api/configuration/#anthropic","title":"Anthropic","text":"<ul> <li>claude-3-5-sonnet-latest</li> <li>claude-3-opus-latest (EXPENSIVE!!!)</li> </ul>"},{"location":"api/configuration/#custom-llm-configuration-advanced-users-only","title":"Custom LLM Configuration (Advanced Users Only)","text":"<p>While it's possible to inject a custom LLM using Langchain's <code>BaseLanguageModel</code>, this is strongly discouraged unless you have extensively tested your model's semantic reasoning capabilities. Smaller or less capable models will likely fail at the semantic testing tasks. If you happen to have a datacenter in your back pocket however, Llama 3.1:405B might be a good bet.</p> <pre><code># Advanced usage - Only for thoroughly tested models\nfrom llm_app_test.semanticassert.semantic_assert import SemanticAssertion\nfrom langchain_core.language_models import BaseLanguageModel\n\ncustom_llm: BaseLanguageModel = your_custom_llm\nasserter = SemanticAssertion(llm=custom_llm)\n</code></pre> <p>If you pass in a custom llm, it will disable ALL other LLM configuration options, and you have to configure that LLM yourself.</p>"},{"location":"api/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Configuration values are resolved in this order:</p> <ol> <li>Custom LLM (if provided, disables ALL other LLM settings and uses the settings of the Langchain <code>BaseLanguageModel</code> object that you have passed)</li> <li>Directly passed parameters</li> <li>Environment variables</li> <li>Default values</li> </ol> <p>Note: if a custom LLM is not passed, llm_app_test validates all configuration values at startup to prevent runtime errors.</p>"},{"location":"api/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for API keys</li> <li>Keep temperature at 0.0 for consistent testing</li> <li>Use default max_tokens unless you have specific needs</li> <li>Start with default max_retries (2)</li> <li>Stick with frontier models for the best results. This library is completely untested with anything other than frontier models.</li> </ol>"},{"location":"api/configuration/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"api/error-handling/","title":"Error Handling","text":"<p>\u2190 Back to Home</p>"},{"location":"api/error-handling/#error-handling","title":"Error Handling","text":"<p>llm_app_test provides specific exceptions for different error scenarios to help you handle errors gracefully in your tests.</p>"},{"location":"api/error-handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>LLMTestError # Base exception for all llm_app_test errors \n\u251c\u2500\u2500 SemanticAssertionError # When semantic matching fails \n\u251c\u2500\u2500 LLMConfigurationError # When configuration is invalid \n\u251c\u2500\u2500 LLMConnectionError # When LLM service fails \n\u2514\u2500\u2500 InvalidPromptError # When prompt construction fails - not currently in use, code left in situ for future development\n</code></pre>"},{"location":"api/error-handling/#exception-details","title":"Exception Details","text":""},{"location":"api/error-handling/#semanticassertionerror","title":"SemanticAssertionError","text":"<p>Raised when the semantic assertion fails:</p> <pre><code>try: \n    semantic_assert.assert_semantic_match(actual=\"Hello Bob\", \n                                          expected_behavior=\"A greeting for Alice\") \nexcept SemanticAssertionError as e: \n    print(f\"Test failed: {e}\") # Includes detailed reason why outputs don't match\n</code></pre>"},{"location":"api/error-handling/#llmconfigurationerror","title":"LLMConfigurationError","text":"<p>Raised when configuration is invalid:</p> <pre><code>try: \n    semantic_assert = SemanticAssertion(provider=\"invalid_provider\") \nexcept LLMConfigurationError as e: \n    print(f\"Configuration error: {e}\") # Details about invalid configuration\n</code></pre>"},{"location":"api/error-handling/#llmconnectionerror","title":"LLMConnectionError","text":"<p>Raised when LLM service fails:</p> <pre><code>try: \n    semantic_assert.assert_semantic_match(...) \nexcept LLMConnectionError as e: \n    print(f\"Service error: {e}\") # Contains original provider error details\n</code></pre>"},{"location":"api/error-handling/#invalidprompterror-not-currently-in-use","title":"InvalidPromptError (NOT CURRENTLY IN USE)","text":"<p>Raised when prompt construction fails:</p> <pre><code>try: semantic_assert.assert_semantic_match(actual=None, # Invalid input \n                                            expected_behavior=\"Some behavior\") \nexcept InvalidPromptError as e: \n    print(f\"Prompt error: {e}\")\n</code></pre>"},{"location":"api/error-handling/#error-information","title":"Error Information","text":"<p>All exceptions should provide: - Clear error message - Detailed reason (when available) - Additional context in details dictionary</p> <p>Example:</p> <pre><code>try: \n    semantic_assert.assert_semantic_match(...) \nexcept LLMTestError as e: \n    print(f\"Message: {e.message}\") print(f\"Reason: {e.reason}\") print(f\"Details: {e.details}\")\n</code></pre>"},{"location":"api/error-handling/#best-practices","title":"Best Practices","text":"<ol> <li>Always catch specific exceptions rather than the base LLMTestError</li> <li>Log the full error information for debugging</li> <li>Handle LLMConnectionError for retry logic</li> <li>Use error details for test reporting</li> </ol> <p>Note: All exceptions properly chain to their root cause, preserving the full error context.</p>"},{"location":"api/error-handling/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"api/semantic-assertion/","title":"SemanticAssertion","text":"<p>\u2190 Back to Home</p>"},{"location":"api/semantic-assertion/#semanticassertion","title":"SemanticAssertion","text":"<p>Core class for semantic testing of LLM applications.</p>"},{"location":"api/semantic-assertion/#constructor","title":"Constructor","text":"<pre><code>SemanticAssertion(api_key: Optional[str] = None, \n                  llm: Optional[BaseLanguageModel] = None, \n                  provider: Optional[Union[str, LLMProvider]] = None, \n                  model: Optional[str] = None, \n                  temperature: Optional[float] = None, \n                  max_tokens: Optional[int] = None, \n                  max_retries: Optional[int] = None )\n</code></pre>"},{"location":"api/semantic-assertion/#parameters","title":"Parameters","text":"<p>All parameters are optional (except for API key) and will use environment variables or defaults if not specified:</p> <ul> <li> <p>api_key: API key for the LLM provider</p> <ul> <li>Environment: <code>OPENAI_API_KEY</code> or <code>ANTHROPIC_API_KEY</code></li> <li>Default: None (must be provided via environment or parameter)</li> <li>If using default provider: use <code>OPENAI_API_KEY</code> since default provider is <code>openai</code></li> </ul> </li> <li> <p>llm: Pre-configured LLM instance (must be of type <code>langchain_core.language_models import BaseLanguageModel</code>)</p> <ul> <li>Default: None (if provided, bypasses all other configuration)</li> </ul> </li> <li> <p>provider: LLM provider ('openai' or 'anthropic')</p> <ul> <li>Environment: <code>LLM_PROVIDER</code></li> <li>Default: 'openai'</li> </ul> </li> <li> <p>model: Model name to use</p> </li> <li> <p>Environment: <code>LLM_MODEL</code></p> <ul> <li>Default: 'gpt-4o' for OpenAI, 'claude-3-5-sonnet-latest' for Anthropic</li> </ul> </li> <li> <p>temperature: Temperature setting for LLM responses</p> </li> <li> <p>Environment: <code>LLM_TEMPERATURE</code></p> <ul> <li>Default: 0.0</li> <li>Range: 0.0 to 1.0</li> </ul> </li> <li> <p>max_tokens: Maximum tokens for response</p> </li> <li> <p>Environment: <code>LLM_MAX_TOKENS</code></p> <ul> <li>Default: 4096</li> </ul> </li> <li> <p>max_retries: Maximum number of API call retries</p> </li> <li>Environment: <code>LLM_MAX_RETRIES</code><ul> <li>Default: 2</li> </ul> </li> </ul>"},{"location":"api/semantic-assertion/#methods","title":"Methods","text":""},{"location":"api/semantic-assertion/#assert_semantic_match","title":"assert_semantic_match","text":"<pre><code>def assert_semantic_match(actual: str, expected_behavior: str ) -&gt; None\n</code></pre> <p>Asserts that actual output semantically matches expected behavior.</p>"},{"location":"api/semantic-assertion/#parameters_1","title":"Parameters","text":"<ul> <li>actual: The actual output to test</li> <li>expected_behavior: Natural language description of expected behavior</li> </ul>"},{"location":"api/semantic-assertion/#raises","title":"Raises","text":"<ul> <li>SemanticAssertionError: If outputs don't match semantically</li> <li>LLMConnectionError: If LLM service fails</li> <li>LLMConfigurationError: If configuration is invalid</li> <li>TypeError: If inputs are None</li> </ul>"},{"location":"api/semantic-assertion/#examples","title":"Examples","text":""},{"location":"api/semantic-assertion/#basic-usage","title":"Basic Usage","text":"<pre><code>asserter = SemanticAssertion() # Uses environment variables \nasserter.assert_semantic_match(actual=\"Hello Alice, how are you?\", # In practice, use the output from your LLM application\n                               expected_behavior=\"A greeting addressing Alice\" \n                               )\n</code></pre>"},{"location":"api/semantic-assertion/#custom-configuration","title":"Custom Configuration","text":"<pre><code>asserter = SemanticAssertion(provider=\"anthropic\", \n                             model=\"claude-3-5-sonnet-latest\", # Look I can't stop you from burning a hole in your wallet but please don't use Claude 3 Opus. \n                             temperature=0.1 )\n</code></pre>"},{"location":"api/semantic-assertion/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>Configuration Reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>\u2190 Back to Home</p>"},{"location":"getting-started/installation/#installation","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip (package installer for Python)</li> </ul>"},{"location":"getting-started/installation/#installing-llm_app_test","title":"Installing llm_app_test","text":"<p>Install directly from GitHub:</p> <pre><code>pip install llm-app-test\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to llm_app_test, install with development dependencies:</p> <pre><code># Clone the repository\n\ngit clone https://github.com/Shredmetal/llmtest.git\n\n# Change to project directory\n\ncd llm_app_test\n\n# Install with development dependencies\n\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Learn how to use llm_app_test</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>\u2190 Back to Home</p>"},{"location":"getting-started/quickstart/#quick-start","title":"Quick Start","text":"<p>Get up and running with llm_app_test in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#1-set-up-environment","title":"1. Set Up Environment","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># For OpenAI\n\nOPENAI_API_KEY=your-openai-api-key-here\n\n# OR for Anthropic\n\nANTHROPIC_API_KEY=your-anthropic-key-here\n</code></pre>"},{"location":"getting-started/quickstart/#2-write-your-first-test","title":"2. Write Your First Test","text":"<pre><code>from llm_app_test.semanticassert.semantic_assert import SemanticAssertion\n\ndef my_first_semantic_test(): \n\n    # Initialize asserter \n    semantic_assert = SemanticAssertion()\n\n    # Your LLM output\n    actual_output = \"The sky is blue\"\n\n    # Expected behavior in natural language\n    expected_behavior = \"A statement about the color of the sky\"\n\n    # Test semantic equivalence\n    semantic_assert.assert_semantic_match(\n        actual=actual_output,\n        expected_behavior=expected_behavior\n)\n</code></pre>"},{"location":"getting-started/quickstart/#3-run-your-test","title":"3. Run Your Test","text":"<pre><code>pytest my_first_semantic_test.py\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>CI/CD Integration - Set up automated testing</li> <li>Configuration - Configure llm_app_test for your needs</li> </ul>"},{"location":"getting-started/quickstart/#additional-notes","title":"Additional notes:","text":""},{"location":"getting-started/quickstart/#real-world-example","title":"Real World Example","text":"<p>Want to see llm_app_test in action? Here's a real test from our test suite:</p> <pre><code>from llm_app_test.semanticassert.semantic_assert import SemanticAssertion \nfrom llm_app_test.tests.test_content_generators.test_greeting_bot import SimpleGreetingBot\n\ndef test_greeting_semantic(): \n\n    semantic_assert = SemanticAssertion()\n\n    bot = SimpleGreetingBot()\n    actual_output = bot.generate_greeting(\"Alice\")\n\n    expected_behavior = \"\"\"\n    A polite greeting that:\n    1. Addresses the person by name (Alice)\n    2. Asks about their wellbeing\n    \"\"\"\n\n    semantic_assert.assert_semantic_match(\n        actual=actual_output,\n        expected_behavior=expected_behavior\n    )\n</code></pre> <p>You can find this example in our repository: test_greeting.py</p> <p>It can be run from this project root with the following command:</p> <pre><code>pytest tests/actual_usage_tests/test_greeting.py\n</code></pre> <p>You can find and run this example:</p> <pre><code># Clone the repository\n\ngit clone https://github.com/Shredmetal/llmtest.git\n\n# Run the test\n\npytest tests/actual_usage_tests/test_greeting.py\n</code></pre>"},{"location":"guides/best-practices/","title":"Best Practices","text":"<p>\u2190 Back to Home</p>"},{"location":"guides/best-practices/#best-practices","title":"Best Practices","text":"<p>Guidelines for effective semantic testing with llm_app_test.</p>"},{"location":"guides/best-practices/#understanding-semantic-testing","title":"Understanding Semantic Testing","text":"<p>Semantic testing focuses on meaning rather than exact matches. For example:</p> <pre><code># THESE ARE SEMANTICALLY EQUIVALENT\n\nactual_1 = \"Hello Alice, how are you today?\" \nactual_2 = \"Hi Alice! Hope you're doing well!\" \nexpected = \"A polite greeting addressing Alice\"\n</code></pre>"},{"location":"guides/best-practices/#writing-good-expected-behaviors","title":"Writing Good Expected Behaviors","text":"<ol> <li>Be Specific:</li> </ol> <pre><code># Good\n\nexpected_behavior = \"\"\" A polite greeting that:\n                    Addresses the person by name (Alice)\n                    Asks about their wellbeing \"\"\"\n\n# Bad - too vague\n\nexpected_behavior = \"A nice greeting\"\n</code></pre> <ol> <li>Focus on Requirements:</li> </ol> <pre><code># Good\n\nexpected_behavior = \"\"\" An error message that:\n                    Explains the API key is invalid\n                    Provides steps to fix the issue \"\"\"\n\n# Bad - testing exact wording - DO NOT USE llm_app_test FOR THIS, JUST USE REGULAR PYTEST\n\nexpected_behavior = \"Should say 'Invalid API key'\"\n</code></pre>"},{"location":"guides/best-practices/#when-to-use-semantic-testing","title":"When to Use Semantic Testing","text":"<p>Good Use Cases: - Testing LLM outputs - Validating natural language responses - Testing content generation</p> <p>Not Suitable For: - Exact string matching - Numerical comparisons - Binary conditions</p>"},{"location":"guides/best-practices/#test-structure","title":"Test Structure","text":"<ol> <li>Keep Tests Focused:</li> </ol> <pre><code># Good\n\ndef test_greeting_format(): \"\"\"Test greeting format only\"\"\"\n\ndef test_greeting_personalization(): \"\"\"Test name usage separately\"\"\"\n\n# Bad - testing too much\n\ndef test_everything_about_greeting(): \"\"\"Testing multiple aspects at once\"\"\"\n</code></pre> <ol> <li>Clear Test Names:</li> </ol> <pre><code># Good\n\ndef test_error_message_includes_solution_steps():\n\n# Bad\n\ndef test_error():\n</code></pre>"},{"location":"guides/best-practices/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Over-Specific Expected Behaviors:</li> </ol> <pre><code># Too specific\n\nexpected = \"Must say hello and use exactly these words\"\n\n# Better\n\nexpected = \"Should be a greeting in conversational English\"\n</code></pre> <ol> <li>Under-Specific Expected Behaviors:</li> </ol> <pre><code># Too vague\n\nexpected = \"Should be good\"\n\n# Better\n\nexpected = \"\"\" Response should:\n           Answer the user's question\n           Use professional language\n           Stay on topic \"\"\"\n</code></pre>"},{"location":"guides/best-practices/#cost-optimization","title":"Cost Optimization","text":"<ol> <li>Use specific, focused tests</li> <li>Group related semantic tests</li> <li>Consider test importance vs cost</li> <li>Use appropriate model tiers</li> </ol>"},{"location":"guides/best-practices/#closing-words","title":"Closing words","text":"<p>In general, this is a complete different type of testing designed to mimic a human testing your LLM application.  You might need to use your brain a little bit to figure out to instruct your assistant on what to check.</p>"},{"location":"guides/best-practices/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"guides/ci-cd/","title":"CI/CD Integration","text":"<p>\u2190 Back to Home</p>"},{"location":"guides/ci-cd/#cicd-integration","title":"CI/CD Integration","text":"<p>llm_app_test is designed for seamless integration with CI/CD pipelines.</p>"},{"location":"guides/ci-cd/#setting-up-cicd","title":"Setting Up CI/CD","text":""},{"location":"guides/ci-cd/#1-environment-setup","title":"1. Environment Setup","text":"<p>Add your API keys as secrets in your CI/CD environment:</p> <pre><code># GitHub Actions example\n\nenv: OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n\n# OR\n\nenv: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n</code></pre>"},{"location":"guides/ci-cd/#2-test-configuration","title":"2. Test Configuration","text":"<pre><code># tests/test_llm_app.py\n\nfrom llm_app_test.semanticassert.semantic_assert import SemanticAssertion\n\ndef test_llm_output(): \n    semantic_assert = SemanticAssertion() # Your tests here\n</code></pre>"},{"location":"guides/ci-cd/#3-cicd-pipeline-configuration","title":"3. CI/CD Pipeline Configuration","text":""},{"location":"guides/ci-cd/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: LLM Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install llm-app-test\n\n      - name: Run tests\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: pytest tests/\n</code></pre>"},{"location":"guides/ci-cd/#best-practices","title":"Best Practices","text":"<ol> <li> <p>API Key Management:</p> <ul> <li>Never commit API keys</li> <li>Use CI/CD environment secrets</li> <li>Rotate keys regularly</li> </ul> </li> <li> <p>Cost Control:</p> <ul> <li>Use cheaper models in CI/CD</li> <li>Run semantic tests only on critical paths</li> <li>Consider test result caching</li> </ul> </li> <li> <p>Pipeline Optimization:</p> <ul> <li>Run traditional tests first</li> <li>Run semantic tests in parallel</li> <li>Set appropriate timeouts</li> </ul> </li> <li> <p>Error Handling:</p> <ul> <li>Implement retry logic for API failures</li> <li>Log detailed error information</li> <li>Set up alerts for repeated failures</li> </ul> </li> </ol>"},{"location":"guides/ci-cd/#common-issues","title":"Common Issues","text":"<ol> <li> <p>API Rate Limits:</p> <ul> <li>Implement exponential backoff</li> <li>Use test result caching</li> <li>Consider parallel test execution</li> </ul> </li> <li> <p>Cost Management:</p> <ul> <li>Monitor API usage</li> <li>Set budget alerts</li> <li>Use test filtering</li> </ul> </li> </ol>"},{"location":"guides/ci-cd/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"guides/testing-patterns/","title":"Testing Patterns","text":"<p>This guide demonstrates common semantic testing patterns using llm-app-test. For API reference and syntax, see API Documentation.</p>"},{"location":"guides/testing-patterns/#pattern-basic-semantic-equivalence","title":"Pattern: Basic Semantic Equivalence","text":"<p>Scenario: Testing simple greeting behaviour</p> <p>Implementation:</p> <pre><code>actual = \"Hello Alice, how are you today?\" \n\nexpected_behavior = \"\"\" A polite greeting that:\n    Addresses the person by name (Alice)\n    Asks about their wellbeing \"\"\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> <p>Result: \u2705 PASS</p> <ul> <li>Recognises personal address</li> <li>Identifies greeting context</li> <li>Validates wellbeing inquiry</li> </ul>"},{"location":"guides/testing-patterns/#pattern-basic-semantic-matching","title":"Pattern: Basic Semantic Matching","text":"<p>Scenario: Testing simple factual statements</p> <p>Implementation:</p> <p><pre><code>actual = \"The sky is blue\"\n\nexpected_behavior = \"A statement about the color of the sky\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Shows direct semantic matching</li> <li>Clear relationship between statement and expectation</li> <li>Passes when meaning aligns</li> </ul>"},{"location":"guides/testing-patterns/#pattern-expected-semantic-mismatch","title":"Pattern: Expected Semantic Mismatch","text":"<p>Scenario: Validating semantic mismatch detection</p> <p>Implementation:</p> <p><pre><code>actual = \"The sky is blue\"\n\nexpected_behavior = \"A statement about the weather forecast\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because the actual statement is about sky colour</li> <li>Expected behaviour asks for weather forecast information</li> <li>Demonstrates how semantic mismatches are caught</li> <li>Shows when assertions will fail in your tests</li> </ul>"},{"location":"guides/testing-patterns/#pattern-multilingual-semantic-testing","title":"Pattern: Multilingual Semantic Testing","text":"<p>Scenario: Testing semantic understanding across languages</p> <p>Implementation:</p> <p><pre><code>actual = \"Bonjour, comment allez-vous?\"\n\nexpected_behavior = \"A polite greeting asking about wellbeing\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Demonstrates language-agnostic understanding</li> <li>Shows cross-language semantic matching</li> <li>Validates international content handling</li> </ul>"},{"location":"guides/testing-patterns/#pattern-technical-documentation-testing","title":"Pattern: Technical Documentation Testing","text":"<p>Scenario: Testing technical concept explanations</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"The TCP handshake is a three-way process where the client \n         sends SYN, server responds with SYN-ACK, and client confirms with ACK\"\"\"\n\nexpected_behavior = \"An explanation of the TCP connection establishment process\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Validates technical accuracy</li> <li>Handles specialised terminology</li> <li>Maintains semantic precision</li> </ul>"},{"location":"guides/testing-patterns/#pattern-contextual-disambiguation","title":"Pattern: Contextual Disambiguation","text":"<p>Scenario: Testing understanding of ambiguous terms</p> <p>Implementation:</p> <p><pre><code>actual = \"The bank was steep and covered in wildflowers\"\n\nexpected_behavior = \"A description of a riverbank or hillside, not a financial institution\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Shows contextual understanding</li> <li>Handles ambiguous terms</li> <li>Validates specific meaning exclusions</li> </ul>"},{"location":"guides/testing-patterns/#pattern-sentiment-analysis","title":"Pattern: Sentiment Analysis","text":"<p>Scenario: Testing subtle emotional content</p> <p>Implementation:</p> <p><pre><code>actual = \"While the presentation wasn't perfect, it showed promise\"\n\nexpected_behavior = \"A constructive criticism with mixed but generally positive sentiment\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Detects nuanced sentiment</li> <li>Understands mixed emotions</li> <li>Validates overall tone</li> </ul>"},{"location":"guides/testing-patterns/#pattern-long-form-content","title":"Pattern: Long-Form Content","text":"<p>Scenario: Testing comprehension of detailed explanations</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"Machine learning is a subset of artificial intelligence \n         that enables systems to learn and improve from experience without \n         explicit programming. It focuses on developing computer programs \n         that can access data and use it to learn for themselves.\"\"\"\n\nexpected_behavior = \"A comprehensive definition of machine learning emphasizing autonomous learning and data usage\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles longer text</li> <li>Maintains context</li> <li>Captures key concepts</li> </ul>"},{"location":"guides/testing-patterns/#pattern-subtle-sentiment-mismatch","title":"Pattern: Subtle Sentiment Mismatch","text":"<p>Scenario: Testing detection of subtle sentiment differences</p> <p>Implementation:</p> <p><pre><code>actual = \"The project was completed on time, though there were some hiccups\"\n\nexpected_behavior = \"A statement expressing complete satisfaction with project execution\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because actual statement indicates mixed satisfaction</li> <li>Expected behaviour suggests complete satisfaction</li> <li>Shows sensitivity to subtle emotional differences</li> </ul>"},{"location":"guides/testing-patterns/#pattern-technical-context-mismatch","title":"Pattern: Technical Context Mismatch","text":"<p>Scenario: Testing technical meaning precision</p> <p>Implementation:</p> <p><pre><code>actual = \"The function returns a pointer to the memory address\"\n\nexpected_behavior = \"A description of a function that returns the value stored at a memory location\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because returning a pointer is different from returning a stored value</li> <li>Shows precision in technical context</li> <li>Validates technical accuracy</li> </ul>"},{"location":"guides/testing-patterns/#pattern-ambiguous-reference-testing","title":"Pattern: Ambiguous Reference Testing","text":"<p>Scenario: Testing handling of context-dependent terms</p> <p>Implementation:</p> <p><pre><code>actual = \"The bank processed the transaction after reviewing the account\"\n\nexpected_behavior = \"A description of a riverbank's geological formation process\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because contexts are completely different</li> <li>Shows strong contextual understanding</li> <li>Validates semantic boundaries</li> </ul>"},{"location":"guides/testing-patterns/#pattern-temporal-context","title":"Pattern: Temporal Context","text":"<p>Scenario: Testing time-based semantic understanding</p> <p>Implementation:</p> <p><pre><code>actual = \"I will have completed the task by tomorrow\"\n\nexpected_behavior = \"A statement about a task that was completed in the past\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because of tense mismatch</li> <li>Shows temporal awareness</li> <li>Validates time context</li> </ul>"},{"location":"guides/testing-patterns/#pattern-logical-implication","title":"Pattern: Logical Implication","text":"<p>Scenario: Testing logical relationship understanding</p> <p>Implementation:</p> <p><pre><code>actual = \"If it rains, the ground will be wet\"\n\nexpected_behavior = \"A statement indicating that wet ground always means it has rained\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because of reversed logical implication</li> <li>Shows logical relationship understanding</li> <li>Validates causality direction</li> </ul>"},{"location":"guides/testing-patterns/#pattern-multi-step-reasoning","title":"Pattern: Multi-Step Reasoning","text":"<p>Scenario: Testing complex logical chains</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"When water freezes, it expands by approximately 9% in volume. \nThis expansion creates less dense ice that floats according to Archimedes' principle of displacement. \nBecause Arctic sea ice is already floating in the ocean, its melting doesn't significantly affect sea levels - \nit's already displacing its weight in water. However, land-based glaciers in places like Greenland \naren't currently displacing any ocean water. When these glaciers melt, they add entirely new water volume \nto the oceans, making them a primary contributor to sea level rise.\"\"\"\n\nexpected_behavior = \"\"\"A multi-step scientific explanation.\nMust maintain logical consistency across all steps.\"\"\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles complex logical chains</li> <li>Maintains consistency across steps</li> <li>Validates scientific reasoning</li> </ul>"},{"location":"guides/testing-patterns/#pattern-nonsensical-content","title":"Pattern: Nonsensical Content","text":"<p>Scenario: Testing handling of grammatically correct but meaningless content</p> <p>Implementation:</p> <p><pre><code>actual = \"The colorless green ideas sleep furiously\"\n\nexpected_behavior = \"A grammatically correct but semantically nonsensical statement\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Recognizes grammatical structure</li> <li>Identifies semantic nonsense</li> <li>Validates meta-understanding</li> </ul>"},{"location":"guides/testing-patterns/#pattern-extended-narrative","title":"Pattern: Extended Narrative","text":"<p>Scenario: Testing long-form narrative understanding</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"\n        The Roman Empire's rise began with modest origins in central Italy. What started as a small \n        settlement along the Tiber River would eventually become one of history's most influential \n        civilizations. In the early days, Rome was ruled by kings, but this system was overthrown \n        in 509 BCE, giving birth to the Roman Republic.\n\n        During the Republic, Rome expanded its territory through military conquest and diplomatic \n        alliances. The Roman army became increasingly professional, developing innovative tactics \n        and technologies. This military success brought wealth and power, but also internal \n        challenges. Social tensions grew between patricians and plebeians, leading to significant \n        political reforms.\n\n        By the 1st century BCE, the Republic faced severe internal strife. Military commanders \n        like Marius, Sulla, and eventually Julius Caesar accumulated unprecedented power. Caesar's \n        crossing of the Rubicon in 49 BCE marked a point of no return. His assassination in 44 BCE \n        led to another civil war, ultimately resulting in his adopted heir Octavian becoming \n        Augustus, the first Roman Emperor.\n\n        Augustus transformed Rome into an empire while maintaining a facade of republican \n        institutions. He implemented sweeping reforms in administration, military organization, \n        and public works. The Pax Romana that followed brought unprecedented peace and prosperity \n        across the Mediterranean world. Trade flourished, cities grew, and Roman culture spread \n        throughout the empire.\n        \"\"\"\n\nexpected_behavior = \"\"\"A historical narrative that:\n1. Maintains chronological progression\n2. Shows cause-and-effect relationships\n3. Develops consistent themes (power, governance, military)\n4. Connects multiple historical events coherently\n5. Demonstrates character development (e.g., Caesar to Augustus)\n\"\"\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles extended narratives</li> <li>Maintains thematic consistency</li> <li>Validates complex relationships</li> <li>Shows chronological understanding</li> </ul>"},{"location":"guides/testing-patterns/#pattern-emoji-quantity-testing","title":"Pattern: Emoji Quantity Testing","text":"<p>Scenario: Testing recognition of repeated emojis</p> <p>Implementation:</p> <pre><code>actual = \"\ud83e\udd16\" * 100\n\nexpected_behavior = \"A lot of emojis\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> <p>Result: \u2705 PASS</p> <ul> <li>Handles repeated Unicode characters</li> <li>Recognises quantity concepts</li> <li>Validates emoji processing</li> </ul>"},{"location":"guides/testing-patterns/#pattern-emoji-quantity-mismatch","title":"Pattern: Emoji Quantity Mismatch","text":"<p>Scenario: Testing quantity recognition accuracy</p> <p>Implementation:</p> <p><pre><code>actual = \"\ud83e\udd16\" * 100\n\nexpected_behavior = \"Only a few emojis\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails due to quantity mismatch</li> <li>Shows quantity awareness</li> <li>Validates numerical understanding</li> </ul>"},{"location":"guides/testing-patterns/#pattern-mixed-unicode-content-known-reliability-issue","title":"Pattern: Mixed Unicode Content \u26a0\ufe0f Known Reliability Issue","text":"<p>Scenario: Testing complex Unicode combinations and repetitive patterns</p>"},{"location":"guides/testing-patterns/#observed-behavior","title":"Observed Behavior","text":"<p>Test Case 1: Strict Pattern Matching</p> <pre><code>actual = \"\ud83e\udd16\ud83d\udc7e\" * 50 + \"\u3053\u3093\u306b\u3061\u306f\" * 20 + \"\ud83c\udf08\" * 30\n\nexpected_behavior = \"A mix of emojis and Japanese text\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> <p>Results:</p> <ul> <li>\u2705 Success Rate: 96% (48/50 runs)</li> <li>\u274c Failure Rate: 4% (2/50 runs)</li> <li>\ud83d\udd0d Failure Analysis:<ul> <li>Occurs primarily during increased API latency</li> <li>GPT-4o occasionally interprets sequential patterns as \"distinct collections\" rather than \"mixed content\"</li> <li>Failure message example: \"This is not a mix as there is a distinct collection of emojis followed by Japanese text and then a collection of rainbows\"</li> </ul> </li> </ul>"},{"location":"guides/testing-patterns/#recommended-implementation","title":"Recommended Implementation","text":"<p>Test Case 2: Pattern-Agnostic Matching</p> <pre><code>actual = \"\ud83e\udd16\ud83d\udc7e\" * 50 + \"\u3053\u3093\u306b\u3061\u306f\" * 20 + \"\ud83c\udf08\" * 30\n\nexpected = \"More than one type of emoji and Japanese text regardless of order\"\n\nasserter.assert_semantic_match(actual, expected)\n</code></pre> <p>Results:</p> <ul> <li>\u2705 Success Rate: 100% (preliminary)</li> <li>\u26a0\ufe0f Extended testing in progress</li> <li>\ud83d\udd0d Monitoring prompt effectiveness across different test scenarios</li> </ul>"},{"location":"guides/testing-patterns/#best-practices","title":"Best Practices","text":"<ol> <li>Use pattern-agnostic assertions for repetitive Unicode content</li> <li>Consider implementing retry logic for critical tests</li> <li>Monitor API response times during failures</li> <li>Use enhanced prompts for complex Unicode pattern testing</li> </ol>"},{"location":"guides/testing-patterns/#ongoing-investigation","title":"Ongoing Investigation","text":"<ul> <li>Testing various prompt configurations to improve reliability</li> <li>Monitoring performance impact of different prompt strategies</li> <li>Collecting data on failure patterns with different prompt versions</li> </ul>"},{"location":"guides/testing-patterns/#pattern-multilingual-emoji-spam","title":"Pattern: Multilingual Emoji Spam","text":"<p>Scenario: Testing repeated multilingual content</p> <p>Implementation:</p> <p><pre><code>actual = \"Hello\u4f60\u597dBonjour\ud83c\udf08\" * 50\n\nexpected_behavior = \"A repetitive greeting in multiple languages with rainbows\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles multilingual text</li> <li>Recognises repetitive patterns</li> <li>Validates mixed content types</li> </ul>"},{"location":"guides/testing-patterns/#pattern-ascii-art-recognition","title":"Pattern: ASCII Art Recognition","text":"<p>Scenario: Testing complex ASCII art patterns</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"\n(\u256f\u00b0\u25a1\u00b0)\u256f\ufe35 \u253b\u2501\u253b\n\"\"\" * 20\n\nexpected_behavior = \"Multiple instances of table-flipping ASCII art\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Recognises ASCII art patterns</li> <li>Understands visual representations</li> <li>Validates repeated patterns</li> </ul>"},{"location":"guides/testing-patterns/#pattern-extreme-whitespace","title":"Pattern: Extreme Whitespace","text":"<p>Scenario: Testing handling of excessive spacing</p> <p>Implementation:</p> <p><pre><code>actual = \"hello    \" + \" \" * 1000 + \"    world\" + \"\\n\" * 500\n\nexpected_behavior = \"A greeting with excessive spacing\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles extreme whitespace</li> <li>Maintains semantic meaning</li> <li>Validates text normalisation</li> </ul>"},{"location":"guides/testing-patterns/#pattern-number-pattern-recognition","title":"Pattern: Number Pattern Recognition","text":"<p>Scenario: Testing numerical pattern understanding</p> <p>Implementation:</p> <p><pre><code>actual = \"\".join([str(i % 10) for i in range(1000)])\n\nexpected_behavior = \"A long sequence of repeating numbers\"\n\nsemantic_assert.assert_semantic_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Recognises numerical patterns</li> <li>Handles long repetitive sequences</li> <li>Validates pattern understanding</li> </ul>"},{"location":"reliability_testing/format_compliance/","title":"Format Compliance","text":"<p>\u2190 Back to Home</p>"},{"location":"reliability_testing/format_compliance/#format-compliance-comprehensive-industry-testing","title":"Format Compliance - Comprehensive Industry Testing","text":""},{"location":"reliability_testing/format_compliance/#overview","title":"Overview","text":"<p>This page documents our extensive format compliance testing of llm-app-test. We designed these tests to validate the library's ability to maintain consistent format requirements across diverse use cases.</p> <p>The test suite demonstrated 100% reliability across 13,000 test executions, with zero format violations detected.</p> <p>The relevant logs can be found here.</p>"},{"location":"reliability_testing/format_compliance/#test-suite-design","title":"Test Suite Design","text":"<p>The test suite covers 13 distinct test cases:</p> <ul> <li>8 positive test cases</li> <li>5 negative test cases</li> </ul> <p>Each test validates different aspects of format compliance across various content types:</p> <ol> <li>Multilingual equivalence</li> <li>Complex technical explanations</li> <li>Contextual understanding</li> <li>Complex sentiment analysis</li> <li>Long-form comparisons</li> <li>Subtle sentiment matching</li> <li>Technical context validation</li> <li>Ambiguous reference handling</li> <li>Temporal context verification</li> <li>Logical implication testing</li> <li>Complex multi-hop reasoning</li> <li>Adversarial content handling</li> <li>Long context understanding</li> </ol>"},{"location":"reliability_testing/format_compliance/#test-characteristics","title":"Test Characteristics","text":"<p>Each test was designed to validate:</p> <ul> <li>Format compliance</li> <li>Response structure</li> <li>Error handling</li> <li>Edge cases</li> <li>Content variations</li> </ul>"},{"location":"reliability_testing/format_compliance/#test-results","title":"Test Results","text":""},{"location":"reliability_testing/format_compliance/#format-compliance","title":"Format Compliance","text":"<ul> <li>Zero format violations across 13,000 executions</li> <li>Consistent PASS/FAIL behavior</li> <li>Proper error handling for negative cases</li> </ul>"},{"location":"reliability_testing/format_compliance/#response-reliability","title":"Response Reliability","text":"<ul> <li>Consistent format across all test types</li> <li>Proper error message formatting</li> <li>Stable behavior across multiple runs</li> </ul>"},{"location":"reliability_testing/format_compliance/#cost-analysis","title":"Cost Analysis","text":"<p>Running 13,000 tests using GPT-4o cost approximately US$7.90, demonstrating the economic viability of comprehensive format testing even with large test suites.</p>"},{"location":"reliability_testing/format_compliance/#test-configuration","title":"Test Configuration","text":"<p>All tests used library defaults:</p> <p><pre><code>LLM_PROVIDER=openai\nLLM_MODEL=gpt-4o \nLLM_TEMPERATURE=0.0 \nLLM_MAX_TOKENS=4096 \nLLM_MAX_RETRIES=2 \nLLM_TIMEOUT=10.0 # Added for OpenAI in 0.1.0b5 using the underlying Langchain implementation \n</code></pre> The <code>semantic_assert_match</code> function also saw slight modification:</p> <pre><code>        if result.startswith(\"FAIL\"):\n            raise SemanticAssertionError(\n                \"Semantic assertion failed\",\n                reason=result.split(\"FAIL: \")[1]\n            )\n\n        # Section below added to cause failure in the event of format violation    \n\n        elif result.startswith(\"PASS\"):\n            pass\n        else:\n            raise RuntimeError(\n                f\"Format Non-compliance Detected {result}\"\n            )\n</code></pre> <p>The prompts to the asserter LLM (that sits behind <code>semantic_assert_match</code>) were:</p> <pre><code>DEFAULT_SYSTEM_PROMPT = \"\"\"You are a testing system. Your job is to determine if an actual output matches the expected behavior.\n\nImportant: You can only respond with EXACTLY: \n1. 'PASS' if it matches, or \n2. 'FAIL: &lt;reason&gt;' if it doesn't match.\n\nAny other type of response will mean disaster which as a testing system, you are meant to prevent.\n\nBe strict but consider semantic meaning rather than exact wording.\"\"\"\n\nDEFAULT_HUMAN_PROMPT = \"\"\"\nExpected Behavior: {expected_behavior}\n\nActual Output: {actual}\n\nDoes the actual output match the expected behavior? Remember, you will fail your task unless you respond EXACTLY \nwith 'PASS' or 'FAIL: &lt;reason&gt;'.\"\"\"\n</code></pre>"},{"location":"reliability_testing/format_compliance/#testing-cost","title":"Testing Cost","text":"<p>It actually cost us very little.</p> <p>This is likely due to how we constrained the model: The temperature is set to 0.0, and it has strict instructions to only respond with \"PASS\" or \"FAIL \". This constrains the much more expensive output token usage. <p>Running the test suite of 13 tests below 100 times (so 1,300 tests), cost us a total of US$0.79, using OpenAI's GPT-4o.</p> <p>Upon observing the cost of throwing thousands of API calls at OpenAI, we decided to just sod it and throw 13,000 API calls at them for the grand sum of... US$7.90. </p> <p>The purpose of these 13,000 runs was primarily to test the reliability of this library insofar as format violations were concerned, and get some insight as to the reliability of the semantic comparison.</p> <p>Based on the testing documented in this page however, we are quite confident that llm-app-test will adhere to the format requirements in most situations and not throw stupid errors by failing to adhere to the requirements. </p> <p>Please refer to the other pages of testing reliability, specifically Semantic Reliability Testing and Semantic Boundary Analysis for more information on reliability testing of the ability of this library to test for semantic equivalence.</p>"},{"location":"reliability_testing/format_compliance/#test-suite","title":"Test Suite","text":"<p>We used the following test suite for the purposes of format compliance testing:</p> <pre><code>import os\nimport pytest\nfrom llm_app_test.semantic_assert.semantic_assert import SemanticAssertion\nfrom llm_app_test.exceptions.test_exceptions import (\n    SemanticAssertionError\n)\n\n\nclass TestComplexSemanticAssertion:\n    @pytest.fixture\n    def asserter(self):\n        return SemanticAssertion()\n\n    def test_multilingual_equivalence(self, asserter):\n        \"\"\"Test semantic matching across different languages\"\"\"\n        actual = \"Bonjour, comment allez-vous?\"\n        expected = \"A polite greeting asking about wellbeing\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_complex_technical_explanation(self, asserter):\n        \"\"\"Test matching of technical explanations\"\"\"\n        actual = \"\"\"The TCP handshake is a three-way process where the client \n                 sends SYN, server responds with SYN-ACK, and client confirms with ACK\"\"\"\n        expected = \"An explanation of the TCP connection establishment process\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_contextual_understanding(self, asserter):\n        \"\"\"Test understanding of context-dependent statements\"\"\"\n        actual = \"The bank was steep and covered in wildflowers\"\n        expected = \"A description of a riverbank or hillside, not a financial institution\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_complex_sentiment_analysis(self, asserter):\n        \"\"\"Test understanding of subtle emotional content\"\"\"\n        actual = \"While the presentation wasn't perfect, it showed promise\"\n        expected = \"A constructive criticism with mixed but generally positive sentiment\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_long_form_comparison(self, asserter):\n        \"\"\"Test handling of longer text passages\"\"\"\n        actual = \"\"\"Machine learning is a subset of artificial intelligence \n                 that enables systems to learn and improve from experience without \n                 explicit programming. It focuses on developing computer programs \n                 that can access data and use it to learn for themselves.\"\"\"\n        expected = \"A comprehensive definition of machine learning emphasizing autonomous learning and data usage\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_subtle_sentiment_mismatch(self, asserter):\n        \"\"\"Test mismatch in subtle sentiment differences\"\"\"\n        actual = \"The project was completed on time, though there were some hiccups\"\n        expected = \"A statement expressing complete satisfaction with project execution\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_technical_context_mismatch(self, asserter):\n        \"\"\"Test mismatch in technical context interpretation\"\"\"\n        actual = \"The function returns a pointer to the memory address\"\n        expected = \"A description of a function that returns the value stored at a memory location\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_ambiguous_reference_mismatch(self, asserter):\n        \"\"\"Test mismatch in ambiguous references\"\"\"\n        actual = \"The bank processed the transaction after reviewing the account\"\n        expected = \"A description of a riverbank's geological formation process\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_temporal_context_mismatch(self, asserter):\n        \"\"\"Test mismatch in temporal context\"\"\"\n        actual = \"I will have completed the task by tomorrow\"\n        expected = \"A statement about a task that was completed in the past\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_logical_implication_mismatch(self, asserter):\n        \"\"\"Test mismatch in logical implications\"\"\"\n        actual = \"If it rains, the ground will be wet\"\n        expected = \"A statement indicating that wet ground always means it has rained\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_complex_multi_hop_reasoning(self, asserter):\n        \"\"\"Test complex multi-hop reasoning chains\"\"\"\n        actual = \"\"\"When water freezes, it expands by approximately 9% in volume. \n        This expansion creates less dense ice that floats according to Archimedes' principle of displacement. \n        Because Arctic sea ice is already floating in the ocean, its melting doesn't significantly affect sea levels - \n        it's already displacing its weight in water. However, land-based glaciers in places like Greenland \n        aren't currently displacing any ocean water. When these glaciers melt, they add entirely new water volume \n        to the oceans, making them a primary contributor to sea level rise.\"\"\"\n\n        expected = \"\"\"A multi-step scientific explanation.\n        Must maintain logical consistency across all steps.\"\"\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_adversarial_content(self, asserter):\n        \"\"\"Test handling of deliberately ambiguous or contradictory content\"\"\"\n        actual = \"The colorless green ideas sleep furiously\"\n        expected = \"A grammatically correct but semantically nonsensical statement\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_long_context_understanding(self, asserter):\n        \"\"\"Test understanding of long, interconnected narratives\"\"\"\n        actual = \"\"\"\n        The Roman Empire's rise began with modest origins in central Italy. What started as a small \n        settlement along the Tiber River would eventually become one of history's most influential \n        civilizations. In the early days, Rome was ruled by kings, but this system was overthrown \n        in 509 BCE, giving birth to the Roman Republic.\n\n        During the Republic, Rome expanded its territory through military conquest and diplomatic \n        alliances. The Roman army became increasingly professional, developing innovative tactics \n        and technologies. This military success brought wealth and power, but also internal \n        challenges. Social tensions grew between patricians and plebeians, leading to significant \n        political reforms.\n\n        By the 1st century BCE, the Republic faced severe internal strife. Military commanders \n        like Marius, Sulla, and eventually Julius Caesar accumulated unprecedented power. Caesar's \n        crossing of the Rubicon in 49 BCE marked a point of no return. His assassination in 44 BCE \n        led to another civil war, ultimately resulting in his adopted heir Octavian becoming \n        Augustus, the first Roman Emperor.\n\n        Augustus transformed Rome into an empire while maintaining a facade of republican \n        institutions. He implemented sweeping reforms in administration, military organization, \n        and public works. The Pax Romana that followed brought unprecedented peace and prosperity \n        across the Mediterranean world. Trade flourished, cities grew, and Roman culture spread \n        throughout the empire.\n        \"\"\"\n        expected = \"\"\"A historical narrative that:\n        1. Maintains chronological progression\n        2. Shows cause-and-effect relationships\n        3. Develops consistent themes (power, governance, military)\n        4. Connects multiple historical events coherently\n        5. Demonstrates character development (e.g., Caesar to Augustus)\n        \"\"\"\n        asserter.assert_semantic_match(actual, expected)\n</code></pre>"},{"location":"reliability_testing/format_compliance/#issue-reporting","title":"Issue reporting","text":"<p>If you experience any issues, especially with library reliability - please let us know, thanks!</p>"},{"location":"reliability_testing/format_compliance/#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>API Reference</li> </ul>"},{"location":"reliability_testing/semantic_boundary_analysis/","title":"Semantic Boundary Analysis","text":""},{"location":"reliability_testing/semantic_boundary_analysis/#overview","title":"Overview","text":"<p>During our real-world style testing, we encountered an interesting semantic boundary case that highlighted both the sophistication and limitations of LLM-based testing. </p> <p>This was discovered during the initial 500 runs of our real-world test suite, as only this one test demonstrated non-determinism.</p> <p>This case provides valuable insights into how LLMs interpret semantic requirements.</p>"},{"location":"reliability_testing/semantic_boundary_analysis/#the-case","title":"The Case","text":"<p>This is our healthcare test case:</p> <pre><code>def test_patient_education_diabetes_management(self, asserter):\n        \"\"\"Test semantic matching for patient education content about diabetes management. Failure is expected because\n        this does not contain emergency response steps.\"\"\"\n        actual = \"\"\"\n        Understanding and Managing Type 2 Diabetes\n\n        Type 2 diabetes is a chronic condition that affects how your body processes blood sugar (glucose). \n        While this condition is serious, it can be effectively managed through lifestyle changes and, \n        when necessary, medication. This guide will help you understand the key aspects of diabetes \n        management.\n\n        Blood Sugar Monitoring:\n        Regular blood sugar monitoring is essential. Your target blood glucose levels should typically \n        be 80-130 mg/dL before meals and less than 180 mg/dL two hours after meals. However, your \n        healthcare provider may set different targets based on your individual needs. Keep a log of \n        your readings to identify patterns and adjust your management strategy accordingly.\n\n        Dietary Considerations:\n        A balanced diet is crucial for managing type 2 diabetes. Focus on:\n        - Controlling portion sizes\n        - Choosing high-fiber, low-glycemic foods\n        - Limiting refined carbohydrates and processed sugars\n        - Including lean proteins and healthy fats\n        - Spacing meals evenly throughout the day\n\n        Physical Activity:\n        Regular exercise helps control blood sugar levels by improving insulin sensitivity. Aim for:\n        - At least 150 minutes of moderate-intensity aerobic activity weekly\n        - Resistance training 2-3 times per week\n        - Daily movement, even if just short walks\n        Always check your blood sugar before and after exercise, and carry a fast-acting \n        carbohydrate source.\n\n        Medication Management:\n        If prescribed, take diabetes medications as directed. Common medications include:\n        - Metformin (helps reduce glucose production)\n        - Sulfonylureas (increase insulin production)\n        - DPP-4 inhibitors (help maintain blood sugar control)\n        Never adjust or stop medications without consulting your healthcare provider.\n\n        Warning Signs:\n        Learn to recognize and respond to:\n        - Hypoglycemia (low blood sugar): shakiness, sweating, confusion\n        - Hyperglycemia (high blood sugar): increased thirst, frequent urination, fatigue\n        Seek immediate medical attention if you experience severe symptoms or sustained \n        high blood sugar levels.\n\n        Regular Health Monitoring:\n        Schedule regular check-ups with your healthcare team, including:\n        - HbA1c tests every 3-6 months\n        - Annual eye examinations\n        - Regular foot checks\n        - Kidney function tests\n        - Cholesterol level monitoring\n\n        Remember, diabetes management is a journey, not a destination. Small, consistent \n        steps in the right direction can lead to significant improvements in your health \n        and quality of life.\n        \"\"\"\n\n        expected = \"\"\"A medical education document that must:\n        1. Contain an overview section explaining the condition\n        2. List specific numerical guidelines (blood sugar ranges, exercise minutes)\n        3. Include structured sections for diet, exercise, and medication\n        4. Provide clear warning signs AND detailed emergency response procedures\n        5. End with follow-up care instructions\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n</code></pre> <p>The relevant portions are here as follows:</p> <p>In <code>actual</code> (i.e. the simulated LLM output): <pre><code>Warning Signs:\nLearn to recognize and respond to:\n- Hypoglycemia (low blood sugar): shakiness, sweating, confusion\n- Hyperglycemia (high blood sugar): increased thirst, frequent urination, fatigue\nSeek immediate medical attention if you experience severe symptoms or sustained \nhigh blood sugar levels.\n</code></pre> In <code>expected_behavior</code> (i.e. the specification we expect developers to write when using our library):</p> <p>Version that led to non-determinism: <pre><code>4. Provide clear warning signs and emergency response steps\n</code></pre></p> <p>Updated version that led to determinism: <pre><code>4. Provide clear warning signs AND detailed emergency response procedures\n</code></pre></p>"},{"location":"reliability_testing/semantic_boundary_analysis/#test-configuration","title":"Test Configuration","text":"<p>All tests used library defaults:</p> <p><pre><code>LLM_PROVIDER=openai\nLLM_MODEL=gpt-4o \nLLM_TEMPERATURE=0.0 \nLLM_MAX_TOKENS=4096 \nLLM_MAX_RETRIES=2 \nLLM_TIMEOUT=10.0 # Added for OpenAI in 0.1.0b5 using the underlying Langchain implementation \n</code></pre> The <code>semantic_assert_match</code> function also saw slight modification:</p> <pre><code>        if result.startswith(\"FAIL\"):\n            raise SemanticAssertionError(\n                \"Semantic assertion failed\",\n                reason=result.split(\"FAIL: \")[1]\n            )\n\n        # Section below added to cause failure in the event of format violation    \n\n        elif result.startswith(\"PASS\"):\n            pass\n        else:\n            raise RuntimeError(\n                f\"Format Non-compliance Detected {result}\"\n            )\n</code></pre> <p>The prompts to the asserter LLM (that sits behind <code>semantic_assert_match</code>) were:</p> <pre><code>DEFAULT_SYSTEM_PROMPT = \"\"\"You are a testing system. Your job is to determine if an actual output matches the expected behavior.\n\nImportant: You can only respond with EXACTLY: \n1. 'PASS' if it matches, or \n2. 'FAIL: &lt;reason&gt;' if it doesn't match.\n\nAny other type of response will mean disaster which as a testing system, you are meant to prevent.\n\nBe strict but consider semantic meaning rather than exact wording.\"\"\"\n\nDEFAULT_HUMAN_PROMPT = \"\"\"\nExpected Behavior: {expected_behavior}\n\nActual Output: {actual}\n\nDoes the actual output match the expected behavior? Remember, you will fail your task unless you respond EXACTLY \nwith 'PASS' or 'FAIL: &lt;reason&gt;'.\"\"\"\n</code></pre>"},{"location":"reliability_testing/semantic_boundary_analysis/#testing-results","title":"Testing Results","text":"<p>This particular test was run 600 times.</p> <p>It was run 500 times in a wider test of the entire suite it is a part of and 100 times on its own with the following results:</p> <ul> <li>PASS: 586</li> <li>FAIL: 14</li> </ul> <p>The 100 individual tests were run when the non-determinism was noticed.</p> <p>API response times were noticeable faster when it started failing. </p> <p>Test suite execution times for 100 iterations (so 1,000 individual tests):</p> <ul> <li>~1550s - 1750s when it appeared to deterministically pass the test</li> <li>~1350s when it began to exhibit non-determinism</li> </ul> <p>We are unable to determine what longer API response times mean, and unless if OpenAI wades in, it is unlikely that we will find out.</p> <p>Logs can be found in the reliability_testing_real_world directory of the 0.1.0b5 branch of the github repo.</p>"},{"location":"reliability_testing/semantic_boundary_analysis/#analysis","title":"Analysis","text":"<p>Upon careful analysis (Library author's note: I had to read the test case several times to pick up on it, and I'm a lawyer by training with 3 years of litigation experience), we identified a subtle but critical semantic boundary in the original requirement:</p> <ol> <li> <p>The Semantic Boundary:</p> </li> <li> <p>Original text: \"emergency response steps\" (plural)</p> </li> <li>Test content provided: \"seek medical attention\" (singular)</li> <li> <p>Question: Does a single, obvious response step satisfy a requirement for \"steps\"?</p> </li> <li> <p>The Test Content:</p> </li> <li> <p>Clearly provided warning signs</p> </li> <li>Had one emergency response (\"seek medical attention\")</li> <li> <p>Lacked detailed, multiple-step emergency procedures</p> </li> <li> <p>Interpretation Analysis:</p> </li> </ol> <p>A. Argument for Single-Step Sufficiency:</p> <ul> <li>The content is patient-focused, not for medical professionals</li> <li>\"Seek medical attention\" is a complete, actionable instruction</li> <li>In an emergency, simplicity and clarity are paramount</li> <li>Additional steps might confuse or delay the critical action</li> </ul> <p>B. Argument for Multiple-Step Requirement:</p> <ul> <li>The plural \"steps\" grammatically implies multiple procedures</li> <li>Medical context demands comprehensive guidance</li> <li>\"Seek medical attention\" is too obvious to constitute meaningful instruction</li> <li>Patients need interim steps while medical help is en route</li> </ul> <p>C. Resolution:</p> <ul> <li> <p>While both interpretations have merit, the requirement for multiple steps is substantially stronger because:</p> <ol> <li>The plural form explicitly requests multiple procedures</li> <li>Medical documentation should err on the side of completeness</li> <li>Interim guidance can be critical during emergency response</li> <li>Self-evident instructions add no value to emergency protocols</li> </ol> </li> <li> <p>Key Insight:</p> </li> </ul> <p>The non-deterministic behavior of the LLM in this case reveals a genuine semantic boundary in medical documentation - the balance between simplicity and comprehensiveness in emergency instructions.</p>"},{"location":"reliability_testing/semantic_boundary_analysis/#key-learnings","title":"Key Learnings","text":"<ol> <li> <p>LLM Sophistication:</p> </li> <li> <p>The LLM detected the semantic ambiguity</p> </li> <li>Demonstrated surprisingly strong interpretation capabilities</li> <li> <p>Highlighted the importance of precise requirements</p> </li> <li> <p>Testing Implications:</p> </li> <li> <p>Requirements must be unambiguous</p> </li> <li>Precision in language can improve test reliability (seriously, this is natural language, you can throw writing the <code>expected_behavior</code> to the non-technical PM)</li> <li>Literal interpretations should ALWAYS be preferred. Try to think like a lawyer - it's how I picked up why the test case was being incorrectly accepted (note that I am testing a negative test, so FAIL means that it was incorrectly passed by <code>assert_semantic_match</code>)</li> </ol>"},{"location":"reliability_testing/semantic_boundary_analysis/#impact-on-library-usage","title":"Impact on Library Usage","text":"<p>When writing semantic test cases:</p> <ol> <li>Be explicit about conjunctive requirements</li> <li>Use \"AND\" when both elements are required</li> <li>Consider potential semantic ambiguities</li> <li>Test requirements for potential boundary cases</li> </ol>"},{"location":"reliability_testing/semantic_boundary_analysis/#quick-links","title":"Quick Links","text":"<ul> <li>Semantic Reliability Testing</li> <li>Format Compliance</li> <li>Quick Start</li> </ul>"},{"location":"reliability_testing/semantic_reliability/","title":"Semantic Reliability - Real-World Industry Specific Testing","text":""},{"location":"reliability_testing/semantic_reliability/#overview","title":"Overview","text":"<p>This page documents our extensive testing of llm-app-test against real-world industry use cases. We designed this test suite to validate the library's reliability with the kind of content that LLM applications would probably generate in production environments.</p> <p>The first test case in this suite initially exhibited non-determinism due to a legitimate semantic boundary. As the library's author (a lawyer by training with three years of litigation experience), I had to carefully analyse the semantic nuances to identify the exact boundary condition. This analysis and its implications are documented in detail here.</p> <p>The remaining 9 test cases maintained 100% consistency across 1,700 runs. For simplicity and clarity in this documentation, we focus on the 1,200 runs where all 10 cases in the suite achieved 100% pass rate.</p> <p>The relevant logs for the testing covered by this page can be found here.</p>"},{"location":"reliability_testing/semantic_reliability/#test-suite-design","title":"Test Suite Design","text":"<p>The test suite covers 10 key industries where LLMs are likely already seeing active use:</p> <ol> <li>Healthcare (Patient Education)</li> <li>Financial Services (Portfolio Reporting)</li> <li>Media/Entertainment (Content Recommendations)</li> <li>Legal (Document Summarization)</li> <li>Manufacturing (Maintenance Prediction)</li> <li>E-commerce (Product Descriptions)</li> <li>Education (Assignment Feedback)</li> <li>Real Estate (Property Listings)</li> <li>Human Resources (Interview Feedback)</li> <li>Customer Service (Ticket Response)</li> </ol> <p>The Test Suite consisted of 5 positive cases and 5 negative cases.</p>"},{"location":"reliability_testing/semantic_reliability/#testing-scale","title":"Testing Scale","text":"<ul> <li>Total Runs: 1,200 (600 Windows, 600 Linux (Pop_OS))</li> <li>Tests per Run: 10</li> <li>Total Test Executions: 12,000</li> <li>Pass Rate: 100%</li> </ul> <p>Cross-references:</p> <ul> <li>See Test Configuration for setup details</li> <li>See Test Results for detailed analysis</li> <li>Full test logs available in reliability_testing</li> </ul>"},{"location":"reliability_testing/semantic_reliability/#test-characteristics","title":"Test Characteristics","text":"<p>Each test was designed to reflect:</p> <ul> <li>Realistic content length</li> <li>Industry-specific terminology</li> <li>Common formatting patterns</li> <li>Typical validation requirements</li> <li>Real-world edge cases</li> <li>Claude 3.5 Sonnet was used to generate the test cases to make it more realistic, but testing was done with GPT-4o</li> </ul>"},{"location":"reliability_testing/semantic_reliability/#test-results","title":"Test Results","text":""},{"location":"reliability_testing/semantic_reliability/#format-compliance","title":"Format Compliance","text":"<ul> <li>Zero format violations across 12,000 executions</li> <li>Consistent PASS/FAIL behaviour</li> </ul>"},{"location":"reliability_testing/semantic_reliability/#cross-platform-reliability","title":"Cross-Platform Reliability","text":"<ul> <li>Identical behavior on Windows and Linux</li> <li>No platform-specific issues detected</li> </ul>"},{"location":"reliability_testing/semantic_reliability/#content-processing","title":"Content Processing","text":"<ul> <li>Successfully handled varying content lengths</li> <li>Maintained accuracy across different domains</li> <li>Consistent behaviour with specialised terminology</li> </ul>"},{"location":"reliability_testing/semantic_reliability/#cost-analysis","title":"Cost Analysis","text":"<p>Running the test suite demonstrated the following costs:</p> <ul> <li>Single Run (10 tests): US$0.014</li> <li>100 Runs (1,000 tests): US$1.40</li> <li>Repeated runs for reliability testing (12,000 tests): US$16.80</li> </ul> <p>Cost Breakdown:</p> <ul> <li>Per Test Cost: ~US$0.0014</li> <li>Per Run (10 tests): US$0.014</li> <li>Per 100 Runs: US$1.40</li> </ul> <p>This demonstrates that comprehensive semantic testing remains economically viable even at scale. The cost per test is minimal considering the confidence gained in library reliability.</p> <p>Key Cost Insights:</p> <ul> <li>Linear cost scaling with test volume</li> <li>Predictable pricing for planning purposes</li> <li>Reasonable expense for production validation</li> </ul>"},{"location":"reliability_testing/semantic_reliability/#test-configuration","title":"Test Configuration","text":"<p>All tests used library defaults:</p> <p><pre><code>LLM_PROVIDER=openai\nLLM_MODEL=gpt-4o \nLLM_TEMPERATURE=0.0 \nLLM_MAX_TOKENS=4096 \nLLM_MAX_RETRIES=2 \nLLM_TIMEOUT=10.0 # Added for OpenAI in 0.1.0b5 using the underlying Langchain implementation \n</code></pre> The <code>semantic_assert_match</code> function also saw slight modification:</p> <pre><code>        if result.startswith(\"FAIL\"):\n            raise SemanticAssertionError(\n                \"Semantic assertion failed\",\n                reason=result.split(\"FAIL: \")[1]\n            )\n\n        # Section below added to cause failure in the event of format violation    \n\n        elif result.startswith(\"PASS\"):\n            pass\n        else:\n            raise RuntimeError(\n                f\"Format Non-compliance Detected {result}\"\n            )\n</code></pre> <p>The prompts to the asserter LLM (that sits behind <code>semantic_assert_match</code>) were:</p> <pre><code>DEFAULT_SYSTEM_PROMPT = \"\"\"You are a testing system. Your job is to determine if an actual output matches the expected behavior.\n\nImportant: You can only respond with EXACTLY: \n1. 'PASS' if it matches, or \n2. 'FAIL: &lt;reason&gt;' if it doesn't match.\n\nAny other type of response will mean disaster which as a testing system, you are meant to prevent.\n\nBe strict but consider semantic meaning rather than exact wording.\"\"\"\n\nDEFAULT_HUMAN_PROMPT = \"\"\"\nExpected Behavior: {expected_behavior}\n\nActual Output: {actual}\n\nDoes the actual output match the expected behavior? Remember, you will fail your task unless you respond EXACTLY \nwith 'PASS' or 'FAIL: &lt;reason&gt;'.\"\"\"\n</code></pre>"},{"location":"reliability_testing/semantic_reliability/#test-suite-code","title":"Test Suite Code","text":"<pre><code>import pytest\nfrom llm_app_test.semantic_assert.semantic_assert import SemanticAssertion\nfrom llm_app_test.exceptions.test_exceptions import (\n    SemanticAssertionError\n)\n\n\nclass TestRealWorldSemanticAssertion:\n    \"\"\"Test suite for semantic matching across diverse industry-specific LLM applications.\n\n    This test class is specifically designed to validate semantic matching capabilities\n    across a wide range of real-world LLM application outputs. It contains both positive\n    and negative test cases that represent actual use cases where LLMs are being used\n    in production environments.\n\n    Industries Covered:\n        - Healthcare (Patient Education)\n        - Financial Services (Portfolio Reporting)\n        - Media/Entertainment (Content Recommendations)\n        - Legal (Document Summarization)\n        - Manufacturing (Maintenance Prediction)\n        - E-commerce (Product Descriptions)\n        - Education (Assignment Feedback)\n        - Real Estate (Property Listings)\n        - Human Resources (Interview Feedback)\n        - Customer Service (Ticket Response)\n\n    Test Structure:\n        - Each test validates specific industry requirements\n        - Mix of positive and negative test cases\n        - Focus on realistic content length and complexity\n        - Industry-specific terminology and formatting\n        - Comprehensive coverage of common LLM outputs\n\n    Purpose:\n        This test suite is designed for brute force reliability testing of the semantic\n        matcher. It ensures the library can handle diverse, real-world content while\n        maintaining consistent behavior across multiple test runs.\n\n    Usage:\n        These tests are intended to be run multiple times (1000+) to validate the\n        consistency and reliability of the semantic matching functionality across\n        different contexts and content types.\n    \"\"\"\n    @pytest.fixture\n    def asserter(self):\n        return SemanticAssertion()\n\n\n    def test_patient_education_diabetes_management(self, asserter):\n        \"\"\"Test semantic matching for patient education content about diabetes management. Failure is expected because\n        this does not contain emergency response steps.\"\"\"\n        actual = \"\"\"\n        Understanding and Managing Type 2 Diabetes\n\n        Type 2 diabetes is a chronic condition that affects how your body processes blood sugar (glucose). \n        While this condition is serious, it can be effectively managed through lifestyle changes and, \n        when necessary, medication. This guide will help you understand the key aspects of diabetes \n        management.\n\n        Blood Sugar Monitoring:\n        Regular blood sugar monitoring is essential. Your target blood glucose levels should typically \n        be 80-130 mg/dL before meals and less than 180 mg/dL two hours after meals. However, your \n        healthcare provider may set different targets based on your individual needs. Keep a log of \n        your readings to identify patterns and adjust your management strategy accordingly.\n\n        Dietary Considerations:\n        A balanced diet is crucial for managing type 2 diabetes. Focus on:\n        - Controlling portion sizes\n        - Choosing high-fiber, low-glycemic foods\n        - Limiting refined carbohydrates and processed sugars\n        - Including lean proteins and healthy fats\n        - Spacing meals evenly throughout the day\n\n        Physical Activity:\n        Regular exercise helps control blood sugar levels by improving insulin sensitivity. Aim for:\n        - At least 150 minutes of moderate-intensity aerobic activity weekly\n        - Resistance training 2-3 times per week\n        - Daily movement, even if just short walks\n        Always check your blood sugar before and after exercise, and carry a fast-acting \n        carbohydrate source.\n\n        Medication Management:\n        If prescribed, take diabetes medications as directed. Common medications include:\n        - Metformin (helps reduce glucose production)\n        - Sulfonylureas (increase insulin production)\n        - DPP-4 inhibitors (help maintain blood sugar control)\n        Never adjust or stop medications without consulting your healthcare provider.\n\n        Warning Signs:\n        Learn to recognize and respond to:\n        - Hypoglycemia (low blood sugar): shakiness, sweating, confusion\n        - Hyperglycemia (high blood sugar): increased thirst, frequent urination, fatigue\n        Seek immediate medical attention if you experience severe symptoms or sustained \n        high blood sugar levels.\n\n        Regular Health Monitoring:\n        Schedule regular check-ups with your healthcare team, including:\n        - HbA1c tests every 3-6 months\n        - Annual eye examinations\n        - Regular foot checks\n        - Kidney function tests\n        - Cholesterol level monitoring\n\n        Remember, diabetes management is a journey, not a destination. Small, consistent \n        steps in the right direction can lead to significant improvements in your health \n        and quality of life.\n        \"\"\"\n\n        expected = \"\"\"A medical education document that must:\n        1. Contain an overview section explaining the condition\n        2. List specific numerical guidelines (blood sugar ranges, exercise minutes)\n        3. Include structured sections for diet, exercise, and medication\n        4. Provide clear warning signs AND detailed emergency response procedures\n        5. End with follow-up care instructions\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_investment_portfolio_report_generation(self, asserter):\n        \"\"\"Test semantic matching for investment portfolio report generation. Tests that the report\n        contains all required sections and maintains professional financial terminology.\"\"\"\n        actual = \"\"\"\n        Q4 2023 Portfolio Performance Summary\n\n        Portfolio Overview:\n        Your investment portfolio has demonstrated resilient performance during Q4 2023, \n        achieving a total return of 8.2% against our benchmark index return of 7.5%. \n        Total portfolio value stands at $1,245,000 as of December 31, 2023.\n\n        Asset Allocation Analysis:\n        Current allocation stands at:\n        - Equities: 65% ($809,250)\n            - US Large Cap: 40% ($498,000)\n            - International Developed: 15% ($186,750)\n            - Emerging Markets: 10% ($124,500)\n        - Fixed Income: 25% ($311,250)\n            - Government Bonds: 15% ($186,750)\n            - Corporate Bonds: 10% ($124,500)\n        - Alternative Investments: 10% ($124,500)\n            - Real Estate: 5% ($62,250)\n            - Commodities: 5% ($62,250)\n\n        Performance Attribution:\n        Key contributors to performance:\n        1. US Technology sector outperformance (+12.3%)\n        2. Emerging Markets recovery (+9.1%)\n        3. Corporate Bond yield optimization (+4.2%)\n\n        Risk Metrics:\n        - Portfolio Beta: 0.85\n        - Sharpe Ratio: 1.45\n        - Maximum Drawdown: -5.2%\n        - Standard Deviation: 12.3%\n\n        Rebalancing Recommendations:\n        Based on current market conditions and your investment objectives:\n        1. Consider increasing Fixed Income allocation by 2%\n        2. Reduce US Large Cap exposure by 3%\n        3. Increase Emerging Markets exposure by 1%\n\n        Market Outlook:\n        Looking ahead to 2024, we anticipate:\n        - Continued monetary policy normalization\n        - Potential emerging markets opportunities\n        - Heightened focus on quality factors in equity selection\n\n        Next Steps:\n        1. Schedule quarterly review meeting\n        2. Discuss rebalancing recommendations\n        3. Update investment policy statement if needed\n        \"\"\"\n\n        expected = \"\"\"A professional investment portfolio report that must:\n        1. Present portfolio performance with specific metrics\n        2. Detail current asset allocation with percentages\n        3. Include risk analysis metrics\n        4. Provide forward-looking recommendations\n        5. Maintain formal financial terminology\n        6. Include clear next steps or action items\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_content_recommendation_missing_viewing_patterns(self, asserter):\n        \"\"\"Test semantic matching for content recommendations. Should fail due to missing viewing patterns\n        and user preferences section.\"\"\"\n        actual = \"\"\"\n        Personalized Content Recommendations - User Profile #A1234\n        Generated: November 22, 2024\n\n        Recommended Content Queue:\n        1. \"Climate Pioneers\" (Documentary Series)\n            - Episode length: 45 minutes\n            - New episodes available\n\n        2. \"Global Power Play\" (Political Drama)\n            - Episode length: 55 minutes\n            - Features actors from previously watched content\n\n        3. \"Earth's Tipping Points\" (Scientific Documentary)\n            - Episode length: 40 minutes\n            - Recently added to platform\n\n        Engagement Optimization:\n        - Scheduled new episode alerts\n        - Downloadable content for offline viewing\n        - Similar content suggestions refreshed weekly\n        - Customized language preferences maintained\n\n        Content Accessibility:\n        All recommended content includes your preferred subtitle options and is \n        available in HD quality. Downloads are enabled for offline viewing during \n        your upcoming travel dates.\n        \"\"\"\n\n        expected = \"\"\"A personalized content recommendation document that must:\n        1. Include the viewing patterns and preferences of the user\n        2. List recommended content with clear reasoning\n        3. Provide matching percentages or relevance metrics\n        4. Include viewing optimization suggestions\n        5. Address content accessibility features\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_legal_document_summary_generation(self, asserter):\n        \"\"\"Test semantic matching for legal document summary generation. Tests that the summary\n        maintains accuracy while being accessible to non-legal readers.\"\"\"\n        actual = \"\"\"\n        Contract Summary Analysis\n        Document Reference: MSA-2024-0892\n        Date of Analysis: November 22, 2024\n\n        Agreement Overview:\n        Software Development Master Services Agreement between TechCorp Inc. (\"Provider\") \n        and GlobalEnterprises LLC (\"Client\") for the development and maintenance of \n        enterprise software solutions.\n\n        Key Terms and Conditions:\n        1. Service Scope\n            - Custom software development services\n            - System integration capabilities\n            - Ongoing maintenance and support\n            - Security compliance implementations\n\n        2. Financial Terms\n            - Base development fee: $750,000\n            - Monthly maintenance: $15,000\n            - Change request rate: $200/hour\n            - Payment terms: Net 30\n\n        3. Performance Standards\n            - 99.9% system availability\n            - 4-hour response time for critical issues\n            - Monthly performance reporting\n            - Quarterly service reviews\n\n        4. Intellectual Property Rights\n            - Client owns all custom development\n            - Provider retains rights to pre-existing IP\n            - Joint ownership of derivative works\n            - Limited license for provider tools\n\n        5. Term and Termination\n            - Initial term: 36 months\n            - Automatic renewal: 12-month periods\n            - 90-day termination notice required\n            - Immediate termination for material breach\n\n        Risk Assessment:\n        - Medium risk: Data protection obligations\n        - Low risk: Service level commitments\n        - Low risk: IP ownership structure\n        - Medium risk: Change management process\n\n        Next Steps:\n        1. Legal team review of data protection terms\n        2. Technical team validation of SLAs\n        3. Finance approval of payment terms\n        4. Compliance review of security standards\n        \"\"\"\n\n        expected = \"\"\"A legal document summary that must:\n        1. Identify key parties and document type\n        2. List main contractual terms\n        3. Include specific numerical values (costs, dates, metrics)\n        4. Provide risk assessment\n        5. Outline required actions or next steps\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_maintenance_prediction_missing_historical_context(self, asserter):\n        \"\"\"Test semantic matching for maintenance prediction report. Should fail due to\n        missing historical maintenance context and pattern analysis.\"\"\"\n        actual = \"\"\"\n        Equipment Maintenance Analysis\n        Machine ID: CNC-1234\n        Analysis Date: November 22, 2024\n\n        Current Status Summary:\n        The CNC machine is showing early indicators of potential bearing wear in the main spindle.\n        Recommended action is to schedule maintenance within the next 2 weeks.\n\n        Operational Parameters:\n        - Current Runtime: 2,450 hours\n        - Average Daily Usage: 18 hours\n        - Last Maintenance: October 15, 2024\n\n        Immediate Recommendations:\n        1. Schedule bearing inspection\n        2. Monitor vibration levels daily\n        3. Prepare replacement parts\n        4. Plan for 4-hour maintenance window\n\n        Impact Assessment:\n        - Production Impact: Minimal if addressed within 2 weeks\n        - Resource Requirements: Standard maintenance team\n        - Parts Cost Estimate: $2,500\n        \"\"\"\n\n        expected = \"\"\"A maintenance prediction report that must:\n        1. Include current machine status\n        2. Provide historical maintenance patterns\n        3. Show failure prediction confidence levels\n        4. List specific maintenance recommendations\n        5. Include impact assessment and timeline\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_product_description_generation(self, asserter):\n        \"\"\"Test semantic matching for e-commerce product description generation. Tests that the description\n        includes all required elements of an effective product listing.\"\"\"\n        actual = \"\"\"\n        Smart Home Security Camera - Model HC2000\n\n        Transform your home security with our latest AI-powered camera system. This next-generation \n        device combines advanced motion detection with crystal-clear 4K video quality, perfect for \n        both indoor and outdoor monitoring.\n\n        Key Features:\n        - 4K Ultra HD resolution with HDR\n        - 160\u00b0 wide-angle view\n        - Advanced AI motion detection\n        - Two-way audio communication\n        - Night vision up to 30 feet\n        - Weather-resistant (IP66 rated)\n\n        Smart Integration:\n        Works seamlessly with major platforms including:\n        - Amazon Alexa\n        - Google Home\n        - Apple HomeKit\n        - IFTTT\n\n        Technical Specifications:\n        - Dimensions: 3.2\" x 3.2\" x 5.1\"\n        - Weight: 12.3 oz\n        - Power: AC adapter or rechargeable battery\n        - Storage: Cloud or local SD card (up to 256GB)\n        - Connectivity: 2.4GHz/5GHz WiFi\n\n        What's in the Box:\n        - HC2000 Camera\n        - Mounting bracket\n        - Power adapter\n        - Quick start guide\n        - Screws and anchors\n\n        Perfect for:\n        - Home security\n        - Baby monitoring\n        - Pet watching\n        - Front door monitoring\n\n        30-day money-back guarantee\n        2-year manufacturer warranty\n        Free technical support\n        \"\"\"\n\n        expected = \"\"\"An e-commerce product description that must:\n        1. Include clear product name and model\n        2. List key features and specifications\n        3. Specify technical details and compatibility\n        4. Describe package contents\n        5. Include warranty and support information\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_assignment_feedback_missing_improvement_steps(self, asserter):\n        \"\"\"Test semantic matching for student assignment feedback. Should fail due to\n        missing specific improvement steps and learning objectives.\"\"\"\n        actual = \"\"\"\n        Assignment Feedback\n        Student ID: STU-2024-456\n        Assignment: Research Paper on Climate Change\n        Submission Date: November 22, 2024\n\n        Overall Assessment:\n        Your research paper demonstrates good understanding of climate change basics.\n        The writing is clear and well-structured, with appropriate use of scientific\n        terminology throughout the document.\n\n        Strengths:\n        - Strong introduction that sets context\n        - Good use of current scientific data\n        - Clear paragraph structure\n        - Proper citation format\n\n        Areas Noted:\n        - Some statistical interpretations could be more precise\n        - Additional peer-reviewed sources would strengthen arguments\n        - Conclusion could be more comprehensive\n\n        Grade: B+ (88/100)\n\n        Additional Comments:\n        The paper shows promise and indicates solid research skills. Your analysis\n        of temperature data trends was particularly well-done. Consider expanding\n        your discussion of potential mitigation strategies in future work.\n        \"\"\"\n\n        expected = \"\"\"An assignment feedback document that must:\n        1. Include basic assignment and student information\n        2. Provide specific strengths and weaknesses\n        3. List concrete steps for improvement\n        4. Reference specific learning objectives\n        5. Include grading criteria and score\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_real_estate_listing_generation(self, asserter):\n        \"\"\"Test semantic matching for real estate listing generation. Tests that the listing\n        includes all essential elements of an effective property description.\"\"\"\n        actual = \"\"\"\n        Stunning Modern Oasis in Prime Location\n        123 Maple Avenue, Riverside Heights\n\n        Discover urban elegance in this meticulously updated contemporary home, where \n        modern luxury meets practical living. This 2,400 sq ft residence seamlessly \n        blends indoor and outdoor living spaces.\n\n        Property Highlights:\n        - 4 bedrooms, 2.5 bathrooms\n        - Built: 2018\n        - Lot size: 0.25 acres\n        - Two-car attached garage\n        - Energy-efficient smart home features\n\n        Interior Features:\n        The open-concept main level showcases:\n        - Chef's kitchen with quartz countertops\n        - Custom Italian cabinetry\n        - Premium stainless steel appliances\n        - Expansive living room with 12-foot ceilings\n        - Primary suite with spa-inspired bathroom\n\n        Outdoor Living:\n        - Professional landscaping\n        - Covered patio with built-in BBQ\n        - Low-maintenance xeriscaping\n        - Private backyard retreat\n\n        Location Benefits:\n        - Walking distance to Central Park\n        - Top-rated school district\n        - 10 minutes to downtown\n        - Easy access to major highways\n\n        Recent Updates:\n        - New HVAC system (2023)\n        - Smart home integration\n        - Updated LED lighting\n        - Fresh interior paint\n\n        Price: $875,000\n        Available for immediate viewing\n        Virtual tour link: [URL]\n        \"\"\"\n\n        expected = \"\"\"A real estate listing that must:\n        1. Include property overview and key features\n        2. List specific amenities and updates\n        3. Describe location benefits\n        4. Use engaging, descriptive language\n        5. Provide essential details (size, bedrooms, price)\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_interview_feedback_missing_criteria(self, asserter):\n        \"\"\"Test semantic matching for interview feedback generation. Should fail due to\n        missing evaluation criteria and specific examples.\"\"\"\n        actual = \"\"\"\n        Interview Feedback Summary\n        Candidate ID: INT-2024-789\n        Position: Senior Software Engineer\n        Interview Date: November 22, 2024\n\n        Overall Impression:\n        The candidate demonstrated strong technical knowledge and communicated well\n        throughout the interview. They showed enthusiasm for the role and our company's\n        mission.\n\n        Discussion Points:\n        - Previous experience with cloud architecture\n        - Team collaboration approaches\n        - Problem-solving methodology\n        - Career goals and aspirations\n\n        Technical Discussion:\n        Candidate showed familiarity with:\n        - Microservices architecture\n        - CI/CD pipelines\n        - Cloud platforms (AWS, Azure)\n        - Agile development practices\n\n        Cultural Fit:\n        Appears to align well with our company values and team dynamics.\n        Demonstrated good communication skills and collaborative mindset.\n\n        Next Steps:\n        Proceed with reference checks if moving forward.\n        Schedule follow-up with hiring manager.\n        \"\"\"\n\n        expected = \"\"\"An interview feedback document that must:\n        1. Include candidate and position information\n        2. List specific evaluation criteria with ratings\n        3. Provide concrete examples of responses\n        4. Include technical assessment scores\n        5. Offer clear hiring recommendation\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_customer_service_ticket_response(self, asserter):\n        \"\"\"Test semantic matching for customer service ticket analysis and response generation.\"\"\"\n        actual = \"\"\"\n        Ticket Analysis and Response\n        Ticket ID: CS-2024-1122\n        Priority: Medium\n        Category: Product Return\n\n        Customer Query Summary:\n        Customer purchased a wireless headphone (Model: WH-1000XM4) three days ago\n        and is experiencing connectivity issues with their iPhone 13. Initial\n        troubleshooting steps were attempted without success.\n\n        Issue Analysis:\n        - Product is within return window (3 of 30 days)\n        - Common compatibility issue identified\n        - Troubleshooting already attempted\n        - Customer tone indicates frustration\n\n        Recommended Response:\n        Dear [Customer Name],\n\n        Thank you for reaching out about the connectivity issues with your WH-1000XM4\n        headphones. I understand how frustrating technical issues can be, especially\n        with a new purchase.\n\n        Based on your description, I can offer you two immediate solutions:\n\n        1. Advanced Troubleshooting:\n           - Reset the headphones (detailed steps attached)\n           - Update iPhone Bluetooth settings\n           - Install latest firmware\n\n        2. Hassle-free Return:\n           - Generate return label through our portal\n           - Full refund processed within 3 business days\n           - Free return shipping\n\n        Would you prefer to try the advanced troubleshooting steps, or would you like\n        to proceed with the return? I'm here to help with either option.\n\n        Next Steps:\n        - Await customer preference\n        - Prepare return label if requested\n        - Schedule follow-up within 24 hours\n\n        Response Tone: Empathetic and Solution-focused\n        Support Resources: KB-2345, RT-6789\n        \"\"\"\n\n        expected = \"\"\"A customer service response that must:\n        1. Include ticket categorization and priority\n        2. Summarize the customer's issue accurately\n        3. Provide multiple solution options\n        4. Include clear next steps\n        5. Maintain appropriate tone and empathy\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n</code></pre>"},{"location":"reliability_testing/semantic_reliability/#conclusion","title":"Conclusion","text":"<p>This real-world test suite demonstrates that llm-app-test can reliably handle the kind of content that LLM applications generate in production environments. </p> <p>The 100% pass rate across 12,000 executions provides strong evidence of the library's reliability for real-world use cases.</p> <p>However, we emphasise that we remain unable to guarantee perfect determinism due to the nature of LLMs. What we are confident in, is that this library is \"good enough\" for production software.</p>"},{"location":"reliability_testing/semantic_reliability/#issue-reporting","title":"Issue reporting","text":"<p>If you experience any issues, especially with library reliability - please let us know, thanks!</p>"},{"location":"reliability_testing/semantic_reliability/#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Format Compliance Testing</li> </ul>"}]}