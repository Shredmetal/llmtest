{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"llmtest A semantic testing framework for LLM applications that uses LLMs to validate semantic equivalence in test outputs. \u2728 Test your LLM apps in minutes, not hours \ud83d\ude80 CI/CD ready out of the box \ud83d\udcb0 Cost-effective testing solution \ud83d\udd27 No infrastructure needed Documentation Installation Quick Start CI/CD Integration Best Practices API Reference License MIT","title":"llmtest"},{"location":"#llmtest","text":"A semantic testing framework for LLM applications that uses LLMs to validate semantic equivalence in test outputs. \u2728 Test your LLM apps in minutes, not hours \ud83d\ude80 CI/CD ready out of the box \ud83d\udcb0 Cost-effective testing solution \ud83d\udd27 No infrastructure needed","title":"llmtest"},{"location":"#documentation","text":"Installation Quick Start CI/CD Integration Best Practices API Reference","title":"Documentation"},{"location":"#license","text":"MIT","title":"License"},{"location":"api/configuration/","text":"\u2190 Back to Home Configuration llmtest provides flexible configuration through environment variables and direct parameters. Environment Variables Provider Selection LLM_PROVIDER=openai # or 'anthropic' API Keys # For OpenAI (default provider) OPENAI_API_KEY=your-openai-key # For Anthropic ANTHROPIC_API_KEY=your-anthropic-key Optional Settings LLM_MODEL=gpt-4o # Model name - default for OpenAI: gpt-4o, default for anthropic: claude-3-5-sonnet-latest LLM_TEMPERATURE=0.0 # Response randomness (0.0-1.0) LLM_MAX_TOKENS=4096 # Maximum response length LLM_MAX_RETRIES=2 # API retry attempts Direct Configuration You can also configure llmtest programmatically: from llmtest.semanticassert.semantic_assert import SemanticAssertion asserter = SemanticAssertion(api_key=\"your-api-key\", # Strongly advised against, use env vars provider=\"openai\", # or 'anthropic' model=\"gpt-4o\", # See Supported Models temperature=0.0, # Default: 0.0 max_tokens=4096, # Default: 4096 max_retries=2 # Default: 2 ) Supported Models Model supported is restricted due to poorer semantic matching performance of less advanced models. OpenAI gpt-4o gpt-4-turbo Anthropic claude-3-5-sonnet-latest claude-3-opus-latest (EXPENSIVE!!!) Configuration Priority Configuration values are resolved in this order: 1. Directly passed parameters 2. Environment variables 3. Default values Best Practices Use environment variables for API keys Keep temperature at 0.0 for consistent testing Use default max_tokens unless you have specific needs Start with default max_retries (2) Note: llmtest validates all configuration values at startup to prevent runtime errors. Navigation Back to Home Installation Guide Quick Start Guide API Reference","title":"Configuration"},{"location":"api/configuration/#configuration","text":"llmtest provides flexible configuration through environment variables and direct parameters.","title":"Configuration"},{"location":"api/configuration/#environment-variables","text":"","title":"Environment Variables"},{"location":"api/configuration/#provider-selection","text":"LLM_PROVIDER=openai # or 'anthropic'","title":"Provider Selection"},{"location":"api/configuration/#api-keys","text":"# For OpenAI (default provider) OPENAI_API_KEY=your-openai-key # For Anthropic ANTHROPIC_API_KEY=your-anthropic-key","title":"API Keys"},{"location":"api/configuration/#optional-settings","text":"LLM_MODEL=gpt-4o # Model name - default for OpenAI: gpt-4o, default for anthropic: claude-3-5-sonnet-latest LLM_TEMPERATURE=0.0 # Response randomness (0.0-1.0) LLM_MAX_TOKENS=4096 # Maximum response length LLM_MAX_RETRIES=2 # API retry attempts","title":"Optional Settings"},{"location":"api/configuration/#direct-configuration","text":"You can also configure llmtest programmatically: from llmtest.semanticassert.semantic_assert import SemanticAssertion asserter = SemanticAssertion(api_key=\"your-api-key\", # Strongly advised against, use env vars provider=\"openai\", # or 'anthropic' model=\"gpt-4o\", # See Supported Models temperature=0.0, # Default: 0.0 max_tokens=4096, # Default: 4096 max_retries=2 # Default: 2 )","title":"Direct Configuration"},{"location":"api/configuration/#supported-models","text":"Model supported is restricted due to poorer semantic matching performance of less advanced models.","title":"Supported Models"},{"location":"api/configuration/#openai","text":"gpt-4o gpt-4-turbo","title":"OpenAI"},{"location":"api/configuration/#anthropic","text":"claude-3-5-sonnet-latest claude-3-opus-latest (EXPENSIVE!!!)","title":"Anthropic"},{"location":"api/configuration/#configuration-priority","text":"Configuration values are resolved in this order: 1. Directly passed parameters 2. Environment variables 3. Default values","title":"Configuration Priority"},{"location":"api/configuration/#best-practices","text":"Use environment variables for API keys Keep temperature at 0.0 for consistent testing Use default max_tokens unless you have specific needs Start with default max_retries (2) Note: llmtest validates all configuration values at startup to prevent runtime errors.","title":"Best Practices"},{"location":"api/configuration/#navigation","text":"Back to Home Installation Guide Quick Start Guide API Reference","title":"Navigation"},{"location":"api/error-handling/","text":"\u2190 Back to Home Error Handling llmtest provides specific exceptions for different error scenarios to help you handle errors gracefully in your tests. Exception Hierarchy LLMTestError # Base exception for all llmtest errors \u251c\u2500\u2500 SemanticAssertionError # When semantic matching fails \u251c\u2500\u2500 LLMConfigurationError # When configuration is invalid \u251c\u2500\u2500 LLMConnectionError # When LLM service fails \u2514\u2500\u2500 InvalidPromptError # When prompt construction fails - not currently in use, code left in situ for future development Exception Details SemanticAssertionError Raised when the semantic assertion fails: try: semantic_assert.assert_semantic_match(actual=\"Hello Bob\", expected_behavior=\"A greeting for Alice\") except SemanticAssertionError as e: print(f\"Test failed: {e}\") # Includes detailed reason why outputs don't match LLMConfigurationError Raised when configuration is invalid: try: semantic_assert = SemanticAssertion(provider=\"invalid_provider\") except LLMConfigurationError as e: print(f\"Configuration error: {e}\") # Details about invalid configuration LLMConnectionError Raised when LLM service fails: try: semantic_assert.assert_semantic_match(...) except LLMConnectionError as e: print(f\"Service error: {e}\") # Contains original provider error details InvalidPromptError (NOT CURRENTLY IN USE) Raised when prompt construction fails: try: semantic_assert.assert_semantic_match(actual=None, # Invalid input expected_behavior=\"Some behavior\") except InvalidPromptError as e: print(f\"Prompt error: {e}\") Error Information All exceptions should provide: - Clear error message - Detailed reason (when available) - Additional context in details dictionary Example: try: semantic_assert.assert_semantic_match(...) except LLMTestError as e: print(f\"Message: {e.message}\") print(f\"Reason: {e.reason}\") print(f\"Details: {e.details}\") Best Practices Always catch specific exceptions rather than the base LLMTestError Log the full error information for debugging Handle LLMConnectionError for retry logic Use error details for test reporting Note: All exceptions properly chain to their root cause, preserving the full error context. Navigation Back to Home Installation Guide Quick Start Guide API Reference","title":"Error handling"},{"location":"api/error-handling/#error-handling","text":"llmtest provides specific exceptions for different error scenarios to help you handle errors gracefully in your tests.","title":"Error Handling"},{"location":"api/error-handling/#exception-hierarchy","text":"LLMTestError # Base exception for all llmtest errors \u251c\u2500\u2500 SemanticAssertionError # When semantic matching fails \u251c\u2500\u2500 LLMConfigurationError # When configuration is invalid \u251c\u2500\u2500 LLMConnectionError # When LLM service fails \u2514\u2500\u2500 InvalidPromptError # When prompt construction fails - not currently in use, code left in situ for future development","title":"Exception Hierarchy"},{"location":"api/error-handling/#exception-details","text":"","title":"Exception Details"},{"location":"api/error-handling/#semanticassertionerror","text":"Raised when the semantic assertion fails: try: semantic_assert.assert_semantic_match(actual=\"Hello Bob\", expected_behavior=\"A greeting for Alice\") except SemanticAssertionError as e: print(f\"Test failed: {e}\") # Includes detailed reason why outputs don't match","title":"SemanticAssertionError"},{"location":"api/error-handling/#llmconfigurationerror","text":"Raised when configuration is invalid: try: semantic_assert = SemanticAssertion(provider=\"invalid_provider\") except LLMConfigurationError as e: print(f\"Configuration error: {e}\") # Details about invalid configuration","title":"LLMConfigurationError"},{"location":"api/error-handling/#llmconnectionerror","text":"Raised when LLM service fails: try: semantic_assert.assert_semantic_match(...) except LLMConnectionError as e: print(f\"Service error: {e}\") # Contains original provider error details","title":"LLMConnectionError"},{"location":"api/error-handling/#invalidprompterror-not-currently-in-use","text":"Raised when prompt construction fails: try: semantic_assert.assert_semantic_match(actual=None, # Invalid input expected_behavior=\"Some behavior\") except InvalidPromptError as e: print(f\"Prompt error: {e}\")","title":"InvalidPromptError (NOT CURRENTLY IN USE)"},{"location":"api/error-handling/#error-information","text":"All exceptions should provide: - Clear error message - Detailed reason (when available) - Additional context in details dictionary Example: try: semantic_assert.assert_semantic_match(...) except LLMTestError as e: print(f\"Message: {e.message}\") print(f\"Reason: {e.reason}\") print(f\"Details: {e.details}\")","title":"Error Information"},{"location":"api/error-handling/#best-practices","text":"Always catch specific exceptions rather than the base LLMTestError Log the full error information for debugging Handle LLMConnectionError for retry logic Use error details for test reporting Note: All exceptions properly chain to their root cause, preserving the full error context.","title":"Best Practices"},{"location":"api/error-handling/#navigation","text":"Back to Home Installation Guide Quick Start Guide API Reference","title":"Navigation"},{"location":"api/semantic-assertion/","text":"\u2190 Back to Home SemanticAssertion Core class for semantic testing of LLM applications. Constructor SemanticAssertion(api_key: Optional[str] = None, llm: Optional[BaseLanguageModel] = None, provider: Optional[Union[str, LLMProvider]] = None, model: Optional[str] = None, temperature: Optional[float] = None, max_tokens: Optional[int] = None, max_retries: Optional[int] = None ) Parameters All parameters are optional (except for API key) and will use environment variables or defaults if not specified: api_key : API key for the LLM provider Environment: OPENAI_API_KEY or ANTHROPIC_API_KEY Default: None (must be provided via environment or parameter) If using default provider: use OPENAI_API_KEY since default provider is openai llm : Pre-configured LLM instance (must be of type langchain_core.language_models import BaseLanguageModel ) Default: None (if provided, bypasses all other configuration) provider : LLM provider ('openai' or 'anthropic') Environment: LLM_PROVIDER Default: 'openai' model : Model name to use Environment: LLM_MODEL Default: 'gpt-4o' for OpenAI, 'claude-3-5-sonnet-latest' for Anthropic temperature : Temperature setting for LLM responses Environment: LLM_TEMPERATURE Default: 0.0 Range: 0.0 to 1.0 max_tokens : Maximum tokens for response Environment: LLM_MAX_TOKENS Default: 4096 max_retries : Maximum number of API call retries Environment: LLM_MAX_RETRIES Default: 2 Methods assert_semantic_match def assert_semantic_match(actual: str, expected_behavior: str ) -> None Asserts that actual output semantically matches expected behavior. Parameters actual : The actual output to test expected_behavior : Natural language description of expected behavior Raises SemanticAssertionError : If outputs don't match semantically LLMConnectionError : If LLM service fails LLMConfigurationError : If configuration is invalid TypeError : If inputs are None Examples Basic Usage asserter = SemanticAssertion() # Uses environment variables asserter.assert_semantic_match(actual=\"Hello Alice, how are you?\", # In practice, use the output from your LLM application expected_behavior=\"A greeting addressing Alice\" ) Custom Configuration asserter = SemanticAssertion(provider=\"anthropic\", model=\"claude-3-5-sonnet-latest\", # Look I can't stop you from burning a hole in your wallet but please don't use Claude 3 Opus. temperature=0.1 ) Navigation Back to Home Installation Guide Quick Start Guide Configuration Reference","title":"Semantic assertion"},{"location":"api/semantic-assertion/#semanticassertion","text":"Core class for semantic testing of LLM applications.","title":"SemanticAssertion"},{"location":"api/semantic-assertion/#constructor","text":"SemanticAssertion(api_key: Optional[str] = None, llm: Optional[BaseLanguageModel] = None, provider: Optional[Union[str, LLMProvider]] = None, model: Optional[str] = None, temperature: Optional[float] = None, max_tokens: Optional[int] = None, max_retries: Optional[int] = None )","title":"Constructor"},{"location":"api/semantic-assertion/#parameters","text":"All parameters are optional (except for API key) and will use environment variables or defaults if not specified: api_key : API key for the LLM provider Environment: OPENAI_API_KEY or ANTHROPIC_API_KEY Default: None (must be provided via environment or parameter) If using default provider: use OPENAI_API_KEY since default provider is openai llm : Pre-configured LLM instance (must be of type langchain_core.language_models import BaseLanguageModel ) Default: None (if provided, bypasses all other configuration) provider : LLM provider ('openai' or 'anthropic') Environment: LLM_PROVIDER Default: 'openai' model : Model name to use Environment: LLM_MODEL Default: 'gpt-4o' for OpenAI, 'claude-3-5-sonnet-latest' for Anthropic temperature : Temperature setting for LLM responses Environment: LLM_TEMPERATURE Default: 0.0 Range: 0.0 to 1.0 max_tokens : Maximum tokens for response Environment: LLM_MAX_TOKENS Default: 4096 max_retries : Maximum number of API call retries Environment: LLM_MAX_RETRIES Default: 2","title":"Parameters"},{"location":"api/semantic-assertion/#methods","text":"","title":"Methods"},{"location":"api/semantic-assertion/#assert_semantic_match","text":"def assert_semantic_match(actual: str, expected_behavior: str ) -> None Asserts that actual output semantically matches expected behavior.","title":"assert_semantic_match"},{"location":"api/semantic-assertion/#parameters_1","text":"actual : The actual output to test expected_behavior : Natural language description of expected behavior","title":"Parameters"},{"location":"api/semantic-assertion/#raises","text":"SemanticAssertionError : If outputs don't match semantically LLMConnectionError : If LLM service fails LLMConfigurationError : If configuration is invalid TypeError : If inputs are None","title":"Raises"},{"location":"api/semantic-assertion/#examples","text":"","title":"Examples"},{"location":"api/semantic-assertion/#basic-usage","text":"asserter = SemanticAssertion() # Uses environment variables asserter.assert_semantic_match(actual=\"Hello Alice, how are you?\", # In practice, use the output from your LLM application expected_behavior=\"A greeting addressing Alice\" )","title":"Basic Usage"},{"location":"api/semantic-assertion/#custom-configuration","text":"asserter = SemanticAssertion(provider=\"anthropic\", model=\"claude-3-5-sonnet-latest\", # Look I can't stop you from burning a hole in your wallet but please don't use Claude 3 Opus. temperature=0.1 )","title":"Custom Configuration"},{"location":"api/semantic-assertion/#navigation","text":"Back to Home Installation Guide Quick Start Guide Configuration Reference","title":"Navigation"},{"location":"getting-started/installation/","text":"\u2190 Back to Home Installation Requirements Python 3.8 or higher pip (package installer for Python) Installing llmtest Install directly from GitHub: pip install git+https://github.com/Shredmetal/llmtest.git Development Installation If you want to contribute to llmtest, install with development dependencies: # Clone the repository git clone https://github.com/Shredmetal/llmtest.git # Change to project directory cd llmtest # Install with development dependencies pip install -e \".[dev]\"] Next Steps Quick Start Guide - Learn how to use llmtest","title":"Installation"},{"location":"getting-started/installation/#installation","text":"","title":"Installation"},{"location":"getting-started/installation/#requirements","text":"Python 3.8 or higher pip (package installer for Python)","title":"Requirements"},{"location":"getting-started/installation/#installing-llmtest","text":"Install directly from GitHub: pip install git+https://github.com/Shredmetal/llmtest.git","title":"Installing llmtest"},{"location":"getting-started/installation/#development-installation","text":"If you want to contribute to llmtest, install with development dependencies: # Clone the repository git clone https://github.com/Shredmetal/llmtest.git # Change to project directory cd llmtest # Install with development dependencies pip install -e \".[dev]\"]","title":"Development Installation"},{"location":"getting-started/installation/#next-steps","text":"Quick Start Guide - Learn how to use llmtest","title":"Next Steps"},{"location":"getting-started/quickstart/","text":"\u2190 Back to Home Quick Start Get up and running with llmtest in under 5 minutes. 1. Set Up Environment Create a .env file in your project root: # For OpenAI OPENAI_API_KEY=your-openai-api-key-here # OR for Anthropic ANTHROPIC_API_KEY=your-anthropic-key-here 2. Write Your First Test from llmtest.semanticassert.semantic_assert import SemanticAssertion def my_first_semantic_test(): # Initialize asserter semantic_assert = SemanticAssertion() # Your LLM output actual_output = \"The sky is blue\" # Expected behavior in natural language expected_behavior = \"A statement about the color of the sky\" # Test semantic equivalence semantic_assert.assert_semantic_match( actual=actual_output, expected_behavior=expected_behavior ) 3. Run Your Test pytest my_first_semantic_test.py Next Steps CI/CD Integration - Set up automated testing Configuration - Configure llmtest for your needs Additional notes: Real World Example Want to see llmtest in action? Here's a real test from our test suite: from llmtest.semanticassert.semantic_assert import SemanticAssertion from llmtest.tests.test_content_generators.test_greeting_bot import SimpleGreetingBot def test_greeting_semantic(): semantic_assert = SemanticAssertion() bot = SimpleGreetingBot() actual_output = bot.generate_greeting(\"Alice\") expected_behavior = \"\"\" A polite greeting that: 1. Addresses the person by name (Alice) 2. Asks about their wellbeing \"\"\" semantic_assert.assert_semantic_match( actual=actual_output, expected_behavior=expected_behavior ) You can find this example in our repository: test_greeting.py It can be run from this project root with the following command: pytest tests/actual_usage_tests/test_greeting.py You can find and run this example: # Clone the repository git clone https://github.com/Shredmetal/llmtest.git # Run the test pytest tests/actual_usage_tests/test_greeting.py","title":"Quickstart"},{"location":"getting-started/quickstart/#quick-start","text":"Get up and running with llmtest in under 5 minutes.","title":"Quick Start"},{"location":"getting-started/quickstart/#1-set-up-environment","text":"Create a .env file in your project root: # For OpenAI OPENAI_API_KEY=your-openai-api-key-here # OR for Anthropic ANTHROPIC_API_KEY=your-anthropic-key-here","title":"1. Set Up Environment"},{"location":"getting-started/quickstart/#2-write-your-first-test","text":"from llmtest.semanticassert.semantic_assert import SemanticAssertion def my_first_semantic_test(): # Initialize asserter semantic_assert = SemanticAssertion() # Your LLM output actual_output = \"The sky is blue\" # Expected behavior in natural language expected_behavior = \"A statement about the color of the sky\" # Test semantic equivalence semantic_assert.assert_semantic_match( actual=actual_output, expected_behavior=expected_behavior )","title":"2. Write Your First Test"},{"location":"getting-started/quickstart/#3-run-your-test","text":"pytest my_first_semantic_test.py","title":"3. Run Your Test"},{"location":"getting-started/quickstart/#next-steps","text":"CI/CD Integration - Set up automated testing Configuration - Configure llmtest for your needs","title":"Next Steps"},{"location":"getting-started/quickstart/#additional-notes","text":"","title":"Additional notes:"},{"location":"getting-started/quickstart/#real-world-example","text":"Want to see llmtest in action? Here's a real test from our test suite: from llmtest.semanticassert.semantic_assert import SemanticAssertion from llmtest.tests.test_content_generators.test_greeting_bot import SimpleGreetingBot def test_greeting_semantic(): semantic_assert = SemanticAssertion() bot = SimpleGreetingBot() actual_output = bot.generate_greeting(\"Alice\") expected_behavior = \"\"\" A polite greeting that: 1. Addresses the person by name (Alice) 2. Asks about their wellbeing \"\"\" semantic_assert.assert_semantic_match( actual=actual_output, expected_behavior=expected_behavior ) You can find this example in our repository: test_greeting.py It can be run from this project root with the following command: pytest tests/actual_usage_tests/test_greeting.py You can find and run this example: # Clone the repository git clone https://github.com/Shredmetal/llmtest.git # Run the test pytest tests/actual_usage_tests/test_greeting.py","title":"Real World Example"},{"location":"guides/best-practices/","text":"\u2190 Back to Home Best Practices Guidelines for effective semantic testing with llmtest. Understanding Semantic Testing Semantic testing focuses on meaning rather than exact matches. For example: # THESE ARE SEMANTICALLY EQUIVALENT actual_1 = \"Hello Alice, how are you today?\" actual_2 = \"Hi Alice! Hope you're doing well!\" expected = \"A polite greeting addressing Alice\" Writing Good Expected Behaviors Be Specific : # Good expected_behavior = \"\"\" A polite greeting that: Addresses the person by name (Alice) Asks about their wellbeing \"\"\" # Bad - too vague expected_behavior = \"A nice greeting\" Focus on Requirements : # Good expected_behavior = \"\"\" An error message that: Explains the API key is invalid Provides steps to fix the issue \"\"\" # Bad - testing exact wording - DO NOT USE LLMTEST FOR THIS, JUST USE REGULAR PYTEST expected_behavior = \"Should say 'Invalid API key'\" When to Use Semantic Testing Good Use Cases: - Testing LLM outputs - Validating natural language responses - Testing content generation Not Suitable For: - Exact string matching - Numerical comparisons - Binary conditions Test Structure Keep Tests Focused : # Good def test_greeting_format(): \"\"\"Test greeting format only\"\"\" def test_greeting_personalization(): \"\"\"Test name usage separately\"\"\" # Bad - testing too much def test_everything_about_greeting(): \"\"\"Testing multiple aspects at once\"\"\" Clear Test Names : # Good def test_error_message_includes_solution_steps(): # Bad def test_error(): Common Pitfalls Over-Specific Expected Behaviors : # Too specific expected = \"Must say hello and use exactly these words\" # Better expected = \"Should be a greeting in conversational English\" Under-Specific Expected Behaviors : # Too vague expected = \"Should be good\" # Better expected = \"\"\" Response should: Answer the user's question Use professional language Stay on topic \"\"\" Cost Optimization Use specific, focused tests Group related semantic tests Consider test importance vs cost Use appropriate model tiers Closing words In general, this is a complete different type of testing designed to mimic a human testing your LLM application. You might need to use your brain a little bit to figure out to instruct your assistant on what to check. Navigation Back to Home Installation Guide Quick Start Guide API Reference","title":"Best practices"},{"location":"guides/best-practices/#best-practices","text":"Guidelines for effective semantic testing with llmtest.","title":"Best Practices"},{"location":"guides/best-practices/#understanding-semantic-testing","text":"Semantic testing focuses on meaning rather than exact matches. For example: # THESE ARE SEMANTICALLY EQUIVALENT actual_1 = \"Hello Alice, how are you today?\" actual_2 = \"Hi Alice! Hope you're doing well!\" expected = \"A polite greeting addressing Alice\"","title":"Understanding Semantic Testing"},{"location":"guides/best-practices/#writing-good-expected-behaviors","text":"Be Specific : # Good expected_behavior = \"\"\" A polite greeting that: Addresses the person by name (Alice) Asks about their wellbeing \"\"\" # Bad - too vague expected_behavior = \"A nice greeting\" Focus on Requirements : # Good expected_behavior = \"\"\" An error message that: Explains the API key is invalid Provides steps to fix the issue \"\"\" # Bad - testing exact wording - DO NOT USE LLMTEST FOR THIS, JUST USE REGULAR PYTEST expected_behavior = \"Should say 'Invalid API key'\"","title":"Writing Good Expected Behaviors"},{"location":"guides/best-practices/#when-to-use-semantic-testing","text":"Good Use Cases: - Testing LLM outputs - Validating natural language responses - Testing content generation Not Suitable For: - Exact string matching - Numerical comparisons - Binary conditions","title":"When to Use Semantic Testing"},{"location":"guides/best-practices/#test-structure","text":"Keep Tests Focused : # Good def test_greeting_format(): \"\"\"Test greeting format only\"\"\" def test_greeting_personalization(): \"\"\"Test name usage separately\"\"\" # Bad - testing too much def test_everything_about_greeting(): \"\"\"Testing multiple aspects at once\"\"\" Clear Test Names : # Good def test_error_message_includes_solution_steps(): # Bad def test_error():","title":"Test Structure"},{"location":"guides/best-practices/#common-pitfalls","text":"Over-Specific Expected Behaviors : # Too specific expected = \"Must say hello and use exactly these words\" # Better expected = \"Should be a greeting in conversational English\" Under-Specific Expected Behaviors : # Too vague expected = \"Should be good\" # Better expected = \"\"\" Response should: Answer the user's question Use professional language Stay on topic \"\"\"","title":"Common Pitfalls"},{"location":"guides/best-practices/#cost-optimization","text":"Use specific, focused tests Group related semantic tests Consider test importance vs cost Use appropriate model tiers","title":"Cost Optimization"},{"location":"guides/best-practices/#closing-words","text":"In general, this is a complete different type of testing designed to mimic a human testing your LLM application. You might need to use your brain a little bit to figure out to instruct your assistant on what to check.","title":"Closing words"},{"location":"guides/best-practices/#navigation","text":"Back to Home Installation Guide Quick Start Guide API Reference","title":"Navigation"},{"location":"guides/ci-cd/","text":"\u2190 Back to Home CI/CD Integration llmtest is designed for seamless integration with CI/CD pipelines. Setting Up CI/CD 1. Environment Setup Add your API keys as secrets in your CI/CD environment: # GitHub Actions example env: OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} # OR env: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }} 2. Test Configuration # tests/test_llm_app.py from llmtest.semanticassert.semantic_assert import SemanticAssertion def test_llm_output(): semantic_assert = SemanticAssertion() # Your tests here 3. CI/CD Pipeline Configuration GitHub Actions Example name: LLM Tests on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: '3.8' - name: Install dependencies run: | pip install -r requirements.txt pip install git+https://github.com/Shredmetal/llmtest.git - name: Run tests env: OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} run: pytest tests/ Best Practices API Key Management : Never commit API keys Use CI/CD environment secrets Rotate keys regularly Cost Control : Use cheaper models in CI/CD Run semantic tests only on critical paths Consider test result caching Pipeline Optimization : Run traditional tests first Run semantic tests in parallel Set appropriate timeouts Error Handling : Implement retry logic for API failures Log detailed error information Set up alerts for repeated failures Common Issues API Rate Limits : Implement exponential backoff Use test result caching Consider parallel test execution Cost Management : Monitor API usage Set budget alerts Use test filtering Navigation Back to Home Installation Guide Quick Start Guide API Reference","title":"Ci cd"},{"location":"guides/ci-cd/#cicd-integration","text":"llmtest is designed for seamless integration with CI/CD pipelines.","title":"CI/CD Integration"},{"location":"guides/ci-cd/#setting-up-cicd","text":"","title":"Setting Up CI/CD"},{"location":"guides/ci-cd/#1-environment-setup","text":"Add your API keys as secrets in your CI/CD environment: # GitHub Actions example env: OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} # OR env: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}","title":"1. Environment Setup"},{"location":"guides/ci-cd/#2-test-configuration","text":"# tests/test_llm_app.py from llmtest.semanticassert.semantic_assert import SemanticAssertion def test_llm_output(): semantic_assert = SemanticAssertion() # Your tests here","title":"2. Test Configuration"},{"location":"guides/ci-cd/#3-cicd-pipeline-configuration","text":"","title":"3. CI/CD Pipeline Configuration"},{"location":"guides/ci-cd/#github-actions-example","text":"name: LLM Tests on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: '3.8' - name: Install dependencies run: | pip install -r requirements.txt pip install git+https://github.com/Shredmetal/llmtest.git - name: Run tests env: OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} run: pytest tests/","title":"GitHub Actions Example"},{"location":"guides/ci-cd/#best-practices","text":"API Key Management : Never commit API keys Use CI/CD environment secrets Rotate keys regularly Cost Control : Use cheaper models in CI/CD Run semantic tests only on critical paths Consider test result caching Pipeline Optimization : Run traditional tests first Run semantic tests in parallel Set appropriate timeouts Error Handling : Implement retry logic for API failures Log detailed error information Set up alerts for repeated failures","title":"Best Practices"},{"location":"guides/ci-cd/#common-issues","text":"API Rate Limits : Implement exponential backoff Use test result caching Consider parallel test execution Cost Management : Monitor API usage Set budget alerts Use test filtering","title":"Common Issues"},{"location":"guides/ci-cd/#navigation","text":"Back to Home Installation Guide Quick Start Guide API Reference","title":"Navigation"}]}