{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"llm_app_test","text":""},{"location":"#quick-start","title":"Quick Start","text":"<ul> <li>Installation</li> <li>Quick Start Guide</li> <li>CI/CD Integration</li> <li>API Reference</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>A behavioral testing framework for applications using large language models (LLMs). It leverages LLMs to validate the behavior of applications containing LLMs against natural language test specifications (reliability validated through 30,000 test executions), providing a powerful tool for unit/integration testing of applications containing an LLM (not for testing LLMs themselves). </p> <p>Please use a proper data science tool to evaluate models, this is first and foremost an engineering tool for application testing.</p> <p>We made this because we were unsatisfied with existing approaches:</p> <ol> <li>String/regex matching and embeddings are too brittle - this is obvious for the former. Embeddings allow slightly more flexibility, but you still need to roughly guess what your LLM-powered app is going to say, and set things like thresholds while having an understanding of vector spaces.</li> <li>Academic metrics are of little help to API consumers like us with no ability to change the model. However, we still believe this tool is useful for the software engineering side of things. Please refer to the Testing Philosophy section below on when to send things back to the data scientists.</li> <li>We just wanted to define a behavior and assert on it.</li> </ol> <p>\u26a0\ufe0f Note on Reliability: While we cannot guarantee 100% reliability (due to the fundamental nature of LLMs), we validated the library with 30,000 test executions with zero format violations and non-determinism only occurring in one case containing a genuine semantic boundary. </p> <p>We stress that  past success doesn't guarantee future determinism - this is an unsolvable problem in LLM testing, but we've implemented extensive mitigations to make it as reliable as possible. We will continue to validate reliability through brute force testing and will report if issues are detected. Please refer to the Format Compliance Testing page, the Behavioral Testing Reliability testing page, and the Behavioral Testing Non-determinism At Semantic Boundary analysis page.</p>"},{"location":"#the-cool-stuff","title":"The Cool Stuff:","text":"<p>\u2728 Test your LLM apps in minutes, not hours</p> <p>\ud83d\ude80 CI/CD ready out of the box (Tested in GitHub Actions CI - Please let us know if it just works(tm) in other CI systems)</p> <p>\ud83d\udcb0 Cost-effective testing solution</p> <p>\ud83d\udd27 No infrastructure needed (Unless if you want to inject a custom LLM, please refer to the configuration page of the documentation for details)</p>"},{"location":"#library-reliability-testing","title":"Library Reliability Testing","text":"<p>Format Compliance Reliability Testing</p> <p>Behavioral Testing Reliability</p> <p>Behavioral Testing Non-determinism At Semantic Boundary - Discovery and Analysis</p>"},{"location":"#testing-philosophy","title":"Testing Philosophy","text":"<p>When integrating LLMs into your application, treat them as you would any closed-source third-party library:</p> <ol> <li>Write tests for expected behavior</li> <li>Focus on interface boundaries</li> <li>Test application-level functionality</li> <li>Maintain clear separation of concerns</li> </ol>"},{"location":"#important-information-on-understanding-responsibilities","title":"\u26a0\ufe0f Important Information on Understanding Responsibilities","text":"<p>This library is built by software engineers to give software engineers a tool to validate the behavior of applications that have had an LLM stuffed in them. It is NOT a Data Science tool nor a replacement for model metrics used by Data Science teams to validate model suitability.</p>"},{"location":"#software-engineers-role","title":"Software Engineer's Role","text":"<ul> <li>Write tests for expected application behavior</li> <li>Validate inputs and outputs</li> <li>Ensure proper integration</li> <li>Monitor system performance</li> <li>Escalate consistent failures to DS team (as this might indicate a fundamental problem with the model, or perhaps to seek assistance with the <code>expected_behavior</code> prompt in the <code>assert_behavioral_match</code> function)</li> </ul>"},{"location":"#data-science-teams-role","title":"Data Science Team's Role","text":"<ul> <li>Handle model-level issues</li> <li>Address consistent test failures</li> <li>Evaluate model suitability</li> <li>Optimise model performance</li> <li>Adjust prompts when needed</li> </ul>"},{"location":"#when-to-escalate","title":"When to Escalate","text":"<p>Escalate to your Data Science team when:</p> <ol> <li>Tests consistently fail despite correct implementation</li> <li>Model responses are consistently inappropriate</li> <li>Performance degradation is observed</li> <li>Pattern of failures indicates model-level issues</li> </ol>"},{"location":"#what-makes-this-different","title":"\ud83d\udd0d What Makes This Different?","text":"<p>This is an ENGINEERING tool, not a data science tool. The difference is crucial:</p> <p>Data Science Tools: - Test model performance - Evaluate model accuracy - Measure model metrics</p> <p>llm_app_test (Engineering Tool): - Tests your APPLICATION code - Validates integration points - Ensures system behavior - Maintains production reliability</p> <p>Think of it this way: You don't test Redis itself, you test your application's use of Redis.  Similarly, llm_app_test helps you test your application's use of LLMs.</p>"},{"location":"#testing-hierarchy","title":"Testing Hierarchy","text":"<p>llm-app-test is designed to complement existing approaches. We recommend this testing hierarchy:</p> <ol> <li> <p>Behavioral Testing (llm-app-test)</p> <ul> <li>Fast, cost-effective first line of testing</li> <li>Validates IF your LLM application is even working as intended</li> <li>Tests core functionality and behavior</li> <li>Must pass before proceeding to benchmarking</li> <li>Failure indicates fundamental problems with the application</li> </ul> </li> <li> <p>Benchmarking and Performance Evaluation</p> <ul> <li>Much slower and more expensive</li> <li>Only run AFTER behavioral tests pass</li> <li>Measures HOW WELL the application performs (in our view, this blurs the lines into LLM evaluation but it should still be done, just not as the first line of defence against broken apps due to the time and cost required)</li> <li>Tests performance metrics, response quality</li> <li>Used for optimization and model selection</li> </ul> </li> </ol> <p>Please visit the Testing Hierarchy Documentation for a visual representation and a more in-depth explanation.</p>"},{"location":"#need-testing-ideas-check-out-the-tests-we-used-to-test-llm_app_test-here","title":"Need testing ideas? Check out the tests we used to test llm_app_test here","text":""},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>CI/CD Integration</li> <li>Best Practices</li> <li>API Reference</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter issues:</p> <ol> <li>Create an issue on our GitHub repository</li> <li>Include your Python version and environment details</li> <li>Describe the problem you encountered with version 0.2.0b1</li> </ol>"},{"location":"#support","title":"\ud83c\udd98 Support","text":"<ul> <li>Discord: Join our community</li> <li>Issues: GitHub Issues</li> <li>Documentation: Full Docs</li> <li>Email: morganj.lee01@gmail.com</li> </ul>"},{"location":"#due-to-the-number-of-downloads-i-am-seeing-on-pypistatsorg-i-am-including-these-instructions-in-case-a-beta-update-breaks-something-on-your-end","title":"Due to the number of downloads I am seeing on pypistats.org, I am including these instructions in case a beta update breaks something on your end:","text":""},{"location":"#emergency-rollback-instructions","title":"Emergency Rollback Instructions","text":"<p>If you experience issues with version 0.2.0b1, you can roll back to the previous stable version (0.1.0b4) using one of these methods:</p>"},{"location":"#method-1-direct-installation-of-previous-version","title":"Method 1: Direct Installation of Previous Version","text":"<pre><code>pip uninstall llm-app-test \npip install llm-app-test==0.1.0b4\n</code></pre>"},{"location":"#method-2-force-reinstall-if-method-1-fails","title":"Method 2: Force Reinstall (if Method 1 fails)","text":"<pre><code>pip install --force-reinstall llm-app-test==0.1.0b4\n</code></pre>"},{"location":"#verification","title":"Verification","text":"<p>After rolling back, verify the installation: <pre><code>import llm_app_test \nprint(llm_app_test.version) # Should show 0.1.0b4\n</code></pre></p>"},{"location":"#important-note-about-rate-limits-if-running-large-numbers-of-tests","title":"\u26a0\ufe0f Important Note About Rate Limits - If Running Large Numbers of Tests:","text":""},{"location":"#anthropic-rate-limits","title":"Anthropic Rate limits:","text":"<p>Tier 1:</p> Model Maximum Requests per minute (RPM) Maximum Tokens per minute (TPM) Maximum Tokens per day (TPD) Claude 3.5 Sonnet 2024-10-22 50 40,000 1,000,000 Claude 3.5 Sonnet 2024-06-20 50 40,000 1,000,000 Claude 3 Opus 50 20,000 1,000,000 <p>Tier 2:</p> Model Maximum Requests per minute (RPM) Maximum Tokens per minute (TPM) Maximum Tokens per day (TPD) Claude 3.5 Sonnet 2024-10-22 1,000 80,000 2,500,000 Claude 3.5 Sonnet 2024-06-20 1,000 80,000 2,500,000 Claude 3 Opus 1,000 40,000 2,500,000"},{"location":"#openai-rate-limits","title":"OpenAI Rate Limits","text":"<p>Tier 1</p> Model RPM RPD TPM Batch Queue Limit gpt-4o 500 - 30,000 90,000 gpt-4o-mini 500 10,000 200,000 2,000,000 gpt-4o-realtime-preview 100 100 20,000 - gpt-4-turbo 500 - 30,000 90,000 <p>Tier 2:</p> Model RPM TPM Batch Queue Limit gpt-4o 5,000 450,000 1,350,000 gpt-4o-mini 5,000 2,000,000 20,000,000 gpt-4o-realtime-preview 200 40,000 - gpt-4-turbo 5,000 450,000 1,350,000"},{"location":"api/behavioral-assertion/","title":"BehavioralAssertion","text":"<p>\u2190 Back to Home</p>"},{"location":"api/behavioral-assertion/#behavioralassertion","title":"BehavioralAssertion","text":"<p>Core class for behavioral testing of LLM applications.</p>"},{"location":"api/behavioral-assertion/#constructor","title":"Constructor","text":"<pre><code>BehavioralAssertion(api_key: Optional[str] = None, \n                    llm: Optional[BaseLanguageModel] = None, \n                    provider: Optional[Union[str, LLMProvider]] = None, \n                    model: Optional[str] = None, \n                    temperature: Optional[float] = None, \n                    max_tokens: Optional[int] = None, \n                    max_retries: Optional[int] = None,\n                    timeout: Optional[float] = None,\n                    custom_prompts: Optional[AsserterPromptConfigurator] = None)\n</code></pre>"},{"location":"api/behavioral-assertion/#parameters","title":"Parameters","text":"<p>All parameters are optional (except for API key) and will use environment variables or defaults if not specified:</p> <ul> <li> <p>api_key: API key for the LLM provider</p> <ul> <li>Environment: OPENAI_API_KEY or ANTHROPIC_API_KEY</li> <li>Default: None (must be provided via environment or parameter unless using custom LLM, see <code>llm</code> parameter)</li> <li>If using default provider: use OPENAI_API_KEY since default provider is openai</li> </ul> </li> <li> <p>llm: Pre-configured LLM instance (must be of type <code>langchain_core.language_models import BaseLanguageModel</code>)</p> <ul> <li>Default: None (if provided, bypasses all other configuration)</li> </ul> </li> <li> <p>provider: LLM provider ('openai' or 'anthropic')</p> <ul> <li>Environment: LLM_PROVIDER</li> <li>Default: 'openai'</li> </ul> </li> <li> <p>model: Model name to use</p> <ul> <li>Environment: LLM_MODEL<ul> <li>Default: 'gpt-4o' for OpenAI, 'claude-3-5-sonnet-latest' for Anthropic</li> </ul> </li> </ul> </li> <li> <p>temperature: Temperature setting for LLM responses</p> <ul> <li> <p>Environment: LLM_TEMPERATURE</p> <ul> <li>Default: 0.0</li> <li>Range: 0.0 to 1.0</li> </ul> </li> </ul> </li> <li> <p>max_tokens: Maximum tokens for response</p> <ul> <li>Environment: LLM_MAX_TOKENS<ul> <li>Default: 4096</li> </ul> </li> </ul> </li> <li> <p>max_retries: Maximum number of API call retries</p> <ul> <li>Environment: LLM_MAX_RETRIES<ul> <li>Default: 2</li> </ul> </li> </ul> </li> <li> <p>timeout: Timeout for API calls in seconds</p> <ul> <li>Environment: LLM_TIMEOUT<ul> <li>Default: 60.0</li> </ul> </li> </ul> </li> <li> <p>custom_prompts: Custom prompts for asserter</p> <ul> <li>Default: None (uses default AsserterPromptConfigurator)</li> <li>Intentional added friction for custom prompts</li> <li>Please refer to this documentation on how to use it.</li> </ul> </li> </ul>"},{"location":"api/behavioral-assertion/#methods","title":"Methods","text":""},{"location":"api/behavioral-assertion/#assert_behavioral_match","title":"assert_behavioral_match","text":"<pre><code>def assert_behavioral_match(actual: str, expected_behavior: str ) -&gt; None\n</code></pre> <p>Asserts that actual output exhibits the expected behavior.</p>"},{"location":"api/behavioral-assertion/#parameters_1","title":"Parameters","text":"<ul> <li>actual: The actual output to test</li> <li>expected_behavior: Natural language description of expected behavior</li> </ul>"},{"location":"api/behavioral-assertion/#raises","title":"Raises","text":"<ul> <li>BehavioralAssertionError: If output doesn't exhibit expected behavior</li> <li>LLMConnectionError: If LLM service fails</li> <li>LLMConfigurationError: If configuration is invalid</li> <li>TypeError: If inputs are None</li> </ul>"},{"location":"api/behavioral-assertion/#examples","title":"Examples","text":""},{"location":"api/behavioral-assertion/#basic-usage","title":"Basic Usage","text":"<pre><code>asserter = BehavioralAssertion() # Uses environment variables \nasserter.assert_behavioral_match(\n    actual=\"Hello Alice, how are you?\", # In practice, use the output from your LLM application\n    expected_behavior=\"A greeting addressing Alice\" \n)\n</code></pre>"},{"location":"api/behavioral-assertion/#custom-configuration","title":"Custom Configuration","text":"<pre><code>asserter = BehavioralAssertion(\n    provider=\"anthropic\", \n    model=\"claude-3-5-sonnet-latest\", # Look I can't stop you from burning a hole in your wallet but please don't use Claude 3 Opus. \n    temperature=0.1,\n    timeout=15.0\n)\n</code></pre>"},{"location":"api/behavioral-assertion/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>Configuration Reference</li> </ul>"},{"location":"api/configuration/","title":"Configuration","text":"<p>\u2190 Back to Home</p>"},{"location":"api/configuration/#configuration","title":"Configuration","text":"<p>llm-app-test provides flexible configuration through environment variables and direct parameters.</p>"},{"location":"api/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"api/configuration/#provider-selection","title":"Provider Selection","text":"<pre><code>LLM_PROVIDER=openai # or 'anthropic', default is openai\n</code></pre>"},{"location":"api/configuration/#api-keys","title":"API Keys","text":"<pre><code># For OpenAI (default provider)\nOPENAI_API_KEY=your-openai-key\n\n# For Anthropic \nANTHROPIC_API_KEY=your-anthropic-key\n</code></pre>"},{"location":"api/configuration/#optional-settings","title":"Optional Settings","text":"<pre><code>LLM_MODEL=gpt-4o # Model name - default for OpenAI: gpt-4o, default for anthropic: claude-3-5-sonnet-latest\nLLM_TEMPERATURE=0.0 # Response randomness (0.0-1.0), default is 0.0\nLLM_MAX_TOKENS=4096 # Maximum response length, default is 4096\nLLM_MAX_RETRIES=2 # API retry attempts, default is 2\nLLM_TIMEOUT=60.0 # API timeout in seconds, default is 60.0\n</code></pre>"},{"location":"api/configuration/#direct-configuration","title":"Direct Configuration","text":"<p>You can also configure llm_app_test programmatically:</p> <p><pre><code>from llm_app_test.behavioral_assert.behavioral_assert import BehavioralAssertion\n\nasserter = BehavioralAssertion(\n    api_key=\"your-api-key\", # Strongly advised against, use env vars\n    provider=\"openai\", # or 'anthropic'\n    model=\"gpt-4o\", # See Supported Models\n    temperature=0.0, # Default: 0.0\n    max_tokens=4096, # Default: 4096\n    max_retries=2, # Default: 2\n    timeout=60.0 # Default: 60.0\n)\n</code></pre> An optional parameter for BehavioralAssertion is <code>custom_prompts</code>, see this page for details.</p>"},{"location":"api/configuration/#supported-and-recommended-models","title":"Supported (and recommended) Models","text":"<p>Model support is restricted due to the complex behavioral reasoning required for accurate testing. We've found that only the most advanced models can reliably handle behavioral comparison tasks.</p>"},{"location":"api/configuration/#openai","title":"OpenAI","text":"<ul> <li>gpt-4o</li> <li>gpt-4-turbo</li> </ul>"},{"location":"api/configuration/#anthropic","title":"Anthropic","text":"<ul> <li>claude-3-5-sonnet-latest</li> <li>claude-3-opus-latest (EXPENSIVE!!!)</li> </ul> <p>Note: GPT-4o is our recommended model because it was what we used to validate reliability, and has demonstrated a level of pedantry and literalism that we find is best for a testing system.</p>"},{"location":"api/configuration/#custom-llm-configuration-advanced-users-only","title":"Custom LLM Configuration (Advanced Users Only)","text":"<p>While it's possible to inject a custom LLM using Langchain's <code>BaseLanguageModel</code>, this is strongly discouraged unless you have extensively tested your model's behavioral reasoning capabilities. Smaller or less capable models will likely fail at the behavioral testing tasks. If you happen to have a datacenter in your back pocket however, Llama 3.1:405B might be a good bet.</p> <pre><code># Advanced usage - Only for thoroughly tested models\nfrom llm_app_test.behavioral_assert.behavioral_assert import BehavioralAssertion\nfrom langchain_core.language_models import BaseLanguageModel\n\ncustom_llm: BaseLanguageModel = your_custom_llm\nasserter = BehavioralAssertion(llm=custom_llm)\n</code></pre> <p>If you pass in a custom llm, it will disable ALL other LLM configuration options, and you have to configure that LLM yourself.</p>"},{"location":"api/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Configuration values are resolved in this order:</p> <ol> <li>Custom LLM (if provided, disables ALL other LLM settings and uses the settings of the Langchain BaseLanguageModel object that you have passed)</li> <li>Directly passed parameters</li> <li>Environment variables</li> <li>Default values</li> </ol> <p>Note: if a custom LLM is not passed, llm-app-test validates all configuration values at startup to prevent runtime errors.</p>"},{"location":"api/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for API keys</li> <li>Keep temperature at 0.0 for consistent testing</li> <li>Use default max_tokens unless you have specific needs</li> <li>Start with default max_retries (2)</li> <li>Stick with frontier models for the best results. This library is completely untested with anything other than frontier models.</li> </ol>"},{"location":"api/configuration/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"api/custom-prompt-configuration/","title":"Custom Prompt Configuration","text":"<p>\u2190 Back to Home</p>"},{"location":"api/custom-prompt-configuration/#custom-prompt-configuration-understanding-the-options","title":"Custom Prompt Configuration - Understanding the Options","text":"<p>\u26a0\ufe0f Important Notice Custom prompt configuration is an advanced feature that can affect the reliability of your testing framework if not used carefully. We strongly recommend using the default prompts unless you have a specific, well-understood need for customization.</p>"},{"location":"api/custom-prompt-configuration/#default-prompts","title":"Default Prompts","text":"<p>The library uses these carefully designed default prompts that have been tested across thousands (&gt;30,000) of iterations:</p> <p>System Prompt: <pre><code>You are a testing system. Your job is to determine if an actual output matches the expected behavior.\n\nImportant: You can only respond with EXACTLY: \n1. 'PASS' if it matches, or \n2. 'FAIL: &lt;reason&gt;' if it doesn't match.\n\nAny other type of response will mean disaster which as a testing system, you are meant to prevent.\n\nBe strict but consider semantic meaning rather than exact wording.\n</code></pre></p> <pre><code>Human Prompt:\nExpected Behavior: {expected_behavior}\n\nActual Output: {actual}\n\nDoes the actual output match the expected behavior? Remember, you will fail your task unless you respond EXACTLY \nwith 'PASS' or 'FAIL: &lt;reason&gt;'.\n</code></pre>"},{"location":"api/custom-prompt-configuration/#why-custom-prompts-require-careful-consideration","title":"Why Custom Prompts Require Careful Consideration","text":"<ol> <li> <p>Format Reliability:</p> <ul> <li>Custom prompts might not enforce the strict PASS/FAIL format</li> <li>This leads to runtime errors when the LLM returns unexpected formats</li> <li>The default prompts are specifically designed to maintain format compliance</li> <li>Format compliance is critical - any deviation can cause immediate test failure</li> </ul> </li> <li> <p>Testing Consistency:</p> <ul> <li>Custom prompts can change how \"matching\" is interpreted</li> <li>This leads to inconsistent test results</li> <li>The default prompts maintain reliable behavioral testing</li> <li>Behavioral testing requires consistent interpretation of \"matching\"</li> </ul> </li> <li> <p>Non-determinism:</p> <ul> <li>Custom prompts haven't been tested at scale</li> <li>May introduce additional non-deterministic behavior</li> <li>Default prompts are validated across thousands of tests</li> </ul> </li> </ol>"},{"location":"api/custom-prompt-configuration/#why-allow-custom-prompts","title":"Why Allow Custom Prompts?","text":"<p>While custom prompts are risky, we recognise that some testing scenarios might require specialized evaluation criteria. However, this should be extremely rare - the default prompts have been proven reliable across a wide range of testing scenarios.</p>"},{"location":"api/custom-prompt-configuration/#if-you-must-use-custom-prompts","title":"If You Must Use Custom Prompts","text":"<p>If you absolutely need to use custom prompts:</p> <ol> <li> <p>Maintain Format Requirements:</p> <ul> <li>MUST enforce PASS/FAIL format - Take a look at the default prompts for an example.</li> <li>MUST include reason for FAIL</li> <li>MUST prevent free-form responses</li> </ul> </li> <li> <p>Test Extensively:</p> <ul> <li>Run at least 1000 iterations</li> <li>Verify format compliance</li> <li>Check for behavioral consistency</li> </ul> </li> <li> <p>Documentation:</p> <ul> <li>Document why custom prompts are needed</li> <li>Detail all modifications made</li> <li>Include reliability implications</li> </ul> </li> </ol>"},{"location":"api/custom-prompt-configuration/#example-of-problematic-custom-prompt-and-custom-prompt-example-bad-example-but-correct-syntax","title":"Example of Problematic Custom Prompt and Custom Prompt Example (Bad Example but Correct Syntax)","text":"<p>DO NOT USE - For Illustration Only:</p> <pre><code>custom_prompts = AsserterPromptConfigurator( \n    system_prompt=\"You are a testing system that only responds with PASS or FAIL: reason...\", \n    human_prompt=\"Compare:\\nExpected: {expected_behavior}\\nActual: {actual}\" \n    )\n\nasserter = BehavioralAssertion(\n    provider=\"openai\",\n    custom_prompts=custom_prompts\n)\n\n# The assertion works the same way\n\nasserter.assert_behavioral_match(\n    actual=\"Hello, world!\",\n    expected_behavior=\"Should greet the world\"\n)\n</code></pre> <p>This example is problematic because it: - Doesn't enforce strict format requirements - Lacks behavioral evaluation guidance - Missing format compliance constraints - No safeguards against non-determinism</p>"},{"location":"api/custom-prompt-configuration/#recommended-approach","title":"Recommended Approach","text":"<ol> <li>Start with default prompts</li> <li>Document why they don't meet your needs</li> <li>Consult the development team</li> <li>Consider alternative solutions</li> <li>Only then, if absolutely necessary, create custom prompt configurations</li> </ol>"},{"location":"api/custom-prompt-configuration/#quick-links","title":"Quick Links","text":"<ul> <li>Behavioral Testing Reliability</li> <li>Format Compliance</li> <li>Testing Best Practices</li> </ul>"},{"location":"api/error-handling/","title":"Error Handling","text":"<p>\u2190 Back to Home</p>"},{"location":"api/error-handling/#error-handling","title":"Error Handling","text":"<p>llm-app-test provides specific exceptions for different error scenarios to help you handle errors gracefully in your tests.</p>"},{"location":"api/error-handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>LLMAppTestError # Base exception for all llm-app-test errors \n\u251c\u2500\u2500 BehavioralAssertionError # When behavioral matching fails \n\u251c\u2500\u2500 LLMConfigurationError # When configuration is invalid \n\u2514\u2500\u2500 LLMConnectionError # When LLM service fails \n</code></pre>"},{"location":"api/error-handling/#exception-details","title":"Exception Details","text":""},{"location":"api/error-handling/#behavioralassertionerror","title":"BehavioralAssertionError","text":"<p>Raised when the behavioral assertion fails:</p> <pre><code>try:\n    behavioral_assert.assert_behavioral_match(\n        actual=\"Hello Bob\", \n        expected_behavior=\"A greeting for Alice\"\n    ) \nexcept BehavioralAssertionError as e:\n    print(f\"Test failed: {e}\") # Includes detailed reason why outputs don't match\n</code></pre>"},{"location":"api/error-handling/#llmconfigurationerror","title":"LLMConfigurationError","text":"<p>Raised when configuration is invalid:</p> <pre><code>try:\n    behavioral_assert = BehavioralAssertion(provider=\"invalid_provider\") \nexcept LLMConfigurationError as e:\n    print(f\"Configuration error: {e}\") # Details about invalid configuration\n</code></pre>"},{"location":"api/error-handling/#llmconnectionerror","title":"LLMConnectionError","text":"<p>Raised when LLM service fails:</p> <pre><code>try:\n    behavioral_assert.assert_behavioral_match(...) \nexcept LLMConnectionError as e:\n    print(f\"Service error: {e}\") # Contains original provider error details\n</code></pre>"},{"location":"api/error-handling/#error-information","title":"Error Information","text":"<p>All exceptions provide:</p> <ul> <li>Clear error message</li> <li>Detailed reason (when available)</li> <li>Additional context in details dictionary</li> </ul> <p>Example:</p> <pre><code>try:\n    behavioral_assert.assert_behavioral_match(...) \nexcept LLMAppTestError as e:\n    print(f\"Message: {e.message}\")\n    print(f\"Reason: {e.reason}\")\n    print(f\"Details: {e.details}\")\n</code></pre>"},{"location":"api/error-handling/#best-practices","title":"Best Practices","text":"<ol> <li>Always catch specific exceptions rather than the base LLMAppTestError</li> <li>Log the full error information for debugging</li> <li>Handle LLMConnectionError for retry logic</li> <li>Use error details for test reporting</li> </ol> <p>Note: All exceptions properly chain to their root cause, preserving the full error context.</p>"},{"location":"api/error-handling/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>\u2190 Back to Home</p>"},{"location":"getting-started/installation/#installation","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip (package installer for Python)</li> </ul>"},{"location":"getting-started/installation/#installing-llm_app_test","title":"Installing llm_app_test","text":"<p>Install from PyPI:</p> <pre><code>pip install llm-app-test\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to llm_app_test, install with development dependencies:</p> <pre><code># Clone the repository\n\ngit clone https://github.com/Shredmetal/llmtest.git\n\n# Change to project directory\n\ncd llm_app_test\n\n# Install with development dependencies\n\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Learn how to use llm_app_test</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>\u2190 Back to Home</p>"},{"location":"getting-started/quickstart/#quick-start","title":"Quick Start","text":"<p>Get up and running with llm-app-test in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#1-set-up-environment","title":"1. Set Up Environment","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># For OpenAI\nOPENAI_API_KEY=your-openai-api-key-here\n\n# OR for Anthropic\nANTHROPIC_API_KEY=your-anthropic-key-here\n</code></pre>"},{"location":"getting-started/quickstart/#2-write-your-first-test","title":"2. Write Your First Test","text":"<pre><code>from llm_app_test.behavioral_assert.behavioral_assert import BehavioralAssertion\n\ndef my_first_behavioral_test(): \n    # Initialize asserter \n    behavioral_assert = BehavioralAssertion()\n\n    # Your LLM output\n    actual_output = \"The sky is blue\"\n\n    # Expected behavior in natural language\n    expected_behavior = \"A statement about the color of the sky\"\n\n    # Test behavioral equivalence\n    behavioral_assert.assert_behavioral_match(\n        actual=actual_output,\n        expected_behavior=expected_behavior\n    )\n</code></pre>"},{"location":"getting-started/quickstart/#3-run-your-test","title":"3. Run Your Test","text":"<pre><code>pytest my_first_behavioral_test.py\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>CI/CD Integration - Set up automated testing</li> <li>Configuration - Configure llm_app_test for your needs</li> </ul>"},{"location":"getting-started/quickstart/#additional-notes","title":"Additional notes:","text":""},{"location":"getting-started/quickstart/#real-world-example","title":"Real World Example","text":"<p>Here's a powerful example showing behavioral testing in action:</p> <pre><code>from langchain_core.messages import SystemMessage, HumanMessage\nfrom llm_app_test.behavioral_assert.behavioral_assert import BehavioralAssertion\nfrom your_bot_module import SimpleApiCallBot  # Your LLM wrapper\n\ndef test_ww2_narrative():\n    behavioral_assert = BehavioralAssertion()\n\n    # Define the bot's behavior with a system message\n    system_message = SystemMessage(\n        \"\"\"\n        You are a historian bot and you will respond to specific requests \n        for information about history. Be detailed but do not go beyond \n        what was asked for.\n        \"\"\"\n    )\n\n    # Initialize the bot\n    # This is a simple API call to openAI - you can find this in our tests/actual_usage_tests directory in the repo\n    bot = SimpleApiCallBot(system_message=system_message) \n\n    # Create the user's request\n    human_message = HumanMessage(\n        content=\"Tell me about the European Theater of World War 2, the major battles, and how the European war ended\"\n    )\n\n    # Get the bot's response\n    actual_output = bot.generate_ai_response(human_message)\n\n    # Define expected behavior\n    expected_behavior = \"\"\"\n    A narrative about World War 2 and the global nature of the war\n    \"\"\"\n\n    # This will fail because the bot's response focuses only on Europe\n    behavioral_assert.assert_behavioral_match(actual_output, expected_behavior)\n</code></pre> <p>Actual bot response from one run:</p> <pre><code>The European Theater of World War II was a significant front in the global conflict that lasted from 1939 to 1945. It involved most of the countries of Europe and was marked by numerous major battles and campaigns. Here is an overview of some of the key events and battles:\n\n1. **Invasion of Poland (1939):** The war in Europe began with Germany's invasion of Poland on September 1, 1939. This prompted Britain and France to declare war on Germany. The swift German victory was achieved through the use of Blitzkrieg tactics.\n\n2. **Battle of France (1940):** In May 1940, Germany launched an invasion of France and the Low Countries. The German forces bypassed the heavily fortified Maginot Line and quickly advanced through the Ardennes, leading to the fall of France in June 1940.\n\n3. **Battle of Britain (1940):** Following the fall of France, Germany attempted to gain air superiority over Britain in preparation for an invasion. The Royal Air Force successfully defended the UK, marking the first major defeat for Hitler's military forces.\n\n4. **Operation Barbarossa (1941):** On June 22, 1941, Germany launched a massive invasion of the Soviet Union. This campaign opened the Eastern Front, which became the largest and bloodiest theater of war in World War II.\n\n5. **Battle of Stalingrad (1942-1943):** One of the deadliest battles in history, the Battle of Stalingrad was a turning point on the Eastern Front. The Soviet victory marked the beginning of a major offensive push against German forces.\n\n6. **North African Campaign (1940-1943):** This series of battles involved the Allies and Axis powers fighting for control of North Africa. The decisive Allied victory at the Second Battle of El Alamein in 1942 marked the beginning of the end for Axis forces in Africa.\n\n7. **Invasion of Italy (1943):** After the successful North African Campaign, the Allies invaded Sicily in July 1943 and then mainland Italy. This led to the fall of Mussolini's regime and Italy's eventual surrender, although fighting continued in Italy until 1945.\n\n8. **D-Day and the Battle of Normandy (1944):** On June 6, 1944, Allied forces launched Operation Overlord, the largest amphibious invasion in history, landing on the beaches of Normandy, France. This marked the beginning of the liberation of Western Europe from Nazi occupation.\n\n9. **Battle of the Bulge (1944-1945):** Germany's last major offensive on the Western Front took place in the Ardennes Forest. Despite initial successes, the Allies eventually repelled the German forces, leading to a rapid advance into Germany.\n\n10. **Fall of Berlin (1945):** The final major offensive in Europe was the Soviet assault on Berlin in April 1945. The city fell on May 2, 1945, leading to the suicide of Adolf Hitler and the unconditional surrender of German forces.\n\nThe European war officially ended with Germany's unconditional surrender on May 7, 1945, which was ratified on May 8, known as Victory in Europe (VE) Day. This marked the end of World War II in Europe, although the war continued in the Pacific until Japan's surrender in September 1945.\n</code></pre> <p>Error message thrown by <code>assert_behavioral_match</code>:</p> <pre><code>E           llm_app_test.exceptions.test_exceptions.BehavioralAssertionError: Behavioral assertion failed: \nBehavioral Assertion Failed:  - Reason: The actual output focuses primarily on the European Theater of World War II, \nrather than providing a narrative about the global nature of the war.\n</code></pre>"},{"location":"getting-started/quickstart/#what-this-example-demonstrates","title":"What This Example Demonstrates","text":"<ol> <li> <p>Real Application Testing</p> <ul> <li>Tests an actual LLM application</li> </ul> </li> <li> <p>Behavioral Testing Power</p> <ul> <li>The bot provides a detailed, accurate response</li> <li>However, the test fails because it doesn't meet the expected behavior</li> <li>Shows how behavioral testing catches incorrect behavior from your app</li> </ul> </li> <li> <p>Clear Error Messages</p> <ul> <li>The error clearly explains why the test failed</li> <li>Points to specific behavioral mismatch</li> <li>Helps developers understand what needs to change</li> </ul> </li> </ol>"},{"location":"getting-started/quickstart/#next-steps_1","title":"Next Steps","text":"<ul> <li>CI/CD Integration - Set up automated testing</li> <li>Configuration - Configure llm-app-test for your needs</li> <li>Behavioral Testing Reliability - Learn about testing at scale</li> </ul>"},{"location":"getting-started/quickstart/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Configuration</li> <li>API Reference</li> </ul>"},{"location":"guides/best-practices/","title":"Best Practices","text":"<p>\u2190 Back to Home</p>"},{"location":"guides/best-practices/#best-practices","title":"Best Practices","text":"<p>Guidelines for effective behavioral testing with llm_app_test.</p>"},{"location":"guides/best-practices/#understanding-behavioral-testing","title":"Understanding Behavioral Testing","text":"<p>When testing LLM applications, focus on describing what your application should do rather than how it should do it.</p> <p>Behavioral testing focuses on validating that outputs exhibit expected characteristics and behaviors in a declarative manner. For example:</p> <p>Expected Behavior: </p> <pre><code>\"A response that provides a weather forecast including temperature and conditions\"\n</code></pre> <p>These responses would PASS:</p> <pre><code>actual_1 = \"Today will be sunny with a high of 75\u00b0F\"\nactual_2 = \"Expect cloudy skies and temperatures around 24\u00b0C\"\nactual_3 = \"The forecast shows clear weather, reaching 298K\"\n</code></pre> <p>These would FAIL:</p> <pre><code>fail_1 = \"Have a nice day!\" (missing forecast elements)\nfail_2 = \"It's weather time!\" (missing required information)\nfail_3 = \"75 degrees\" (incomplete behavior)\n</code></pre>"},{"location":"guides/best-practices/#writing-good-expected-behaviors","title":"Writing Good Expected Behaviors","text":"<ol> <li> <p>Be Specific:</p> <pre><code># Good\nexpected_behavior = \"\"\" A polite greeting that:\n                    Addresses the person by name (Alice)\n                    Asks about their wellbeing \"\"\"\n\n# Bad - too vague\nexpected_behavior = \"A nice greeting\"\n</code></pre> </li> <li> <p>Focus on Behaviors:</p> <pre><code># Good\nexpected_behavior = \"\"\" An error message that:\n                    Explains the API key is invalid\n                    Provides steps to fix the issue \"\"\"\n\n# Bad - testing exact wording - DO NOT USE llm_app_test FOR THIS, USE REGULAR PYTEST\nexpected_behavior = \"Should say 'Invalid API key'\"\n</code></pre> </li> </ol>"},{"location":"guides/best-practices/#when-to-use-behavioral-testing","title":"When to Use Behavioral Testing","text":"<p>Good Use Cases:</p> <ul> <li>Testing LLM outputs</li> <li>Validating natural language responses</li> <li>Testing content generation</li> <li>Checking response behaviors</li> </ul> <p>Not Suitable For:</p> <ul> <li>Exact string matching</li> <li>Numerical comparisons</li> <li>Binary conditions</li> </ul>"},{"location":"guides/best-practices/#test-structure","title":"Test Structure","text":"<ol> <li> <p>Test Scope Based on Behavioral Requirements:</p> <pre><code># Testing Single Behavior \ndef test_greeting_format(): \n  \"\"\"Test that greeting follows required format\"\"\"\n</code></pre> <pre><code># Testing Multiple Related Behaviors\ndef test_patient_education_diabetes():\n\n   expected = \"\"\"Test comprehensive diabetes education document including:\n    - Overview section\n    - Numerical guidelines\n    - Structured sections\n    - Warning signs\n    - Follow-up instructions\"\"\"\n</code></pre> </li> <li> <p>Choose Test Scope Based on Document Purpose:</p> <ul> <li>Single behavior tests for simple, focused requirements</li> <li>Comprehensive behavior tests for complete documents</li> <li>Let document purpose drive test structure</li> </ul> </li> <li> <p>Clear Test Names:</p> <pre><code># Good\ndef test_error_message_includes_solution_steps():\n\n# Bad\ndef test_error():\n</code></pre> </li> </ol>"},{"location":"guides/best-practices/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li> <p>Over-Specific Expected Behaviors:</p> <pre><code># Too specific\nexpected = \"Must say hello and use exactly these words\"\n\n# Better\nexpected = \"Should be a greeting in conversational English\"\n</code></pre> </li> <li> <p>Under-Specific Expected Behaviors:</p> <pre><code># Too vague\nexpected = \"Should be good\"\n\n# Better\nexpected = \"\"\" Response should:\n           Answer the user's question\n           Use professional language\n           Stay on topic \"\"\"\n</code></pre> </li> </ol>"},{"location":"guides/best-practices/#cost-optimisation","title":"Cost Optimisation","text":"<ol> <li>Use specific, focused tests</li> <li>Group related behavioral tests</li> <li>Consider test importance vs cost</li> </ol>"},{"location":"guides/best-practices/#model-selection","title":"Model Selection","text":"<p>We ran extensive tests on our library with GPT-4o, and limited tests with Claude 3.5 Sonnet.</p> <p>What we found is that Claude is a little more \"lax\" (for want of a better word). While GPT-4o will adopt a more strict and literal interpretation, Claude has shown a tendency to be more lenient (and seems to try to pass cases based on \"intent\"), and adopt a less pedantic stance on what constitutes meeting the requirements.</p> <p>Let's look at the example from our behavioral testing at semantic boundaries documentation:</p> <p>In <code>actual</code> (i.e. the simulated LLM output): <pre><code>Warning Signs:\nLearn to recognize and respond to:\n- Hypoglycemia (low blood sugar): shakiness, sweating, confusion\n- Hyperglycemia (high blood sugar): increased thirst, frequent urination, fatigue\nSeek immediate medical attention if you experience severe symptoms or sustained \nhigh blood sugar levels.\n</code></pre></p> <p>In <code>expected_behavior</code> (i.e. the specification we expect developers to write when using our library):</p> <p>Version that led to non-determinism (14 out of 600 runs with GPT-4o did not fail this test case for not providing emergency response steps): <pre><code>4. Provide clear warning signs and emergency response steps\n</code></pre></p> <p>Updated version that led to determinism (1,200 out of 1,200 test runs were correctly failed): <pre><code>4. Provide clear warning signs AND detailed emergency response procedures\n</code></pre></p> <p>However, Claude 3.5 Sonnet does not consistently fail this test case even when asked for detailed emergency response procedures (no volume testing done but significant non-determinism observed in &lt;100 runs).</p> <p>Claude actually needs to specifically be told to look for multi-step emergency response procedures like so:</p> <pre><code>4. Provide clear warning signs AND detailed multi-step emergency response procedures\n</code></pre> <p>Needless to say, this is generally NOT what you want in a testing system, unless if you want to simulate a flexible developer manually testing your bot. We do not recommend this. We recommend writing test cases with the expectation that the LLM will apply a strict literal interpretation.</p> <p>Therefore, we recommend sticking to the library default of GPT-4o, unless if you have a specific reason for doing otherwise.</p>"},{"location":"guides/best-practices/#closing-words","title":"Closing words","text":"<p>This testing approach is designed to mimic how a human would validate your LLM application's behavior.  Think about what behaviors you want to verify and express them clearly in natural language.</p>"},{"location":"guides/best-practices/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"guides/ci-cd/","title":"CI/CD Integration","text":"<p>\u2190 Back to Home</p>"},{"location":"guides/ci-cd/#cicd-integration","title":"CI/CD Integration","text":"<p>llm_app_test is designed for seamless integration with CI/CD pipelines.</p>"},{"location":"guides/ci-cd/#setting-up-cicd","title":"Setting Up CI/CD","text":""},{"location":"guides/ci-cd/#1-environment-setup","title":"1. Environment Setup","text":"<p>Add your API keys as secrets in your CI/CD environment:</p> <pre><code>env:\n  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n\n# OR\nenv:\n  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n</code></pre>"},{"location":"guides/ci-cd/#2-test-configuration","title":"2. Test Configuration","text":"<pre><code>from llm_app_test.behavioral_assert.behavioral_assert import BehavioralAssertion\n\ndef test_llm_output():\n    behavioral_assert = BehavioralAssertion() # Your tests here\n</code></pre>"},{"location":"guides/ci-cd/#3-cicd-pipeline-configuration","title":"3. CI/CD Pipeline Configuration","text":""},{"location":"guides/ci-cd/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: LLM Application Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install llm-app-test\n\n      - name: Run tests\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: pytest tests/\n</code></pre>"},{"location":"guides/ci-cd/#best-practices","title":"Best Practices","text":"<ol> <li> <p>API Key Management:</p> <ul> <li>Never commit API keys</li> <li>Use CI/CD environment secrets</li> <li>Rotate keys regularly</li> </ul> </li> <li> <p>Cost Control:</p> <ul> <li>Run behavioral tests only on critical paths</li> <li>Consider test result caching</li> </ul> </li> <li> <p>Pipeline Optimization:</p> <ul> <li>Run traditional tests first</li> <li>Run behavioral tests in parallel</li> <li>Set appropriate timeouts</li> </ul> </li> <li> <p>Error Handling:</p> <ul> <li>Implement retry logic for API failures (though Langchain should have this covered)</li> <li>Log detailed error information</li> <li>Set up alerts for repeated failures</li> </ul> </li> </ol>"},{"location":"guides/ci-cd/#common-issues","title":"Common Issues","text":"<ol> <li> <p>API Rate Limits:</p> <ul> <li>Implement exponential backoff (configuration and integration of Langchain implementation planned for future version)</li> <li>Use test result caching</li> <li>Consider parallel test execution</li> </ul> </li> <li> <p>Cost Management:</p> <ul> <li>Monitor API usage</li> <li>Set budget alerts</li> <li>Use test filtering</li> </ul> </li> </ol>"},{"location":"guides/ci-cd/#navigation","title":"Navigation","text":"<ul> <li>Back to Home</li> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>API Reference</li> </ul>"},{"location":"guides/testing-hierarchy/","title":"Testing Hierarchy","text":""},{"location":"guides/testing-hierarchy/#testing-hierarchy","title":"Testing Hierarchy","text":"<p>llm-app-test is designed to complement existing approaches. We recommend this testing hierarchy:</p> <ol> <li> <p>Behavioral Testing (llm-app-test)</p> <ul> <li>Fast, cost-effective first line of testing</li> <li>Validates IF your LLM application is even working as intended</li> <li>Tests core functionality and behavior</li> <li>Must pass before proceeding to benchmarking</li> <li>Failure indicates fundamental problems with the application</li> </ul> </li> <li> <p>Benchmarking and Performance Evaluation</p> <ul> <li>Much slower and more expensive</li> <li>Only run AFTER behavioral tests pass</li> <li>Measures HOW WELL the application performs (in our view, this blurs the lines into LLM evaluation but it should still be done, just not as the first line of defence against broken apps due to the time and cost required)</li> <li>Tests performance metrics, response quality</li> <li>Used for optimization and model selection</li> </ul> </li> </ol> <p>[!IMPORTANT] Always ensure behavioral tests pass before running benchmarks.  A failing behavioral test indicates your application is fundamentally  broken - no amount of performance optimization will fix incorrect behavior.</p>"},{"location":"guides/testing-hierarchy/#example-flow","title":"Example Flow:","text":"<pre><code># 1. First, run behavioral tests\nbehavioral_asserter.assert_behavioral_match(result, \"Expected behavior description\")\n\n# 2. Only if behavioral tests pass, run benchmarks\nif behavioral_tests_pass: \n    run_performance_benchmarks()\n</code></pre> <p>This hierarchy ensures:</p> <ul> <li>Core functionality is correct before optimization</li> <li>Clear separation of behavior and performance testing</li> <li>Efficient use of compute resources and API calls</li> <li>Structured approach to LLM application testing</li> </ul>"},{"location":"guides/testing-patterns/","title":"Testing Patterns","text":"<p>This guide demonstrates common behavioral testing patterns using llm-app-test. For API reference and syntax, see API Documentation.</p>"},{"location":"guides/testing-patterns/#pattern-basic-behavioral-testing","title":"Pattern: Basic Behavioral Testing","text":"<p>Scenario: Testing simple greeting behaviour</p> <p>Implementation:</p> <pre><code>actual = \"Hello Alice, how are you today?\" \n\nexpected_behavior = \"\"\"A polite greeting that:\n    Addresses the person by name (Alice)\n    Asks about their wellbeing\"\"\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> <p>Result: \u2705 PASS</p> <ul> <li>Recognises personal address</li> <li>Identifies greeting context</li> <li>Validates wellbeing inquiry</li> </ul>"},{"location":"guides/testing-patterns/#pattern-basic-behavioral-matching","title":"Pattern: Basic Behavioral Matching","text":"<p>Scenario: Testing simple factual statements</p> <p>Implementation:</p> <p><pre><code>actual = \"The sky is blue\"\n\nexpected_behavior = \"A statement about the color of the sky\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Shows direct behavioral matching</li> <li>Clear relationship between statement and expectation</li> <li>Passes when meaning aligns</li> </ul>"},{"location":"guides/testing-patterns/#pattern-expected-behavioral-mismatch","title":"Pattern: Expected Behavioral Mismatch","text":"<p>Scenario: Validating behavioral mismatch detection</p> <p>Implementation:</p> <p><pre><code>actual = \"The sky is blue\"\n\nexpected_behavior = \"A statement about the weather forecast\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because the actual statement is about sky colour</li> <li>Expected behaviour asks for weather forecast information</li> <li>Demonstrates how behavioral mismatches are caught</li> <li>Shows when assertions will fail in your tests</li> </ul>"},{"location":"guides/testing-patterns/#pattern-multilingual-behavioral-testing","title":"Pattern: Multilingual Behavioral Testing","text":"<p>Scenario: Testing behavioral understanding across languages</p> <p>Implementation:</p> <p><pre><code>actual = \"Bonjour, comment allez-vous?\"\n\nexpected_behavior = \"A polite greeting asking about wellbeing\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Demonstrates language-agnostic understanding</li> <li>Shows cross-language behavioral matching</li> <li>Validates international content handling</li> </ul>"},{"location":"guides/testing-patterns/#pattern-technical-documentation-testing","title":"Pattern: Technical Documentation Testing","text":"<p>Scenario: Testing technical concept explanations</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"The TCP handshake is a three-way process where the client \n         sends SYN, server responds with SYN-ACK, and client confirms with ACK\"\"\"\n\nexpected_behavior = \"An explanation of the TCP connection establishment process\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Validates technical accuracy</li> <li>Handles specialised terminology</li> <li>Maintains precision in behavioral assessment</li> </ul>"},{"location":"guides/testing-patterns/#pattern-contextual-disambiguation","title":"Pattern: Contextual Disambiguation","text":"<p>Scenario: Testing understanding of ambiguous terms</p> <p>Implementation:</p> <p><pre><code>actual = \"The bank was steep and covered in wildflowers\"\n\nexpected_behavior = \"A description of a riverbank or hillside, not a financial institution\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Shows contextual understanding</li> <li>Handles ambiguous terms</li> <li>Validates specific meaning exclusions</li> </ul>"},{"location":"guides/testing-patterns/#pattern-sentiment-analysis","title":"Pattern: Sentiment Analysis","text":"<p>Scenario: Testing subtle emotional content</p> <p>Implementation:</p> <p><pre><code>actual = \"While the presentation wasn't perfect, it showed promise\"\n\nexpected_behavior = \"A constructive criticism with mixed but generally positive sentiment\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Detects nuanced sentiment</li> <li>Understands mixed emotions</li> <li>Validates overall tone</li> </ul>"},{"location":"guides/testing-patterns/#pattern-long-form-content","title":"Pattern: Long-Form Content","text":"<p>Scenario: Testing comprehension of detailed explanations</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"Machine learning is a subset of artificial intelligence \n         that enables systems to learn and improve from experience without \n         explicit programming. It focuses on developing computer programs \n         that can access data and use it to learn for themselves.\"\"\"\n\nexpected_behavior = \"A comprehensive definition of machine learning emphasizing autonomous learning and data usage\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles longer text</li> <li>Maintains context</li> <li>Captures key concepts</li> </ul>"},{"location":"guides/testing-patterns/#pattern-subtle-sentiment-mismatch","title":"Pattern: Subtle Sentiment Mismatch","text":"<p>Scenario: Testing detection of subtle sentiment differences</p> <p>Implementation:</p> <p><pre><code>actual = \"The project was completed on time, though there were some hiccups\"\n\nexpected_behavior = \"A statement expressing complete satisfaction with project execution\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because actual statement indicates mixed satisfaction</li> <li>Expected behaviour suggests complete satisfaction</li> <li>Shows sensitivity to subtle emotional differences</li> </ul>"},{"location":"guides/testing-patterns/#pattern-technical-context-mismatch","title":"Pattern: Technical Context Mismatch","text":"<p>Scenario: Testing technical meaning precision</p> <p>Implementation:</p> <p><pre><code>actual = \"The function returns a pointer to the memory address\"\n\nexpected_behavior = \"A description of a function that returns the value stored at a memory location\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because returning a pointer is different from returning a stored value</li> <li>Shows precision in technical context</li> <li>Validates technical accuracy</li> </ul>"},{"location":"guides/testing-patterns/#pattern-ambiguous-reference-testing","title":"Pattern: Ambiguous Reference Testing","text":"<p>Scenario: Testing handling of context-dependent terms</p> <p>Implementation:</p> <p><pre><code>actual = \"The bank processed the transaction after reviewing the account\"\n\nexpected_behavior = \"A description of a riverbank's geological formation process\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because contexts are completely different</li> <li>Shows strong contextual understanding</li> <li>Validates semantic boundaries in behavioral matching</li> </ul>"},{"location":"guides/testing-patterns/#pattern-temporal-context","title":"Pattern: Temporal Context","text":"<p>Scenario: Testing time-based behavioral understanding</p> <p>Implementation:</p> <p><pre><code>actual = \"I will have completed the task by tomorrow\"\n\nexpected_behavior = \"A statement about a task that was completed in the past\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because of tense mismatch</li> <li>Shows temporal awareness</li> <li>Validates time context</li> </ul>"},{"location":"guides/testing-patterns/#pattern-logical-implication","title":"Pattern: Logical Implication","text":"<p>Scenario: Testing logical relationship understanding</p> <p>Implementation:</p> <p><pre><code>actual = \"If it rains, the ground will be wet\"\n\nexpected_behavior = \"A statement indicating that wet ground always means it has rained\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because of reversed logical implication</li> <li>Shows logical relationship understanding</li> <li>Validates causality direction</li> </ul>"},{"location":"guides/testing-patterns/#pattern-multi-step-reasoning","title":"Pattern: Multi-Step Reasoning","text":"<p>Scenario: Testing complex logical chains</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"When water freezes, it expands by approximately 9% in volume. \nThis expansion creates less dense ice that floats according to Archimedes' principle of displacement. \nBecause Arctic sea ice is already floating in the ocean, its melting doesn't significantly affect sea levels - \nit's already displacing its weight in water. However, land-based glaciers in places like Greenland \naren't currently displacing any ocean water. When these glaciers melt, they add entirely new water volume \nto the oceans, making them a primary contributor to sea level rise.\"\"\"\n\nexpected_behavior = \"\"\"A multi-step scientific explanation.\nMust maintain logical consistency across all steps.\"\"\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles complex logical chains</li> <li>Maintains consistency across steps</li> <li>Validates scientific reasoning</li> </ul>"},{"location":"guides/testing-patterns/#pattern-nonsensical-content","title":"Pattern: Nonsensical Content","text":"<p>Scenario: Testing handling of grammatically correct but meaningless content</p> <p>Implementation:</p> <p><pre><code>actual = \"The colorless green ideas sleep furiously\"\n\nexpected_behavior = \"A grammatically correct but semantically nonsensical statement\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Recognizes grammatical structure</li> <li>Identifies semantic nonsense</li> <li>Validates meta-understanding</li> </ul>"},{"location":"guides/testing-patterns/#pattern-extended-narrative","title":"Pattern: Extended Narrative","text":"<p>Scenario: Testing long-form narrative understanding</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"\n        The Roman Empire's rise began with modest origins in central Italy. What started as a small \n        settlement along the Tiber River would eventually become one of history's most influential \n        civilizations. In the early days, Rome was ruled by kings, but this system was overthrown \n        in 509 BCE, giving birth to the Roman Republic.\n\n        During the Republic, Rome expanded its territory through military conquest and diplomatic \n        alliances. The Roman army became increasingly professional, developing innovative tactics \n        and technologies. This military success brought wealth and power, but also internal \n        challenges. Social tensions grew between patricians and plebeians, leading to significant \n        political reforms.\n\n        By the 1st century BCE, the Republic faced severe internal strife. Military commanders \n        like Marius, Sulla, and eventually Julius Caesar accumulated unprecedented power. Caesar's \n        crossing of the Rubicon in 49 BCE marked a point of no return. His assassination in 44 BCE \n        led to another civil war, ultimately resulting in his adopted heir Octavian becoming \n        Augustus, the first Roman Emperor.\n\n        Augustus transformed Rome into an empire while maintaining a facade of republican \n        institutions. He implemented sweeping reforms in administration, military organization, \n        and public works. The Pax Romana that followed brought unprecedented peace and prosperity \n        across the Mediterranean world. Trade flourished, cities grew, and Roman culture spread \n        throughout the empire.\n        \"\"\"\n\nexpected_behavior = \"\"\"A historical narrative that:\n1. Maintains chronological progression\n2. Shows cause-and-effect relationships\n3. Develops consistent themes (power, governance, military)\n4. Connects multiple historical events coherently\n5. Demonstrates character development (e.g., Caesar to Augustus)\n\"\"\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles extended narratives</li> <li>Maintains thematic consistency</li> <li>Validates complex relationships</li> <li>Shows chronological understanding</li> </ul>"},{"location":"guides/testing-patterns/#pattern-emoji-quantity-testing","title":"Pattern: Emoji Quantity Testing","text":"<p>Scenario: Testing recognition of repeated emojis</p> <p>Implementation:</p> <pre><code>actual = \"\ud83e\udd16\" * 100\n\nexpected_behavior = \"A lot of emojis\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> <p>Result: \u2705 PASS</p> <ul> <li>Handles repeated Unicode characters</li> <li>Recognises quantity concepts</li> <li>Validates emoji processing</li> </ul>"},{"location":"guides/testing-patterns/#pattern-emoji-quantity-mismatch","title":"Pattern: Emoji Quantity Mismatch","text":"<p>Scenario: Testing quantity recognition accuracy</p> <p>Implementation:</p> <p><pre><code>actual = \"\ud83e\udd16\" * 100\n\nexpected_behavior = \"Only a few emojis\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails due to quantity mismatch</li> <li>Shows quantity awareness</li> <li>Validates numerical understanding</li> </ul>"},{"location":"guides/testing-patterns/#pattern-mixed-unicode-content-known-reliability-issue","title":"Pattern: Mixed Unicode Content \u26a0\ufe0f Known Reliability Issue","text":"<p>Scenario: Testing complex Unicode combinations and repetitive patterns</p>"},{"location":"guides/testing-patterns/#observed-behavior","title":"Observed Behavior","text":"<p>Test Case 1: Strict Pattern Matching</p> <pre><code>actual = \"\ud83e\udd16\ud83d\udc7e\" * 50 + \"\u3053\u3093\u306b\u3061\u306f\" * 20 + \"\ud83c\udf08\" * 30\n\nexpected_behavior = \"A mix of emojis and Japanese text\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> <p>Results:</p> <ul> <li>\u2705 Success Rate: 96% (48/50 runs)</li> <li>\u274c Failure Rate: 4% (2/50 runs)</li> <li>\ud83d\udd0d Failure Analysis:<ul> <li>Occurs primarily during increased API latency</li> <li>GPT-4o occasionally interprets sequential patterns as \"distinct collections\" rather than \"mixed content\"</li> <li>Failure message example: \"This is not a mix as there is a distinct collection of emojis followed by Japanese text and then a collection of rainbows\"</li> </ul> </li> </ul>"},{"location":"guides/testing-patterns/#recommended-implementation","title":"Recommended Implementation","text":"<p>Test Case 2: Pattern-Agnostic Matching</p> <pre><code>actual = \"\ud83e\udd16\ud83d\udc7e\" * 50 + \"\u3053\u3093\u306b\u3061\u306f\" * 20 + \"\ud83c\udf08\" * 30\n\nexpected = \"More than one type of emoji and Japanese text regardless of order\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> <p>Results:</p> <ul> <li>\u2705 Success Rate: 100% (preliminary)</li> <li>\u26a0\ufe0f Extended testing in progress</li> <li>\ud83d\udd0d Monitoring prompt effectiveness across different test scenarios</li> </ul>"},{"location":"guides/testing-patterns/#best-practices","title":"Best Practices","text":"<ol> <li>Use pattern-agnostic assertions for repetitive Unicode content</li> <li>Consider implementing retry logic for critical tests</li> <li>Monitor API response times during failures</li> <li>Use enhanced prompts for complex Unicode pattern testing</li> </ol>"},{"location":"guides/testing-patterns/#ongoing-investigation","title":"Ongoing Investigation","text":"<ul> <li>Testing various prompt configurations to improve reliability</li> <li>Monitoring performance impact of different prompt strategies</li> <li>Collecting data on failure patterns with different prompt versions</li> </ul>"},{"location":"guides/testing-patterns/#pattern-multilingual-emoji-spam","title":"Pattern: Multilingual Emoji Spam","text":"<p>Scenario: Testing repeated multilingual content</p> <p>Implementation:</p> <p><pre><code>actual = \"Hello\u4f60\u597dBonjour\ud83c\udf08\" * 50\n\nexpected_behavior = \"A repetitive greeting in multiple languages with rainbows\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles multilingual text</li> <li>Recognises repetitive patterns</li> <li>Validates mixed content types</li> </ul>"},{"location":"guides/testing-patterns/#pattern-ascii-art-recognition","title":"Pattern: ASCII Art Recognition","text":"<p>Scenario: Testing complex ASCII art patterns</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"\n(\u256f\u00b0\u25a1\u00b0)\u256f\ufe35 \u253b\u2501\u253b\n\"\"\" * 20\n\nexpected_behavior = \"Multiple instances of table-flipping ASCII art\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Recognises ASCII art patterns</li> <li>Understands visual representations</li> <li>Validates repeated patterns</li> </ul>"},{"location":"guides/testing-patterns/#pattern-extreme-whitespace","title":"Pattern: Extreme Whitespace","text":"<p>Scenario: Testing handling of excessive spacing</p> <p>Implementation:</p> <p><pre><code>actual = \"hello    \" + \" \" * 1000 + \"    world\" + \"\\n\" * 500\n\nexpected_behavior = \"A greeting with excessive spacing\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Handles extreme whitespace</li> <li>Maintains semantic meaning</li> <li>Validates text normalisation</li> </ul>"},{"location":"guides/testing-patterns/#pattern-number-pattern-recognition","title":"Pattern: Number Pattern Recognition","text":"<p>Scenario: Testing numerical pattern understanding</p> <p>Implementation:</p> <p><pre><code>actual = \"\".join([str(i % 10) for i in range(1000)])\n\nexpected_behavior = \"A long sequence of repeating numbers\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Recognises numerical patterns</li> <li>Handles long repetitive sequences</li> <li>Validates pattern understanding</li> </ul>"},{"location":"guides/testing-patterns/#pattern-patient-education-content-testing","title":"Pattern: Patient Education Content Testing","text":"<p>Scenario: Testing medical education content for diabetes management</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"\nUnderstanding and Managing Type 2 Diabetes\n\nType 2 diabetes is a chronic condition that affects how your body processes blood sugar (glucose). \nWhile this condition is serious, it can be effectively managed through lifestyle changes and, \nwhen necessary, medication. This guide will help you understand the key aspects of diabetes \nmanagement.\n\nBlood Sugar Monitoring:\nRegular blood sugar monitoring is essential. Your target blood glucose levels should typically \nbe 80-130 mg/dL before meals and less than 180 mg/dL two hours after meals. However, your \nhealthcare provider may set different targets based on your individual needs. Keep a log of \nyour readings to identify patterns and adjust your management strategy accordingly.\n\nDietary Considerations:\nA balanced diet is crucial for managing type 2 diabetes. Focus on:\n- Controlling portion sizes\n- Choosing high-fiber, low-glycemic foods\n- Limiting refined carbohydrates and processed sugars\n- Including lean proteins and healthy fats\n- Spacing meals evenly throughout the day\n\nPhysical Activity:\nRegular exercise helps control blood sugar levels by improving insulin sensitivity. Aim for:\n- At least 150 minutes of moderate-intensity aerobic activity weekly\n- Resistance training 2-3 times per week\n- Daily movement, even if just short walks\nAlways check your blood sugar before and after exercise, and carry a fast-acting \ncarbohydrate source.\n\nMedication Management:\nIf prescribed, take diabetes medications as directed. Common medications include:\n- Metformin (helps reduce glucose production)\n- Sulfonylureas (increase insulin production)\n- DPP-4 inhibitors (help maintain blood sugar control)\nNever adjust or stop medications without consulting your healthcare provider.\n\nWarning Signs:\nLearn to recognize and respond to:\n- Hypoglycemia (low blood sugar): shakiness, sweating, confusion\n- Hyperglycemia (high blood sugar): increased thirst, frequent urination, fatigue\nSeek immediate medical attention if you experience severe symptoms or sustained \nhigh blood sugar levels.\n\nRegular Health Monitoring:\nSchedule regular check-ups with your healthcare team, including:\n- HbA1c tests every 3-6 months\n- Annual eye examinations\n- Regular foot checks\n- Kidney function tests\n- Cholesterol level monitoring\n\nRemember, diabetes management is a journey, not a destination. Small, consistent \nsteps in the right direction can lead to significant improvements in your health \nand quality of life.\n\"\"\"\n\nexpected = \"\"\"A medical education document that must:\n1. Contain an overview section explaining the condition\n2. List specific numerical guidelines (blood sugar ranges, exercise minutes)\n3. Include structured sections for diet, exercise, and medication\n4. Provide clear warning signs AND detailed emergency response procedures\n5. End with follow-up care instructions\"\"\"\n\nbehavioral_assert.assert_behavioral_match(actual, expected_behavior)       \n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because emergency response procedures are missing</li> <li>Fails because follow-up care instructions are incomplete</li> <li>Shows precision in medical content requirements</li> <li>Validates structured information presentation</li> </ul>"},{"location":"guides/testing-patterns/#pattern-investment-portfolio-report-testing","title":"Pattern: Investment Portfolio Report Testing","text":"<p>Scenario: Testing professional investment portfolio report generation</p> <p>Implementation:</p> <pre><code>actual = \"\"\"\nQ4 2023 Portfolio Performance Summary\n\nPortfolio Overview:\nYour investment portfolio has demonstrated resilient performance during Q4 2023, \nachieving a total return of 8.2% against our benchmark index return of 7.5%. \nTotal portfolio value stands at $1,245,000 as of December 31, 2023.\n\nAsset Allocation Analysis:\nCurrent allocation stands at:\n- Equities: 65% ($809,250)\n  - US Large Cap: 40% ($498,000)\n  - International Developed: 15% ($186,750)\n  - Emerging Markets: 10% ($124,500)\n- Fixed Income: 25% ($311,250)\n  - Government Bonds: 15% ($186,750)\n  - Corporate Bonds: 10% ($124,500)\n- Alternative Investments: 10% ($124,500)\n  - Real Estate: 5% ($62,250)\n  - Commodities: 5% ($62,250)\n\nPerformance Attribution:\nKey contributors to performance:\n1. US Technology sector outperformance (+12.3%)\n2. Emerging Markets recovery (+9.1%)\n3. Corporate Bond yield optimization (+4.2%)\n\nRisk Metrics:\n- Portfolio Beta: 0.85\n- Sharpe Ratio: 1.45\n- Maximum Drawdown: -5.2%\n- Standard Deviation: 12.3%\n\nRebalancing Recommendations:\nBased on current market conditions and your investment objectives:\n1. Consider increasing Fixed Income allocation by 2%\n2. Reduce US Large Cap exposure by 3%\n3. Increase Emerging Markets exposure by 1%\n\nMarket Outlook:\nLooking ahead to 2024, we anticipate:\n- Continued monetary policy normalization\n- Potential emerging markets opportunities\n- Heightened focus on quality factors in equity selection\n\nNext Steps:\n1. Schedule quarterly review meeting\n2. Discuss rebalancing recommendations\n3. Update investment policy statement if needed\n\"\"\"\n\nexpected = \"\"\"A professional investment portfolio report that must:\n1. Present portfolio performance with specific metrics\n2. Detail current asset allocation with percentages\n3. Include risk analysis metrics\n4. Provide forward-looking recommendations\n5. Maintain formal financial terminology\n6. Include clear next steps or action items\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> <p>Result: \u2705 PASS</p> <ul> <li>Shows comprehensive financial reporting structure</li> <li>Validates precise numerical data presentation</li> <li>Confirms professional financial terminology</li> <li>Demonstrates clear action items and recommendations</li> <li>Verifies complete risk metric inclusion</li> </ul>"},{"location":"guides/testing-patterns/#pattern-personalized-content-recommendation-testing","title":"Pattern: Personalized Content Recommendation Testing","text":"<p>Scenario: Testing personalized content recommendation generation</p> <p>Implementation:</p> <pre><code>actual = \"\"\"\nPersonalized Content Recommendations - User Profile #A1234\nGenerated: November 22, 2024\n\nRecommended Content Queue:\n1. \"Climate Pioneers\" (Documentary Series)\n- Episode length: 45 minutes\n- New episodes available\n\n2. \"Global Power Play\" (Political Drama)\n- Episode length: 55 minutes\n- Features actors from previously watched content\n\n3. \"Earth's Tipping Points\" (Scientific Documentary)\n- Episode length: 40 minutes\n- Recently added to platform\n\nEngagement Optimization:\n- Scheduled new episode alerts\n- Downloadable content for offline viewing\n- Similar content suggestions refreshed weekly\n- Customized language preferences maintained\n\nContent Accessibility:\nAll recommended content includes your preferred subtitle options and is \navailable in HD quality. Downloads are enabled for offline viewing during \nyour upcoming travel dates.\n\"\"\"\n\nexpected = \"\"\"A personalized content recommendation document that must:\n1. Include the viewing patterns and preferences of the user\n2. List recommended content with clear reasoning\n3. Provide matching percentages or relevance metrics\n4. Include viewing optimization suggestions\n5. Address content accessibility features\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> <p>Result: \u274c FAIL</p> <ul> <li>Fails because relevance metrics (percentages) are missing</li> <li>Lacks explicit matching percentages or relevance scores</li> <li>Shows content recommendations without quantified relevance</li> <li>Demonstrates engagement optimization suggestions</li> <li>Validates content accessibility features</li> </ul>"},{"location":"guides/testing-patterns/#pattern-legal-document-summary-testing","title":"Pattern: Legal Document Summary Testing","text":"<p>Scenario: Testing legal document summary generation</p> <p>Implementation:</p> <pre><code>actual = \"\"\"\nContract Summary Analysis\nDocument Reference: MSA-2024-0892\nDate of Analysis: November 22, 2024\n\nAgreement Overview:\nSoftware Development Master Services Agreement between TechCorp Inc. (\"Provider\") \nand GlobalEnterprises LLC (\"Client\") for the development and maintenance of \nenterprise software solutions.\n\nKey Terms and Conditions:\n1. Service Scope\n- Custom software development services\n- System integration capabilities\n- Ongoing maintenance and support\n- Security compliance implementations\n\n2. Financial Terms\n- Base development fee: $750,000\n- Monthly maintenance: $15,000\n- Change request rate: $200/hour\n- Payment terms: Net 30\n\n3. Performance Standards\n- 99.9% system availability\n- 4-hour response time for critical issues\n- Monthly performance reporting\n- Quarterly service reviews\n\n4. Intellectual Property Rights\n- Client owns all custom development\n- Provider retains rights to pre-existing IP\n- Joint ownership of derivative works\n- Limited license for provider tools\n\n5. Term and Termination\n- Initial term: 36 months\n- Automatic renewal: 12-month periods\n- 90-day termination notice required\n- Immediate termination for material breach\n\nRisk Assessment:\n- Medium risk: Data protection obligations\n- Low risk: Service level commitments\n- Low risk: IP ownership structure\n- Medium risk: Change management process\n\nNext Steps:\n1. Legal team review of data protection terms\n2. Technical team validation of SLAs\n3. Finance approval of payment terms\n4. Compliance review of security standards\n\"\"\"\n\nexpected = \"\"\"A legal document summary that must:\n1. Identify key parties and document type\n2. List main contractual terms\n3. Include specific numerical values (costs, dates, metrics)\n4. Provide risk assessment\n5. Outline required actions or next steps\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> <p>Result: \u2705 PASS</p> <ul> <li>Shows comprehensive legal document structure</li> <li>Validates precise contractual terms</li> <li>Confirms specific numerical values inclusion</li> <li>Demonstrates clear risk assessment</li> <li>Verifies actionable next steps</li> </ul>"},{"location":"guides/testing-patterns/#pattern-maintenance-prediction-testing","title":"Pattern: Maintenance Prediction Testing","text":"<p>Scenario: Testing maintenance prediction report generation for CNC equipment</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"\nEquipment Maintenance Analysis\nMachine ID: CNC-1234\nAnalysis Date: November 22, 2024\n\nCurrent Status Summary:\nThe CNC machine is showing early indicators of potential bearing wear in the main spindle.\nRecommended action is to schedule maintenance within the next 2 weeks.\n\nOperational Parameters:\n- Current Runtime: 2,450 hours\n- Average Daily Usage: 18 hours\n- Last Maintenance: October 15, 2024\n\nImmediate Recommendations:\n1. Schedule bearing inspection\n2. Monitor vibration levels daily\n3. Prepare replacement parts\n4. Plan for 4-hour maintenance window\n\nImpact Assessment:\n- Production Impact: Minimal if addressed within 2 weeks\n- Resource Requirements: Standard maintenance team\n- Parts Cost Estimate: $2,500\n\"\"\"\n\nexpected = \"\"\"A maintenance prediction report that must:\n1. Include current machine status\n2. Provide historical maintenance patterns\n3. Show failure prediction confidence levels\n4. List specific maintenance recommendations\n5. Include impact assessment and timeline\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because historical maintenance patterns are incomplete</li> <li>Missing failure prediction confidence levels</li> <li>Shows current status and recommendations</li> <li>Includes basic impact assessment</li> <li>Demonstrates timeline considerations</li> </ul>"},{"location":"guides/testing-patterns/#pattern-e-commerce-product-description-testing","title":"Pattern: E-commerce Product Description Testing","text":"<p>Scenario: Testing product description for a smart home security camera</p> <p>Implementation:</p> <p><pre><code>actual = \"\"\"\nSmart Home Security Camera - Model HC2000\n\nTransform your home security with our latest AI-powered camera system. This next-generation \ndevice combines advanced motion detection with crystal-clear 4K video quality, perfect for \nboth indoor and outdoor monitoring.\n\nKey Features:\n- 4K Ultra HD resolution with HDR\n- 160\u00b0 wide-angle view\n- Advanced AI motion detection\n- Two-way audio communication\n- Night vision up to 30 feet\n- Weather-resistant (IP66 rated)\n\nSmart Integration:\nWorks seamlessly with major platforms including:\n- Amazon Alexa\n- Google Home\n- Apple HomeKit\n- IFTTT\n\nTechnical Specifications:\n- Dimensions: 3.2\" x 3.2\" x 5.1\"\n- Weight: 12.3 oz\n- Power: AC adapter or rechargeable battery\n- Storage: Cloud or local SD card (up to 256GB)\n- Connectivity: 2.4GHz/5GHz WiFi\n\nWhat's in the Box:\n- HC2000 Camera\n- Mounting bracket\n- Power adapter\n- Quick start guide\n- Screws and anchors\n\nPerfect for:\n- Home security\n- Baby monitoring\n- Pet watching\n- Front door monitoring\n\n30-day money-back guarantee\n2-year manufacturer warranty\nFree technical support\n\"\"\"\n\nexpected = \"\"\"An e-commerce product description that must:\n1. Include clear product name and model\n2. List key features and specifications\n3. Specify technical details and compatibility\n4. Describe package contents\n5. Include warranty and support information\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> Result: \u2705 PASS</p> <ul> <li>Shows comprehensive product information structure</li> <li>Validates technical specifications inclusion</li> <li>Confirms compatibility details</li> <li>Demonstrates complete package contents listing</li> <li>Verifies warranty and support information</li> </ul>"},{"location":"guides/testing-patterns/#pattern-assignment-feedback-testing","title":"Pattern: Assignment Feedback Testing","text":"<p>Scenario: Testing student assignment feedback generation</p> <p>Implementation:</p> <pre><code>actual = \"\"\"\nAssignment Feedback\nStudent ID: STU-2024-456\nAssignment: Research Paper on Climate Change\nSubmission Date: November 22, 2024\n\nOverall Assessment:\nYour research paper demonstrates good understanding of climate change basics.\nThe writing is clear and well-structured, with appropriate use of scientific\nterminology throughout the document.\n\nStrengths:\n- Strong introduction that sets context\n- Good use of current scientific data\n- Clear paragraph structure\n- Proper citation format\n\nAreas Noted:\n- Some statistical interpretations could be more precise\n- Additional peer-reviewed sources would strengthen arguments\n- Conclusion could be more comprehensive\n\nGrade: B+ (88/100)\n\nAdditional Comments:\nThe paper shows promise and indicates solid research skills. Your analysis\nof temperature data trends was particularly well-done. Consider expanding\nyour discussion of potential mitigation strategies in future work.\n\"\"\"\n\nexpected = \"\"\"An assignment feedback document that must:\n1. Include basic assignment and student information\n2. Provide specific strengths and weaknesses\n3. List concrete steps for improvement\n4. Reference specific learning objectives\n5. Include grading criteria and score\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> <p>Result: \u274c FAIL</p> <ul> <li>Fails because concrete improvement steps are missing</li> <li>Missing specific learning objectives</li> <li>Shows basic assignment information</li> <li>Includes strengths and weaknesses</li> <li>Demonstrates grading information</li> </ul>"},{"location":"guides/testing-patterns/#pattern-real-estate-listing-testing","title":"Pattern: Real Estate Listing Testing","text":"<p>Scenario: Testing real estate property listing generation</p> <p>Implementation:</p> <pre><code>actual = \"\"\"\nStunning Modern Oasis in Prime Location\n123 Maple Avenue, Riverside Heights\n\nDiscover urban elegance in this meticulously updated contemporary home, where \nmodern luxury meets practical living. This 2,400 sq ft residence seamlessly \nblends indoor and outdoor living spaces.\n\nProperty Highlights:\n- 4 bedrooms, 2.5 bathrooms\n- Built: 2018\n- Lot size: 0.25 acres\n- Two-car attached garage\n- Energy-efficient smart home features\n\nInterior Features:\nThe open-concept main level showcases:\n- Chef's kitchen with quartz countertops\n- Custom Italian cabinetry\n- Premium stainless steel appliances\n- Expansive living room with 12-foot ceilings\n- Primary suite with spa-inspired bathroom\n\nOutdoor Living:\n- Professional landscaping\n- Covered patio with built-in BBQ\n- Low-maintenance xeriscaping\n- Private backyard retreat\n\nLocation Benefits:\n- Walking distance to Central Park\n- Top-rated school district\n- 10 minutes to downtown\n- Easy access to major highways\n\nRecent Updates:\n- New HVAC system (2023)\n- Smart home integration\n- Updated LED lighting\n- Fresh interior paint\n\nPrice: $875,000\nAvailable for immediate viewing\nVirtual tour link: [URL]\n\"\"\"\n\nexpected = \"\"\"A real estate listing that must:\n1. Include property overview and key features\n2. List specific amenities and updates\n3. Describe location benefits\n4. Use engaging, descriptive language\n5. Provide essential details (size, bedrooms, price)\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> <p>Result: \u2705 PASS</p> <ul> <li>Shows comprehensive property information</li> <li>Validates specific amenities and features</li> <li>Confirms location benefits inclusion</li> <li>Demonstrates engaging descriptive language</li> <li>Verifies essential property details</li> </ul>"},{"location":"guides/testing-patterns/#pattern-interview-feedback-testing","title":"Pattern: Interview Feedback Testing","text":"<p>Scenario: Testing interview feedback generation for technical position</p> <p>Implementation:</p> <p><pre><code>def test_interview_feedback_missing_criteria(self, asserter):\n\"\"\"Test semantic matching for interview feedback generation. Should fail due to\nmissing evaluation criteria and specific examples.\"\"\"\nactual = \"\"\"\nInterview Feedback Summary\nCandidate ID: INT-2024-789\nPosition: Senior Software Engineer\nInterview Date: November 22, 2024\n\nOverall Impression:\nThe candidate demonstrated strong technical knowledge and communicated well\nthroughout the interview. They showed enthusiasm for the role and our company's\nmission.\n\nDiscussion Points:\n- Previous experience with cloud architecture\n- Team collaboration approaches\n- Problem-solving methodology\n- Career goals and aspirations\n\nTechnical Discussion:\nCandidate showed familiarity with:\n- Microservices architecture\n- CI/CD pipelines\n- Cloud platforms (AWS, Azure)\n- Agile development practices\n\nCultural Fit:\nAppears to align well with our company values and team dynamics.\nDemonstrated good communication skills and collaborative mindset.\n\nNext Steps:\nProceed with reference checks if moving forward.\nSchedule follow-up with hiring manager.\n\"\"\"\n\nexpected = \"\"\"An interview feedback document that must:\n1. Include candidate and position information\n2. List specific evaluation criteria with ratings\n3. Provide concrete examples of responses\n4. Include technical assessment scores\n5. Offer clear hiring recommendation\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> Result: \u274c FAIL</p> <ul> <li>Fails because evaluation criteria lack ratings</li> <li>Missing concrete response examples</li> <li>Missing technical assessment scores</li> <li>Shows basic candidate information</li> <li>Lacks clear hiring recommendation</li> </ul>"},{"location":"guides/testing-patterns/#pattern-customer-service-response-testing","title":"Pattern: Customer Service Response Testing","text":"<p>Scenario: Testing customer service ticket response generation</p> <p>Implementation:</p> <pre><code>actual = \"\"\"\nTicket Analysis and Response\nTicket ID: CS-2024-1122\nPriority: Medium\nCategory: Product Return\n\nCustomer Query Summary:\nCustomer purchased a wireless headphone (Model: WH-1000XM4) three days ago\nand is experiencing connectivity issues with their iPhone 13. Initial\ntroubleshooting steps were attempted without success.\n\nIssue Analysis:\n- Product is within return window (3 of 30 days)\n- Common compatibility issue identified\n- Troubleshooting already attempted\n- Customer tone indicates frustration\n\nRecommended Response:\nDear [Customer Name],\n\nThank you for reaching out about the connectivity issues with your WH-1000XM4\nheadphones. I understand how frustrating technical issues can be, especially\nwith a new purchase.\n\nBased on your description, I can offer you two immediate solutions:\n\n1. Advanced Troubleshooting:\n- Reset the headphones (detailed steps attached)\n- Update iPhone Bluetooth settings\n- Install latest firmware\n\n2. Hassle-free Return:\n- Generate return label through our portal\n- Full refund processed within 3 business days\n- Free return shipping\n\nWould you prefer to try the advanced troubleshooting steps, or would you like\nto proceed with the return? I'm here to help with either option.\n\nNext Steps:\n- Await customer preference\n- Prepare return label if requested\n- Schedule follow-up within 24 hours\n\nResponse Tone: Empathetic and Solution-focused\nSupport Resources: KB-2345, RT-6789\n\"\"\"\n\nexpected = \"\"\"A customer service response that must:\n1. Include ticket categorization and priority\n2. Summarize the customer's issue accurately\n3. Provide multiple solution options\n4. Include clear next steps\n5. Maintain appropriate tone and empathy\"\"\"\n\nasserter.assert_behavioral_match(actual, expected)\n</code></pre> <p>Result: \u2705 PASS</p> <ul> <li>Shows comprehensive ticket information structure</li> <li>Validates accurate issue summary</li> <li>Confirms multiple solution options</li> <li>Demonstrates clear next steps</li> <li>Verifies empathetic and professional tone</li> </ul>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/","title":"Behavior At Semantic Boundaries","text":"<p>\u26a0\ufe0f Important Notice About Semantic Testing</p> <p>This documentation refers to tests originally named semantic assertion. We have since deprecated the name <code>semantic</code> in favor of behavioral testing, as we found that it more accurately describes what we were doing. The underlying implementation and reliability testing remain valid, as the core functionality is identical - we've simply improved the conceptual framework to better reflect what the library is actually doing.</p>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/#overview","title":"Overview","text":"<p>During our real-world style testing, we encountered an interesting boundary case that highlighted both the sophistication and limitations of LLM-based behavioral testing. </p> <p>This case was discovered during the initial 500 runs of our real-world test suite, where only this one test demonstrated non-determinism. The non-determinism occurred at what we identified as a semantic boundary - a point where the interpretation of expected behavior could legitimately differ.</p> <p>This case provides valuable insights into how LLMs interpret behavioral requirements and the importance of precise specification in behavioral testing.</p>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/#the-case","title":"The Case","text":"<p>This is our healthcare test case:</p> <pre><code>def test_patient_education_diabetes_management(self, asserter):\n        \"\"\"Test semantic matching for patient education content about diabetes management. Failure is expected because\n        this does not contain emergency response steps.\"\"\"\n        actual = \"\"\"\n        Understanding and Managing Type 2 Diabetes\n\n        Type 2 diabetes is a chronic condition that affects how your body processes blood sugar (glucose). \n        While this condition is serious, it can be effectively managed through lifestyle changes and, \n        when necessary, medication. This guide will help you understand the key aspects of diabetes \n        management.\n\n        Blood Sugar Monitoring:\n        Regular blood sugar monitoring is essential. Your target blood glucose levels should typically \n        be 80-130 mg/dL before meals and less than 180 mg/dL two hours after meals. However, your \n        healthcare provider may set different targets based on your individual needs. Keep a log of \n        your readings to identify patterns and adjust your management strategy accordingly.\n\n        Dietary Considerations:\n        A balanced diet is crucial for managing type 2 diabetes. Focus on:\n        - Controlling portion sizes\n        - Choosing high-fiber, low-glycemic foods\n        - Limiting refined carbohydrates and processed sugars\n        - Including lean proteins and healthy fats\n        - Spacing meals evenly throughout the day\n\n        Physical Activity:\n        Regular exercise helps control blood sugar levels by improving insulin sensitivity. Aim for:\n        - At least 150 minutes of moderate-intensity aerobic activity weekly\n        - Resistance training 2-3 times per week\n        - Daily movement, even if just short walks\n        Always check your blood sugar before and after exercise, and carry a fast-acting \n        carbohydrate source.\n\n        Medication Management:\n        If prescribed, take diabetes medications as directed. Common medications include:\n        - Metformin (helps reduce glucose production)\n        - Sulfonylureas (increase insulin production)\n        - DPP-4 inhibitors (help maintain blood sugar control)\n        Never adjust or stop medications without consulting your healthcare provider.\n\n        Warning Signs:\n        Learn to recognize and respond to:\n        - Hypoglycemia (low blood sugar): shakiness, sweating, confusion\n        - Hyperglycemia (high blood sugar): increased thirst, frequent urination, fatigue\n        Seek immediate medical attention if you experience severe symptoms or sustained \n        high blood sugar levels.\n\n        Regular Health Monitoring:\n        Schedule regular check-ups with your healthcare team, including:\n        - HbA1c tests every 3-6 months\n        - Annual eye examinations\n        - Regular foot checks\n        - Kidney function tests\n        - Cholesterol level monitoring\n\n        Remember, diabetes management is a journey, not a destination. Small, consistent \n        steps in the right direction can lead to significant improvements in your health \n        and quality of life.\n        \"\"\"\n\n        expected = \"\"\"A medical education document that must:\n        1. Contain an overview section explaining the condition\n        2. List specific numerical guidelines (blood sugar ranges, exercise minutes)\n        3. Include structured sections for diet, exercise, and medication\n        4. Provide clear warning signs AND detailed emergency response procedures\n        5. End with follow-up care instructions\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n</code></pre> <p>The relevant portions are here as follows:</p> <p>In <code>actual</code> (i.e. the simulated LLM output): <pre><code>Warning Signs:\nLearn to recognize and respond to:\n- Hypoglycemia (low blood sugar): shakiness, sweating, confusion\n- Hyperglycemia (high blood sugar): increased thirst, frequent urination, fatigue\nSeek immediate medical attention if you experience severe symptoms or sustained \nhigh blood sugar levels.\n</code></pre> In <code>expected_behavior</code> (i.e. the specification we expect developers to write when using our library):</p> <p>Version that led to non-determinism: <pre><code>4. Provide clear warning signs and emergency response steps\n</code></pre></p> <p>Updated version that led to determinism: <pre><code>4. Provide clear warning signs AND detailed emergency response procedures\n</code></pre></p> <p>\u26a0\ufe0f New update (30 November 2024):</p> <p>Claude is more conversational and does not consistently fail this test case even when asked for detailed emergency response procedures (no volume testing done but significant non-determinism observed in &lt;100 runs).</p> <p>Claude actually needs to be told to look for multi-step emergency response procedures like so:</p> <pre><code>4. Provide clear warning signs AND detailed multi-step emergency response procedures\n</code></pre> <p>Therefore, we strongly recommend using GPT-4o for your behavioral testing needs as GPT-4o's strictness is what is required.</p>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/#test-configuration","title":"Test Configuration","text":"<p>All tests used library defaults:</p> <p><pre><code>LLM_PROVIDER=openai\nLLM_MODEL=gpt-4o \nLLM_TEMPERATURE=0.0 \nLLM_MAX_TOKENS=4096 \nLLM_MAX_RETRIES=2 \nLLM_TIMEOUT=10.0 # Added for OpenAI in 0.1.0b5 using the underlying Langchain implementation in dev branch \n</code></pre> The <code>semantic_assert_match</code> function (Update: Deprecated and replaced with identical <code>assert_behavioral_match</code>) also saw slight modification:</p> <pre><code>        if result.startswith(\"FAIL\"):\n            raise SemanticAssertionError(\n                \"Semantic assertion failed\",\n                reason=result.split(\"FAIL: \")[1]\n            )\n\n        # Section below added to cause failure in the event of format violation    \n\n        elif result.startswith(\"PASS\"):\n            pass\n        else:\n            raise RuntimeError(\n                f\"Format Non-compliance Detected {result}\"\n            )\n</code></pre> <p>The prompts to the asserter LLM (that sits behind <code>semantic_assert_match</code>(Update: Deprecated and replaced with identical <code>assert_behavioral_match</code>)) were:</p> <pre><code>DEFAULT_SYSTEM_PROMPT = \"\"\"You are a testing system. Your job is to determine if an actual output matches the expected behavior.\n\nImportant: You can only respond with EXACTLY: \n1. 'PASS' if it matches, or \n2. 'FAIL: &lt;reason&gt;' if it doesn't match.\n\nAny other type of response will mean disaster which as a testing system, you are meant to prevent.\n\nBe strict but consider semantic meaning rather than exact wording.\"\"\"\n\nDEFAULT_HUMAN_PROMPT = \"\"\"\nExpected Behavior: {expected_behavior}\n\nActual Output: {actual}\n\nDoes the actual output match the expected behavior? Remember, you will fail your task unless you respond EXACTLY \nwith 'PASS' or 'FAIL: &lt;reason&gt;'.\"\"\"\n</code></pre>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/#testing-results","title":"Testing Results","text":"<p>This particular test was run 600 times.</p> <p>It was run 500 times in a wider test of the entire suite it is a part of and 100 times on its own with the following results:</p> <ul> <li>PASS: 586</li> <li>FAIL: 14</li> </ul> <p>The 100 individual tests were run when the non-determinism was noticed.</p> <p>API response times were noticeable faster when it started failing. </p> <p>Test suite execution times for 100 iterations (so 1,000 individual tests):</p> <ul> <li>~1550s - 1750s when it appeared to deterministically pass the test</li> <li>~1350s when it began to exhibit non-determinism</li> </ul> <p>We are unable to determine what longer API response times mean, and unless if OpenAI wades in, it is unlikely that we will find out.</p> <p>Logs can be found in the reliability_testing_real_world directory of the 0.1.0b5 branch of the github repo.</p>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/#analysis","title":"Analysis","text":"<p>Upon careful analysis, we identified a subtle but critical semantic boundary that affected the behavioral testing:</p> <ol> <li> <p>The Semantic Boundary:</p> <ul> <li>Original text: \"emergency response steps\" (plural)</li> <li>Test content provided: \"seek medical attention\" (singular)</li> <li>Question: Does a single, obvious response step satisfy a requirement for \"steps\"?</li> </ul> </li> </ol> <p>This semantic boundary - the interpretation of whether a single step satisfies a plural requirement - caused non-deterministic behavior in our behavioral tests. This demonstrates how semantic ambiguities can directly impact the reliability of behavioral testing.</p> <ol> <li> <p>The Test Content:</p> <ul> <li>Clearly provided warning signs</li> <li>Had one emergency response (\"seek medical attention\")</li> <li>Lacked detailed, multiple-step emergency procedures</li> </ul> </li> <li> <p>Interpretation Analysis:</p> </li> </ol> <p>A. Argument for Single-Step Sufficiency:</p> <pre><code>- The content is patient-focused, not for medical professionals\n- \"Seek medical attention\" is a complete, actionable instruction\n- In an emergency, simplicity and clarity are paramount\n- Additional steps might confuse or delay the critical action\n</code></pre> <p>B. Argument for Multiple-Step Requirement:</p> <pre><code>- The plural \"steps\" grammatically implies multiple procedures\n- Medical context demands comprehensive guidance\n- \"Seek medical attention\" is too obvious to constitute meaningful instruction\n- Patients need interim steps while medical help is en route\n</code></pre> <p>C. Resolution:</p> <pre><code>- While both interpretations have merit, the requirement for multiple steps is substantially stronger because:\n\n    1. The plural form explicitly requests multiple procedures\n    2. Medical documentation should err on the side of completeness\n    3. Interim guidance can be critical during emergency response\n    4. Self-evident instructions add no value to emergency protocols\n</code></pre> <ol> <li>Key Insight:</li> </ol> <p>The non-deterministic behavior of the LLM in this case reveals a genuine semantic boundary in medical documentation - the balance between simplicity and comprehensiveness in emergency instructions.</p>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/#key-learnings","title":"Key Learnings","text":"<ol> <li> <p>LLM Sophistication:</p> <ul> <li>The LLM detected the semantic ambiguity</li> <li>Demonstrated surprisingly strong interpretation capabilities</li> <li>Highlighted the importance of precise requirements</li> </ul> </li> <li> <p>Testing Implications:</p> <ul> <li>Requirements must be unambiguous</li> <li>Precision in language can improve test reliability (seriously, this is natural language, you can throw writing the <code>expected_behavior</code> to the non-technical PM)</li> <li>Literal interpretations should ALWAYS be preferred. Try to think like a lawyer - it's how I picked up why the test case was being incorrectly accepted (note that I am testing a negative test, so FAIL means that it was incorrectly passed by <code>assert_semantic_match</code>)</li> </ul> </li> </ol>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/#impact-on-library-usage","title":"Impact on Library Usage","text":"<p>When writing behavioral test cases:</p> <ol> <li>Be explicit about conjunctive requirements</li> <li>Use \"AND\" when both elements are required</li> <li>Consider potential semantic ambiguities</li> <li>Test requirements for potential boundary cases</li> </ol>"},{"location":"reliability_testing/behavior_at_semantic_boundaries/#quick-links","title":"Quick Links","text":"<ul> <li>Behavioral Reliability Testing</li> <li>Format Compliance</li> <li>Quick Start</li> </ul>"},{"location":"reliability_testing/behavioral_testing_reliability/","title":"Behavioral Testing Reliability - Real-World Industry Specific Testing","text":"<p>\u26a0\ufe0f Important Notice About Semantic Testing</p> <p>This documentation refers to tests originally named semantic assertion. We have since deprecated the name <code>semantic</code> in favor of behavioral testing, as we found that it more accurately describes what we were doing. The underlying implementation and reliability testing remain valid, as the core functionality is identical - we've simply improved the conceptual framework to better reflect what the library is actually doing.</p>"},{"location":"reliability_testing/behavioral_testing_reliability/#overview","title":"Overview","text":"<p>This page documents our extensive testing of llm-app-test against real-world industry use cases. We designed this test suite to validate the library's reliability with the kind of content that LLM applications would probably generate in production environments.</p> <p>The first test case in this suite initially exhibited behavioral non-determinism. Upon investigation, this was caused by a legitimate semantic boundary in the test case - a situation where the expected and actual outputs sit at the edge of what could be considered equivalent behaviors. As the library's author (a lawyer by training with three years of litigation experience), I had to carefully analyze the semantic boundaries to identify the exact conditions causing the non-determinism. This analysis and its implications are documented in detail here.</p> <p>The remaining 9 test cases maintained 100% pass rate across 1,700 runs. For simplicity and clarity in this documentation, we focus on the 1,200 runs where all 10 cases in the suite achieved a 100% pass rate.</p> <p>The relevant logs for the testing covered by this page can be found here.</p>"},{"location":"reliability_testing/behavioral_testing_reliability/#test-suite-design","title":"Test Suite Design","text":"<p>The test suite covers 10 key industries where LLMs are likely already seeing active use:</p> <ol> <li>Healthcare (Patient Education)</li> <li>Financial Services (Portfolio Reporting)</li> <li>Media/Entertainment (Content Recommendations)</li> <li>Legal (Document Summarization)</li> <li>Manufacturing (Maintenance Prediction)</li> <li>E-commerce (Product Descriptions)</li> <li>Education (Assignment Feedback)</li> <li>Real Estate (Property Listings)</li> <li>Human Resources (Interview Feedback)</li> <li>Customer Service (Ticket Response)</li> </ol> <p>The Test Suite consisted of 5 positive cases and 5 negative cases.</p>"},{"location":"reliability_testing/behavioral_testing_reliability/#testing-scale","title":"Testing Scale","text":"<ul> <li>Total Runs: 1,200 (600 Windows, 600 Linux (Pop_OS))</li> <li>Tests per Run: 10</li> <li>Total Test Executions: 12,000</li> <li>Pass Rate: 100%</li> </ul> <p>Cross-references:</p> <ul> <li>See Test Configuration for setup details</li> <li>See Test Results for detailed analysis</li> <li>Full test logs available in reliability_testing</li> </ul>"},{"location":"reliability_testing/behavioral_testing_reliability/#test-characteristics","title":"Test Characteristics","text":"<p>Each test was designed to reflect:</p> <ul> <li>Realistic content length</li> <li>Industry-specific terminology</li> <li>Common formatting patterns</li> <li>Typical validation requirements</li> <li>Real-world edge cases</li> <li>Claude 3.5 Sonnet was used to generate the test cases to make it more realistic, but testing was done with GPT-4o</li> </ul>"},{"location":"reliability_testing/behavioral_testing_reliability/#test-results","title":"Test Results","text":""},{"location":"reliability_testing/behavioral_testing_reliability/#format-compliance","title":"Format Compliance","text":"<ul> <li>Zero format violations across 12,000 executions</li> <li>Consistent PASS/FAIL behaviour</li> </ul>"},{"location":"reliability_testing/behavioral_testing_reliability/#cross-platform-reliability","title":"Cross-Platform Reliability","text":"<ul> <li>Identical behavior on Windows and Linux</li> <li>No platform-specific issues detected</li> </ul>"},{"location":"reliability_testing/behavioral_testing_reliability/#content-processing","title":"Content Processing","text":"<ul> <li>Successfully handled varying content lengths</li> <li>Maintained accuracy across different domains</li> <li>Consistent behaviour with specialised terminology</li> </ul>"},{"location":"reliability_testing/behavioral_testing_reliability/#cost-analysis","title":"Cost Analysis","text":"<p>Running the test suite demonstrated the following costs:</p> <ul> <li>Single Run (10 tests): US$0.014</li> <li>100 Runs (1,000 tests): US$1.40</li> <li>Repeated runs for reliability testing (12,000 tests): US$16.80</li> </ul> <p>Cost Breakdown:</p> <ul> <li>Per Test Cost: ~US$0.0014</li> <li>Per Run (10 tests): US$0.014</li> <li>Per 100 Runs: US$1.40</li> </ul> <p>This demonstrates that comprehensive behavioral testing remains economically viable even at scale. The cost per test is minimal considering the confidence gained in library reliability.</p> <p>Key Cost Insights:</p> <ul> <li>Linear cost scaling with test volume</li> <li>Predictable pricing for planning purposes</li> <li>Reasonable expense for production validation</li> </ul>"},{"location":"reliability_testing/behavioral_testing_reliability/#test-configuration","title":"Test Configuration","text":"<p>All tests used library defaults:</p> <p><pre><code>LLM_PROVIDER=openai\nLLM_MODEL=gpt-4o \nLLM_TEMPERATURE=0.0 \nLLM_MAX_TOKENS=4096 \nLLM_MAX_RETRIES=2 \nLLM_TIMEOUT=10.0 # Added for OpenAI in 0.1.0b5 using the underlying Langchain implementation in dev branch\n</code></pre> The <code>semantic_assert_match</code> function (Update: Deprecated and replaced with identical <code>assert_behavioral_match</code>) also saw slight modification:</p> <pre><code>        if result.startswith(\"FAIL\"):\n            raise SemanticAssertionError(\n                \"Semantic assertion failed\",\n                reason=result.split(\"FAIL: \")[1]\n            )\n\n        # Section below added to cause failure in the event of format violation    \n\n        elif result.startswith(\"PASS\"):\n            pass\n        else:\n            raise RuntimeError(\n                f\"Format Non-compliance Detected {result}\"\n            )\n</code></pre> <p>The prompts to the asserter LLM (that sits behind <code>semantic_assert_match</code>(Update: Deprecated and replaced with identical <code>assert_behavioral_match</code>)) were:</p> <pre><code>DEFAULT_SYSTEM_PROMPT = \"\"\"You are a testing system. Your job is to determine if an actual output matches the expected behavior.\n\nImportant: You can only respond with EXACTLY: \n1. 'PASS' if it matches, or \n2. 'FAIL: &lt;reason&gt;' if it doesn't match.\n\nAny other type of response will mean disaster which as a testing system, you are meant to prevent.\n\nBe strict but consider semantic meaning rather than exact wording.\"\"\"\n\nDEFAULT_HUMAN_PROMPT = \"\"\"\nExpected Behavior: {expected_behavior}\n\nActual Output: {actual}\n\nDoes the actual output match the expected behavior? Remember, you will fail your task unless you respond EXACTLY \nwith 'PASS' or 'FAIL: &lt;reason&gt;'.\"\"\"\n</code></pre>"},{"location":"reliability_testing/behavioral_testing_reliability/#test-suite-code","title":"Test Suite Code","text":"<p>\u26a0\ufe0f Note About Test Code:  The test suite shown uses the deprecated <code>SemanticAssertion</code> class and <code>assert_semantic_match</code> method.  These tests remain valid as the underlying implementation is identical in the new <code>BehavioralAssertion</code> class  and <code>assert_behavioral_match</code> method. The only change is in terminology to better reflect the testing approach.</p> <pre><code>import pytest\nfrom llm_app_test.semantic_assert.semantic_assert import SemanticAssertion\nfrom llm_app_test.exceptions.test_exceptions import (\n    SemanticAssertionError\n)\n\n\nclass TestRealWorldSemanticAssertion:\n    \"\"\"Test suite for semantic matching across diverse industry-specific LLM applications.\n\n    This test class is specifically designed to validate semantic matching capabilities\n    across a wide range of real-world LLM application outputs. It contains both positive\n    and negative test cases that represent actual use cases where LLMs are being used\n    in production environments.\n\n    Industries Covered:\n        - Healthcare (Patient Education)\n        - Financial Services (Portfolio Reporting)\n        - Media/Entertainment (Content Recommendations)\n        - Legal (Document Summarization)\n        - Manufacturing (Maintenance Prediction)\n        - E-commerce (Product Descriptions)\n        - Education (Assignment Feedback)\n        - Real Estate (Property Listings)\n        - Human Resources (Interview Feedback)\n        - Customer Service (Ticket Response)\n\n    Test Structure:\n        - Each test validates specific industry requirements\n        - Mix of positive and negative test cases\n        - Focus on realistic content length and complexity\n        - Industry-specific terminology and formatting\n        - Comprehensive coverage of common LLM outputs\n\n    Purpose:\n        This test suite is designed for brute force reliability testing of the semantic\n        matcher. It ensures the library can handle diverse, real-world content while\n        maintaining consistent behavior across multiple test runs.\n\n    Usage:\n        These tests are intended to be run multiple times (1000+) to validate the\n        consistency and reliability of the semantic matching functionality across\n        different contexts and content types.\n    \"\"\"\n    @pytest.fixture\n    def asserter(self):\n        return SemanticAssertion()\n\n\n    def test_patient_education_diabetes_management(self, asserter):\n        \"\"\"Test semantic matching for patient education content about diabetes management. Failure is expected because\n        this does not contain emergency response steps.\"\"\"\n        actual = \"\"\"\n        Understanding and Managing Type 2 Diabetes\n\n        Type 2 diabetes is a chronic condition that affects how your body processes blood sugar (glucose). \n        While this condition is serious, it can be effectively managed through lifestyle changes and, \n        when necessary, medication. This guide will help you understand the key aspects of diabetes \n        management.\n\n        Blood Sugar Monitoring:\n        Regular blood sugar monitoring is essential. Your target blood glucose levels should typically \n        be 80-130 mg/dL before meals and less than 180 mg/dL two hours after meals. However, your \n        healthcare provider may set different targets based on your individual needs. Keep a log of \n        your readings to identify patterns and adjust your management strategy accordingly.\n\n        Dietary Considerations:\n        A balanced diet is crucial for managing type 2 diabetes. Focus on:\n        - Controlling portion sizes\n        - Choosing high-fiber, low-glycemic foods\n        - Limiting refined carbohydrates and processed sugars\n        - Including lean proteins and healthy fats\n        - Spacing meals evenly throughout the day\n\n        Physical Activity:\n        Regular exercise helps control blood sugar levels by improving insulin sensitivity. Aim for:\n        - At least 150 minutes of moderate-intensity aerobic activity weekly\n        - Resistance training 2-3 times per week\n        - Daily movement, even if just short walks\n        Always check your blood sugar before and after exercise, and carry a fast-acting \n        carbohydrate source.\n\n        Medication Management:\n        If prescribed, take diabetes medications as directed. Common medications include:\n        - Metformin (helps reduce glucose production)\n        - Sulfonylureas (increase insulin production)\n        - DPP-4 inhibitors (help maintain blood sugar control)\n        Never adjust or stop medications without consulting your healthcare provider.\n\n        Warning Signs:\n        Learn to recognize and respond to:\n        - Hypoglycemia (low blood sugar): shakiness, sweating, confusion\n        - Hyperglycemia (high blood sugar): increased thirst, frequent urination, fatigue\n        Seek immediate medical attention if you experience severe symptoms or sustained \n        high blood sugar levels.\n\n        Regular Health Monitoring:\n        Schedule regular check-ups with your healthcare team, including:\n        - HbA1c tests every 3-6 months\n        - Annual eye examinations\n        - Regular foot checks\n        - Kidney function tests\n        - Cholesterol level monitoring\n\n        Remember, diabetes management is a journey, not a destination. Small, consistent \n        steps in the right direction can lead to significant improvements in your health \n        and quality of life.\n        \"\"\"\n\n        expected = \"\"\"A medical education document that must:\n        1. Contain an overview section explaining the condition\n        2. List specific numerical guidelines (blood sugar ranges, exercise minutes)\n        3. Include structured sections for diet, exercise, and medication\n        4. Provide clear warning signs AND detailed emergency response procedures\n        5. End with follow-up care instructions\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_investment_portfolio_report_generation(self, asserter):\n        \"\"\"Test semantic matching for investment portfolio report generation. Tests that the report\n        contains all required sections and maintains professional financial terminology.\"\"\"\n        actual = \"\"\"\n        Q4 2023 Portfolio Performance Summary\n\n        Portfolio Overview:\n        Your investment portfolio has demonstrated resilient performance during Q4 2023, \n        achieving a total return of 8.2% against our benchmark index return of 7.5%. \n        Total portfolio value stands at $1,245,000 as of December 31, 2023.\n\n        Asset Allocation Analysis:\n        Current allocation stands at:\n        - Equities: 65% ($809,250)\n            - US Large Cap: 40% ($498,000)\n            - International Developed: 15% ($186,750)\n            - Emerging Markets: 10% ($124,500)\n        - Fixed Income: 25% ($311,250)\n            - Government Bonds: 15% ($186,750)\n            - Corporate Bonds: 10% ($124,500)\n        - Alternative Investments: 10% ($124,500)\n            - Real Estate: 5% ($62,250)\n            - Commodities: 5% ($62,250)\n\n        Performance Attribution:\n        Key contributors to performance:\n        1. US Technology sector outperformance (+12.3%)\n        2. Emerging Markets recovery (+9.1%)\n        3. Corporate Bond yield optimization (+4.2%)\n\n        Risk Metrics:\n        - Portfolio Beta: 0.85\n        - Sharpe Ratio: 1.45\n        - Maximum Drawdown: -5.2%\n        - Standard Deviation: 12.3%\n\n        Rebalancing Recommendations:\n        Based on current market conditions and your investment objectives:\n        1. Consider increasing Fixed Income allocation by 2%\n        2. Reduce US Large Cap exposure by 3%\n        3. Increase Emerging Markets exposure by 1%\n\n        Market Outlook:\n        Looking ahead to 2024, we anticipate:\n        - Continued monetary policy normalization\n        - Potential emerging markets opportunities\n        - Heightened focus on quality factors in equity selection\n\n        Next Steps:\n        1. Schedule quarterly review meeting\n        2. Discuss rebalancing recommendations\n        3. Update investment policy statement if needed\n        \"\"\"\n\n        expected = \"\"\"A professional investment portfolio report that must:\n        1. Present portfolio performance with specific metrics\n        2. Detail current asset allocation with percentages\n        3. Include risk analysis metrics\n        4. Provide forward-looking recommendations\n        5. Maintain formal financial terminology\n        6. Include clear next steps or action items\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_content_recommendation_missing_viewing_patterns(self, asserter):\n        \"\"\"Test semantic matching for content recommendations. Should fail due to missing viewing patterns\n        and user preferences section.\"\"\"\n        actual = \"\"\"\n        Personalized Content Recommendations - User Profile #A1234\n        Generated: November 22, 2024\n\n        Recommended Content Queue:\n        1. \"Climate Pioneers\" (Documentary Series)\n            - Episode length: 45 minutes\n            - New episodes available\n\n        2. \"Global Power Play\" (Political Drama)\n            - Episode length: 55 minutes\n            - Features actors from previously watched content\n\n        3. \"Earth's Tipping Points\" (Scientific Documentary)\n            - Episode length: 40 minutes\n            - Recently added to platform\n\n        Engagement Optimization:\n        - Scheduled new episode alerts\n        - Downloadable content for offline viewing\n        - Similar content suggestions refreshed weekly\n        - Customized language preferences maintained\n\n        Content Accessibility:\n        All recommended content includes your preferred subtitle options and is \n        available in HD quality. Downloads are enabled for offline viewing during \n        your upcoming travel dates.\n        \"\"\"\n\n        expected = \"\"\"A personalized content recommendation document that must:\n        1. Include the viewing patterns and preferences of the user\n        2. List recommended content with clear reasoning\n        3. Provide matching percentages or relevance metrics\n        4. Include viewing optimization suggestions\n        5. Address content accessibility features\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_legal_document_summary_generation(self, asserter):\n        \"\"\"Test semantic matching for legal document summary generation. Tests that the summary\n        maintains accuracy while being accessible to non-legal readers.\"\"\"\n        actual = \"\"\"\n        Contract Summary Analysis\n        Document Reference: MSA-2024-0892\n        Date of Analysis: November 22, 2024\n\n        Agreement Overview:\n        Software Development Master Services Agreement between TechCorp Inc. (\"Provider\") \n        and GlobalEnterprises LLC (\"Client\") for the development and maintenance of \n        enterprise software solutions.\n\n        Key Terms and Conditions:\n        1. Service Scope\n            - Custom software development services\n            - System integration capabilities\n            - Ongoing maintenance and support\n            - Security compliance implementations\n\n        2. Financial Terms\n            - Base development fee: $750,000\n            - Monthly maintenance: $15,000\n            - Change request rate: $200/hour\n            - Payment terms: Net 30\n\n        3. Performance Standards\n            - 99.9% system availability\n            - 4-hour response time for critical issues\n            - Monthly performance reporting\n            - Quarterly service reviews\n\n        4. Intellectual Property Rights\n            - Client owns all custom development\n            - Provider retains rights to pre-existing IP\n            - Joint ownership of derivative works\n            - Limited license for provider tools\n\n        5. Term and Termination\n            - Initial term: 36 months\n            - Automatic renewal: 12-month periods\n            - 90-day termination notice required\n            - Immediate termination for material breach\n\n        Risk Assessment:\n        - Medium risk: Data protection obligations\n        - Low risk: Service level commitments\n        - Low risk: IP ownership structure\n        - Medium risk: Change management process\n\n        Next Steps:\n        1. Legal team review of data protection terms\n        2. Technical team validation of SLAs\n        3. Finance approval of payment terms\n        4. Compliance review of security standards\n        \"\"\"\n\n        expected = \"\"\"A legal document summary that must:\n        1. Identify key parties and document type\n        2. List main contractual terms\n        3. Include specific numerical values (costs, dates, metrics)\n        4. Provide risk assessment\n        5. Outline required actions or next steps\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_maintenance_prediction_missing_historical_context(self, asserter):\n        \"\"\"Test semantic matching for maintenance prediction report. Should fail due to\n        missing historical maintenance context and pattern analysis.\"\"\"\n        actual = \"\"\"\n        Equipment Maintenance Analysis\n        Machine ID: CNC-1234\n        Analysis Date: November 22, 2024\n\n        Current Status Summary:\n        The CNC machine is showing early indicators of potential bearing wear in the main spindle.\n        Recommended action is to schedule maintenance within the next 2 weeks.\n\n        Operational Parameters:\n        - Current Runtime: 2,450 hours\n        - Average Daily Usage: 18 hours\n        - Last Maintenance: October 15, 2024\n\n        Immediate Recommendations:\n        1. Schedule bearing inspection\n        2. Monitor vibration levels daily\n        3. Prepare replacement parts\n        4. Plan for 4-hour maintenance window\n\n        Impact Assessment:\n        - Production Impact: Minimal if addressed within 2 weeks\n        - Resource Requirements: Standard maintenance team\n        - Parts Cost Estimate: $2,500\n        \"\"\"\n\n        expected = \"\"\"A maintenance prediction report that must:\n        1. Include current machine status\n        2. Provide historical maintenance patterns\n        3. Show failure prediction confidence levels\n        4. List specific maintenance recommendations\n        5. Include impact assessment and timeline\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_product_description_generation(self, asserter):\n        \"\"\"Test semantic matching for e-commerce product description generation. Tests that the description\n        includes all required elements of an effective product listing.\"\"\"\n        actual = \"\"\"\n        Smart Home Security Camera - Model HC2000\n\n        Transform your home security with our latest AI-powered camera system. This next-generation \n        device combines advanced motion detection with crystal-clear 4K video quality, perfect for \n        both indoor and outdoor monitoring.\n\n        Key Features:\n        - 4K Ultra HD resolution with HDR\n        - 160\u00b0 wide-angle view\n        - Advanced AI motion detection\n        - Two-way audio communication\n        - Night vision up to 30 feet\n        - Weather-resistant (IP66 rated)\n\n        Smart Integration:\n        Works seamlessly with major platforms including:\n        - Amazon Alexa\n        - Google Home\n        - Apple HomeKit\n        - IFTTT\n\n        Technical Specifications:\n        - Dimensions: 3.2\" x 3.2\" x 5.1\"\n        - Weight: 12.3 oz\n        - Power: AC adapter or rechargeable battery\n        - Storage: Cloud or local SD card (up to 256GB)\n        - Connectivity: 2.4GHz/5GHz WiFi\n\n        What's in the Box:\n        - HC2000 Camera\n        - Mounting bracket\n        - Power adapter\n        - Quick start guide\n        - Screws and anchors\n\n        Perfect for:\n        - Home security\n        - Baby monitoring\n        - Pet watching\n        - Front door monitoring\n\n        30-day money-back guarantee\n        2-year manufacturer warranty\n        Free technical support\n        \"\"\"\n\n        expected = \"\"\"An e-commerce product description that must:\n        1. Include clear product name and model\n        2. List key features and specifications\n        3. Specify technical details and compatibility\n        4. Describe package contents\n        5. Include warranty and support information\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_assignment_feedback_missing_improvement_steps(self, asserter):\n        \"\"\"Test semantic matching for student assignment feedback. Should fail due to\n        missing specific improvement steps and learning objectives.\"\"\"\n        actual = \"\"\"\n        Assignment Feedback\n        Student ID: STU-2024-456\n        Assignment: Research Paper on Climate Change\n        Submission Date: November 22, 2024\n\n        Overall Assessment:\n        Your research paper demonstrates good understanding of climate change basics.\n        The writing is clear and well-structured, with appropriate use of scientific\n        terminology throughout the document.\n\n        Strengths:\n        - Strong introduction that sets context\n        - Good use of current scientific data\n        - Clear paragraph structure\n        - Proper citation format\n\n        Areas Noted:\n        - Some statistical interpretations could be more precise\n        - Additional peer-reviewed sources would strengthen arguments\n        - Conclusion could be more comprehensive\n\n        Grade: B+ (88/100)\n\n        Additional Comments:\n        The paper shows promise and indicates solid research skills. Your analysis\n        of temperature data trends was particularly well-done. Consider expanding\n        your discussion of potential mitigation strategies in future work.\n        \"\"\"\n\n        expected = \"\"\"An assignment feedback document that must:\n        1. Include basic assignment and student information\n        2. Provide specific strengths and weaknesses\n        3. List concrete steps for improvement\n        4. Reference specific learning objectives\n        5. Include grading criteria and score\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_real_estate_listing_generation(self, asserter):\n        \"\"\"Test semantic matching for real estate listing generation. Tests that the listing\n        includes all essential elements of an effective property description.\"\"\"\n        actual = \"\"\"\n        Stunning Modern Oasis in Prime Location\n        123 Maple Avenue, Riverside Heights\n\n        Discover urban elegance in this meticulously updated contemporary home, where \n        modern luxury meets practical living. This 2,400 sq ft residence seamlessly \n        blends indoor and outdoor living spaces.\n\n        Property Highlights:\n        - 4 bedrooms, 2.5 bathrooms\n        - Built: 2018\n        - Lot size: 0.25 acres\n        - Two-car attached garage\n        - Energy-efficient smart home features\n\n        Interior Features:\n        The open-concept main level showcases:\n        - Chef's kitchen with quartz countertops\n        - Custom Italian cabinetry\n        - Premium stainless steel appliances\n        - Expansive living room with 12-foot ceilings\n        - Primary suite with spa-inspired bathroom\n\n        Outdoor Living:\n        - Professional landscaping\n        - Covered patio with built-in BBQ\n        - Low-maintenance xeriscaping\n        - Private backyard retreat\n\n        Location Benefits:\n        - Walking distance to Central Park\n        - Top-rated school district\n        - 10 minutes to downtown\n        - Easy access to major highways\n\n        Recent Updates:\n        - New HVAC system (2023)\n        - Smart home integration\n        - Updated LED lighting\n        - Fresh interior paint\n\n        Price: $875,000\n        Available for immediate viewing\n        Virtual tour link: [URL]\n        \"\"\"\n\n        expected = \"\"\"A real estate listing that must:\n        1. Include property overview and key features\n        2. List specific amenities and updates\n        3. Describe location benefits\n        4. Use engaging, descriptive language\n        5. Provide essential details (size, bedrooms, price)\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_interview_feedback_missing_criteria(self, asserter):\n        \"\"\"Test semantic matching for interview feedback generation. Should fail due to\n        missing evaluation criteria and specific examples.\"\"\"\n        actual = \"\"\"\n        Interview Feedback Summary\n        Candidate ID: INT-2024-789\n        Position: Senior Software Engineer\n        Interview Date: November 22, 2024\n\n        Overall Impression:\n        The candidate demonstrated strong technical knowledge and communicated well\n        throughout the interview. They showed enthusiasm for the role and our company's\n        mission.\n\n        Discussion Points:\n        - Previous experience with cloud architecture\n        - Team collaboration approaches\n        - Problem-solving methodology\n        - Career goals and aspirations\n\n        Technical Discussion:\n        Candidate showed familiarity with:\n        - Microservices architecture\n        - CI/CD pipelines\n        - Cloud platforms (AWS, Azure)\n        - Agile development practices\n\n        Cultural Fit:\n        Appears to align well with our company values and team dynamics.\n        Demonstrated good communication skills and collaborative mindset.\n\n        Next Steps:\n        Proceed with reference checks if moving forward.\n        Schedule follow-up with hiring manager.\n        \"\"\"\n\n        expected = \"\"\"An interview feedback document that must:\n        1. Include candidate and position information\n        2. List specific evaluation criteria with ratings\n        3. Provide concrete examples of responses\n        4. Include technical assessment scores\n        5. Offer clear hiring recommendation\"\"\"\n\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_customer_service_ticket_response(self, asserter):\n        \"\"\"Test semantic matching for customer service ticket analysis and response generation.\"\"\"\n        actual = \"\"\"\n        Ticket Analysis and Response\n        Ticket ID: CS-2024-1122\n        Priority: Medium\n        Category: Product Return\n\n        Customer Query Summary:\n        Customer purchased a wireless headphone (Model: WH-1000XM4) three days ago\n        and is experiencing connectivity issues with their iPhone 13. Initial\n        troubleshooting steps were attempted without success.\n\n        Issue Analysis:\n        - Product is within return window (3 of 30 days)\n        - Common compatibility issue identified\n        - Troubleshooting already attempted\n        - Customer tone indicates frustration\n\n        Recommended Response:\n        Dear [Customer Name],\n\n        Thank you for reaching out about the connectivity issues with your WH-1000XM4\n        headphones. I understand how frustrating technical issues can be, especially\n        with a new purchase.\n\n        Based on your description, I can offer you two immediate solutions:\n\n        1. Advanced Troubleshooting:\n           - Reset the headphones (detailed steps attached)\n           - Update iPhone Bluetooth settings\n           - Install latest firmware\n\n        2. Hassle-free Return:\n           - Generate return label through our portal\n           - Full refund processed within 3 business days\n           - Free return shipping\n\n        Would you prefer to try the advanced troubleshooting steps, or would you like\n        to proceed with the return? I'm here to help with either option.\n\n        Next Steps:\n        - Await customer preference\n        - Prepare return label if requested\n        - Schedule follow-up within 24 hours\n\n        Response Tone: Empathetic and Solution-focused\n        Support Resources: KB-2345, RT-6789\n        \"\"\"\n\n        expected = \"\"\"A customer service response that must:\n        1. Include ticket categorization and priority\n        2. Summarize the customer's issue accurately\n        3. Provide multiple solution options\n        4. Include clear next steps\n        5. Maintain appropriate tone and empathy\"\"\"\n\n        asserter.assert_semantic_match(actual, expected)\n</code></pre>"},{"location":"reliability_testing/behavioral_testing_reliability/#conclusion","title":"Conclusion","text":"<p>This real-world test suite demonstrates that llm-app-test can reliably handle the kind of content that LLM applications generate in production environments. </p> <p>The 100% pass rate across 12,000 executions provides strong evidence of the library's reliability for real-world use cases.</p> <p>However, we emphasise that we remain unable to guarantee perfect determinism due to the nature of LLMs. What we are confident in, is that this library is \"good enough\" for production software.</p>"},{"location":"reliability_testing/behavioral_testing_reliability/#issue-reporting","title":"Issue reporting","text":"<p>If you experience any issues, especially with library reliability - please let us know, thanks!</p>"},{"location":"reliability_testing/behavioral_testing_reliability/#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Format Compliance Testing</li> </ul>"},{"location":"reliability_testing/format_compliance/","title":"Format Compliance","text":"<p>\u2190 Back to Home</p>"},{"location":"reliability_testing/format_compliance/#format-compliance-brute-force-validation-of-format-compliance","title":"Format Compliance - Brute Force Validation of Format Compliance","text":"<p>\u26a0\ufe0f Important Notice About Semantic Testing</p> <p>This documentation refers to tests originally named semantic assertion. We have since deprecated the name <code>semantic</code> in favor of behavioral testing, as we found that it more accurately describes what we were doing. The underlying implementation and reliability testing remain valid, as the core functionality is identical - we've simply improved the conceptual framework to better reflect what the library is actually doing.</p>"},{"location":"reliability_testing/format_compliance/#overview","title":"Overview","text":"<p>This page documents our extensive format compliance testing of llm-app-test's behavioral testing capabilities. We designed these tests to validate the library's ability to maintain consistent format requirements across diverse use cases.</p> <p>The test suite demonstrated 100% reliability across 13,000 test executions, with zero format violations detected.</p> <p>The relevant logs can be found here.</p>"},{"location":"reliability_testing/format_compliance/#test-suite-design","title":"Test Suite Design","text":"<p>The test suite covers 13 distinct test cases:</p> <ul> <li>8 positive test cases</li> <li>5 negative test cases</li> </ul> <p>Each test validates different aspects of format compliance across various content types:</p> <ol> <li>Multilingual equivalence</li> <li>Complex technical explanations</li> <li>Contextual understanding</li> <li>Complex sentiment analysis</li> <li>Long-form comparisons</li> <li>Subtle sentiment matching</li> <li>Technical context validation</li> <li>Ambiguous reference handling</li> <li>Temporal context verification</li> <li>Logical implication testing</li> <li>Complex multi-hop reasoning</li> <li>Adversarial content handling</li> <li>Long context understanding</li> </ol>"},{"location":"reliability_testing/format_compliance/#test-characteristics","title":"Test Characteristics","text":"<p>Each test was designed to validate:</p> <ul> <li>Format compliance</li> <li>Response structure</li> <li>Error handling</li> <li>Edge cases</li> <li>Content variations</li> </ul>"},{"location":"reliability_testing/format_compliance/#test-results","title":"Test Results","text":""},{"location":"reliability_testing/format_compliance/#format-compliance","title":"Format Compliance","text":"<ul> <li>Zero format violations across 13,000 executions</li> <li>Consistent PASS/FAIL behavior</li> <li>Proper error handling for negative cases</li> </ul>"},{"location":"reliability_testing/format_compliance/#response-reliability","title":"Response Reliability","text":"<ul> <li>Consistent format across all test types</li> <li>Proper error message formatting</li> <li>Stable behavior across multiple runs</li> </ul>"},{"location":"reliability_testing/format_compliance/#cost-analysis","title":"Cost Analysis","text":"<p>Running 13,000 tests using GPT-4o cost approximately US$7.90, demonstrating the economic viability of comprehensive format testing even with large test suites.</p>"},{"location":"reliability_testing/format_compliance/#test-configuration","title":"Test Configuration","text":"<p>All tests used library defaults:</p> <p><pre><code>LLM_PROVIDER=openai\nLLM_MODEL=gpt-4o \nLLM_TEMPERATURE=0.0 \nLLM_MAX_TOKENS=4096 \nLLM_MAX_RETRIES=2 \nLLM_TIMEOUT=10.0 # Added for OpenAI in 0.1.0b5 using the underlying Langchain implementation in dev branch\n</code></pre> The <code>assert_semantic_match</code>(Update: Deprecated and replaced with identical <code>assert_behavioral_match</code>) function also saw slight modification:</p> <pre><code>        if result.startswith(\"FAIL\"):\n            raise SemanticAssertionError(\n                \"Semantic assertion failed\",\n                reason=result.split(\"FAIL: \")[1]\n            )\n\n        # Section below added to cause failure in the event of format violation    \n\n        elif result.startswith(\"PASS\"):\n            pass\n        else:\n            raise RuntimeError(\n                f\"Format Non-compliance Detected {result}\"\n            )\n</code></pre> <p>The prompts to the asserter LLM (that sits behind <code>semantic_assert_match</code> (Update: Deprecated and replaced with identical <code>assert_behavioral_match</code>)) were:</p> <pre><code>DEFAULT_SYSTEM_PROMPT = \"\"\"You are a testing system. Your job is to determine if an actual output matches the expected behavior.\n\nImportant: You can only respond with EXACTLY: \n1. 'PASS' if it matches, or \n2. 'FAIL: &lt;reason&gt;' if it doesn't match.\n\nAny other type of response will mean disaster which as a testing system, you are meant to prevent.\n\nBe strict but consider semantic meaning rather than exact wording.\"\"\"\n\nDEFAULT_HUMAN_PROMPT = \"\"\"\nExpected Behavior: {expected_behavior}\n\nActual Output: {actual}\n\nDoes the actual output match the expected behavior? Remember, you will fail your task unless you respond EXACTLY \nwith 'PASS' or 'FAIL: &lt;reason&gt;'.\"\"\"\n</code></pre>"},{"location":"reliability_testing/format_compliance/#testing-cost","title":"Testing Cost","text":"<p>It actually cost us very little.</p> <p>This is likely due to how we constrained the model: The temperature is set to 0.0, and it has strict instructions to only respond with \"PASS\" or \"FAIL \". This constrains the much more expensive output token usage. <p>Running the test suite of 13 tests below 100 times (so 1,300 tests), cost us a total of US$0.79, using OpenAI's GPT-4o.</p> <p>Upon observing the cost of throwing thousands of API calls at OpenAI, we decided to just sod it and throw 13,000 API calls at them for the grand sum of... US$7.90. </p> <p>The purpose of these 13,000 runs was primarily to test the reliability of this library insofar as format violations were concerned, while lightly testing behavioral matching capabilities.</p> <p>Based on the testing documented in this page however, we are quite confident that llm-app-test will adhere to the format requirements in most situations and not throw stupid errors by failing to adhere to the requirements. </p> <p>Please refer to the other pages of testing reliability, specifically Behavioral Reliability Testing and Behavior At Semantic Boundary Analysis for more information on reliability testing of the ability of this library to test for behavioral matches.</p>"},{"location":"reliability_testing/format_compliance/#test-suite","title":"Test Suite","text":"<p>\u26a0\ufe0f Note About Test Code:  The test suite shown below uses the deprecated <code>SemanticAssertion</code> class and <code>assert_semantic_match</code> method.  These tests remain valid as the underlying implementation is identical in the new <code>BehavioralAssertion</code> class  and <code>assert_behavioral_match</code> method. The only change is in terminology to better reflect the testing approach.</p> <p>We used the following test suite for the purposes of format compliance testing:</p> <pre><code>import os\nimport pytest\nfrom llm_app_test.semantic_assert.semantic_assert import SemanticAssertion\nfrom llm_app_test.exceptions.test_exceptions import (\n    SemanticAssertionError\n)\n\n\nclass TestComplexSemanticAssertion:\n    @pytest.fixture\n    def asserter(self):\n        return SemanticAssertion()\n\n    def test_multilingual_equivalence(self, asserter):\n        \"\"\"Test semantic matching across different languages\"\"\"\n        actual = \"Bonjour, comment allez-vous?\"\n        expected = \"A polite greeting asking about wellbeing\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_complex_technical_explanation(self, asserter):\n        \"\"\"Test matching of technical explanations\"\"\"\n        actual = \"\"\"The TCP handshake is a three-way process where the client \n                 sends SYN, server responds with SYN-ACK, and client confirms with ACK\"\"\"\n        expected = \"An explanation of the TCP connection establishment process\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_contextual_understanding(self, asserter):\n        \"\"\"Test understanding of context-dependent statements\"\"\"\n        actual = \"The bank was steep and covered in wildflowers\"\n        expected = \"A description of a riverbank or hillside, not a financial institution\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_complex_sentiment_analysis(self, asserter):\n        \"\"\"Test understanding of subtle emotional content\"\"\"\n        actual = \"While the presentation wasn't perfect, it showed promise\"\n        expected = \"A constructive criticism with mixed but generally positive sentiment\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_long_form_comparison(self, asserter):\n        \"\"\"Test handling of longer text passages\"\"\"\n        actual = \"\"\"Machine learning is a subset of artificial intelligence \n                 that enables systems to learn and improve from experience without \n                 explicit programming. It focuses on developing computer programs \n                 that can access data and use it to learn for themselves.\"\"\"\n        expected = \"A comprehensive definition of machine learning emphasizing autonomous learning and data usage\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_subtle_sentiment_mismatch(self, asserter):\n        \"\"\"Test mismatch in subtle sentiment differences\"\"\"\n        actual = \"The project was completed on time, though there were some hiccups\"\n        expected = \"A statement expressing complete satisfaction with project execution\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_technical_context_mismatch(self, asserter):\n        \"\"\"Test mismatch in technical context interpretation\"\"\"\n        actual = \"The function returns a pointer to the memory address\"\n        expected = \"A description of a function that returns the value stored at a memory location\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_ambiguous_reference_mismatch(self, asserter):\n        \"\"\"Test mismatch in ambiguous references\"\"\"\n        actual = \"The bank processed the transaction after reviewing the account\"\n        expected = \"A description of a riverbank's geological formation process\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_temporal_context_mismatch(self, asserter):\n        \"\"\"Test mismatch in temporal context\"\"\"\n        actual = \"I will have completed the task by tomorrow\"\n        expected = \"A statement about a task that was completed in the past\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_logical_implication_mismatch(self, asserter):\n        \"\"\"Test mismatch in logical implications\"\"\"\n        actual = \"If it rains, the ground will be wet\"\n        expected = \"A statement indicating that wet ground always means it has rained\"\n        with pytest.raises(SemanticAssertionError) as excinfo:\n            asserter.assert_semantic_match(actual, expected)\n        assert \"Semantic assertion failed\" in str(excinfo.value)\n\n    def test_complex_multi_hop_reasoning(self, asserter):\n        \"\"\"Test complex multi-hop reasoning chains\"\"\"\n        actual = \"\"\"When water freezes, it expands by approximately 9% in volume. \n        This expansion creates less dense ice that floats according to Archimedes' principle of displacement. \n        Because Arctic sea ice is already floating in the ocean, its melting doesn't significantly affect sea levels - \n        it's already displacing its weight in water. However, land-based glaciers in places like Greenland \n        aren't currently displacing any ocean water. When these glaciers melt, they add entirely new water volume \n        to the oceans, making them a primary contributor to sea level rise.\"\"\"\n\n        expected = \"\"\"A multi-step scientific explanation.\n        Must maintain logical consistency across all steps.\"\"\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_adversarial_content(self, asserter):\n        \"\"\"Test handling of deliberately ambiguous or contradictory content\"\"\"\n        actual = \"The colorless green ideas sleep furiously\"\n        expected = \"A grammatically correct but semantically nonsensical statement\"\n        asserter.assert_semantic_match(actual, expected)\n\n    def test_long_context_understanding(self, asserter):\n        \"\"\"Test understanding of long, interconnected narratives\"\"\"\n        actual = \"\"\"\n        The Roman Empire's rise began with modest origins in central Italy. What started as a small \n        settlement along the Tiber River would eventually become one of history's most influential \n        civilizations. In the early days, Rome was ruled by kings, but this system was overthrown \n        in 509 BCE, giving birth to the Roman Republic.\n\n        During the Republic, Rome expanded its territory through military conquest and diplomatic \n        alliances. The Roman army became increasingly professional, developing innovative tactics \n        and technologies. This military success brought wealth and power, but also internal \n        challenges. Social tensions grew between patricians and plebeians, leading to significant \n        political reforms.\n\n        By the 1st century BCE, the Republic faced severe internal strife. Military commanders \n        like Marius, Sulla, and eventually Julius Caesar accumulated unprecedented power. Caesar's \n        crossing of the Rubicon in 49 BCE marked a point of no return. His assassination in 44 BCE \n        led to another civil war, ultimately resulting in his adopted heir Octavian becoming \n        Augustus, the first Roman Emperor.\n\n        Augustus transformed Rome into an empire while maintaining a facade of republican \n        institutions. He implemented sweeping reforms in administration, military organization, \n        and public works. The Pax Romana that followed brought unprecedented peace and prosperity \n        across the Mediterranean world. Trade flourished, cities grew, and Roman culture spread \n        throughout the empire.\n        \"\"\"\n        expected = \"\"\"A historical narrative that:\n        1. Maintains chronological progression\n        2. Shows cause-and-effect relationships\n        3. Develops consistent themes (power, governance, military)\n        4. Connects multiple historical events coherently\n        5. Demonstrates character development (e.g., Caesar to Augustus)\n        \"\"\"\n        asserter.assert_semantic_match(actual, expected)\n</code></pre>"},{"location":"reliability_testing/format_compliance/#issue-reporting","title":"Issue reporting","text":"<p>If you experience any issues, especially with library reliability - please let us know, thanks!</p>"},{"location":"reliability_testing/format_compliance/#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>API Reference</li> </ul>"}]}